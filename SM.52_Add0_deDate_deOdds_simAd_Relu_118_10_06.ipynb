{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist\n",
    "1. Make sure your \"Regional Format\" is set to \"English (United Kingdom)\" before Excel opens files that contain English date-like objects,\n",
    "   if you want them to be parsed as English datetime. English (United States) regional format will convert them to American Datetime as possible.\n",
    "   You have no way to prevent Excel from converting date-like strings or objects to datetime when opening a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from config import config\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "OPERATION = 'PRETRAIN'  # 'PRETRAIN'  'TRAIN_C'   'FINETUNE'   'TEST'\n",
    "\n",
    "TEST_ID = 'SM.52'\n",
    "\n",
    "HISTORY_LEN = 300\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0\n",
    "TRAIN_PERCENT= 94   # chronically the earliest part over the databased past rows.  In the end of this part, there is BACK part with the same size as VALID part.\n",
    "VALID_PERCENT = 4   # chronically the next part to train part over the databased past rows\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT # chronically the last part over the databased past rows. This part will be added with post-database rows, either past or known coming\n",
    "\n",
    "BUFFER_SIZE = 35000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "TEAM_EMBS = 50  #\n",
    "DECODE_BASE_DATE = True\n",
    "EMBED_AB_COLS = False    # True: Pls choose a small LR so that we have plenty of train epochs and the embedded values have enough chance to seek their proper places.\n",
    "ODDS_IN_ENCODER = False   #--------------\n",
    "ODDS_IN_DECODER = True\n",
    "ADDITIONAL_D_MODEL = 0   #------------ Try increase it when underfitting.\n",
    "\n",
    "TRANSFORMER_LAYERS = 10\n",
    "TRANSFORMER_HEADS = 6\n",
    "DROPOUT = 0.0  # 0.1\n",
    "ADAPTORS_LAYERS = 0 #------------ 10\n",
    "ADAPTORS_WIDTH_FACTOR = 30  # 30\n",
    "\n",
    "# This is an exponential curve that hits STARTING_LEARNING_RATE at step zero and EXAMPLE_LEARNING_RATE at step EXAMPLE_LEARNING_STEP.\n",
    "# lr(step) = STARTING_LEARNING_RATE * pow( pow(EXAMPLE_LEARNING_RATE/STARTING_LEARNING_RATE, 1/EXAMPLE_LEARNING_STEP), step )\n",
    "STARTING_LEARNING_RATE = 1e-7 #   1e-7 [:34]\n",
    "EXAMPLE_LEARNING_RATE = STARTING_LEARNING_RATE * 0.1\n",
    "EXAMPLE_LEARNING_STEP = 100\n",
    "\n",
    "STAKE_ACTIVATION = 'relu'   # 'softmax', 'sigmoid', 'relu'\n",
    "VECTOR_BETTING = True\n",
    "MODEL_TYPE_CHECK = True\n",
    "GET_VAL_LOSS_0 = True\n",
    "EXCLUDE_TRANSFORMER = False\n",
    "SIMPLIFY_ADAPTOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#-------------------- England ---------------------\\nTime range of data: 2004/2005 - 2024/2025\\nLeagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\\n!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\\nBookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\\nWilliam Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\\nWilliam Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\\nWilliam Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\\nWilliam Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\\nWilliam Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\\nBWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTRY = 'England'\n",
    "NUMBER_BOOKIES = (3-1)  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "BOOKIE_TO_EXCLUDE = ['BW']    # 'BWIN' odds don't show up since mid Febrary 2025. This may reduce the effective NUMBER_BOOKIES.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds makers are not archived for the league since 2013.\n",
    "\n",
    "\"\"\"\n",
    "#-------------------- England ---------------------\n",
    "Time range of data: 2004/2005 - 2024/2025\n",
    "Leagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\n",
    "!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\"\"\"\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed): np.random.seed(seed); tf.random.set_seed(seed); random.seed(seed)\n",
    "set_seed(23)    # For serendipity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "\n",
    "# Make sure Odds_cols comes first !!!\n",
    "_Cols_to_Always_Normalize = Odds_cols\n",
    "\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A', 'HTHG', 'HTAG', 'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY']\n",
      "['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A']\n",
      "['FTHG', 'FTAG', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A']\n"
     ]
    }
   ],
   "source": [
    "print(BBAB_cols)\n",
    "print(BB_cols)\n",
    "print(_Label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_params(df_grown):\n",
    "    def get_mean_and_std(col):\n",
    "        array = df_grown[col]\n",
    "        array = np.array(array)\n",
    "        return (array.mean(), array.std(), np.max(array))\n",
    "    params = {}\n",
    "    cols = _Cols_to_Always_Normalize + AB_cols\n",
    "    for col in cols:\n",
    "        params[col] = get_mean_and_std(col)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files found to rename:  0\n",
      "total rows in renamed files:  0\n",
      "df_grown:  (41248, 29)\n",
      "df_new:  (0, 29)\n"
     ]
    }
   ],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/\" + COUNTRY\n",
    "data_helpers.assign_seasonal_filenames(countryDirPath)\n",
    "df_grown, df_new = data_helpers.get_grown_and_new_from_football_data(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, train_mode = TRAIN_MODE, skip=True)\n",
    "if df_grown is not None: print(\"df_grown: \", df_grown.shape)\n",
    "if df_new is not None: print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "def createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens):\n",
    "        tokenizer = Tokenizer(models.WordLevel(unk_token=unknown_token))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "        tokenizer.train(corpus_files, trainer=trainer)\n",
    "        tokenizer.decoder = decoders.WordPiece(prefix=\" \")\n",
    "        return tokenizer\n",
    "\n",
    "def creat_team_tokenizer_uk(df_grown):\n",
    "    teams = list(set(list(df_grown['HomeTeam']) + list(df_grown['AwayTeam'])))\n",
    "    teams_string = [str(team) for team in teams]\n",
    "    teams_string = [re.sub(r\"\\s\", \"_\", item) for item in teams_string]    # replace spaces with a '_'\n",
    "    teams_text = \" \".join(teams_string)\n",
    "\n",
    "    corpus_file = os.path.join(countryDirPath, '_tokenizers', 'team_ids_text_uk.txt')\n",
    "    f = open(corpus_file, \"w+\", encoding=\"utf-8\")\n",
    "    f.write(teams_text)\n",
    "    f.close()\n",
    "\n",
    "    corpus_files = [corpus_file]\n",
    "    unknown_token = config['unknown_token']\n",
    "    special_tokens = [unknown_token] ################### + [\"[HOME]\", \"[AWAY]\"]\n",
    "    vocab_size = len(teams_string) + len(special_tokens)\n",
    "\n",
    "    tokenizer_team = createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)\n",
    "    return tokenizer_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "['tottenham', 'arsenal', 'liverpool', '[UNK]', 'tottenham', 'chelsea', '[UNK]', 'man_united', '[UNK]', '[UNK]', '[UNK]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[102, 4, 59, 0, 102, 28, 0, 63, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tottenham arsenal liverpool tottenham chelsea man_united'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df_grown)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41248\n"
     ]
    }
   ],
   "source": [
    "#================= build_id_to_ids.ipynb\n",
    "\n",
    "QUALITY = 75\n",
    "\n",
    "targetLength = HISTORY_LEN          # A target game will be explained by maximum 'targetLength' count of past games.\n",
    "minCurrent = 1e-7\n",
    "sinceDaysAgo = 365 * 20     # A target game will be explained by past games that took place since 'sinceDaysAgo' days ago. \n",
    "qualityPct = QUALITY        #\n",
    "conductance365 = 0.9        # 0.9 comes from test.\n",
    "chooseDivs=False\n",
    "\n",
    "id_to_ids_filename = str(targetLength) + '-' + str(minCurrent) + '-' + str(sinceDaysAgo) + '-' + str(qualityPct) + '-' + str(conductance365) + '-' + str(chooseDivs)\n",
    "id_to_ids_filePath = os.path.join(countryDirPath, '_id_to_ids', id_to_ids_filename + \".json\")\n",
    "\n",
    "id_to_ids_existing = None\n",
    "id_to_ids_existing = data_helpers.LoadJsonData(id_to_ids_filePath)\n",
    "tf_total = df_grown; df_search = df_grown\n",
    "id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, id_to_ids_filename, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, tf_total, df_search, chooseDivs=chooseDivs)\n",
    "data_helpers.SaveJsonData(id_to_ids, id_to_ids_filePath)\n",
    "\n",
    "print(len(id_to_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_raw_bbab(bbab, tokenizer_team, normalization_parms, train_mode=True):\n",
    "\n",
    "    label = []\n",
    "    if train_mode:\n",
    "        #---------------------- label, before changing bbab. They are raw full-time goals and raw odds.\n",
    "        for col in _Label_cols:\n",
    "            start = BBAB_cols.index(col)\n",
    "            label.append(bbab[start])\n",
    "\n",
    "    #----------------------- \n",
    "    start = BBAB_cols.index(Div_cols[0])\n",
    "    Div = bbab[start]\n",
    "    bbab[start] = DIVIISONS.index(Div)  # Assumes no n/a\n",
    "\n",
    "    start = BBAB_cols.index(Team_cols[0]); end = BBAB_cols.index(Team_cols[-1]) + 1\n",
    "    pair_str = [str(team) for team in bbab[start : end]]    # Team names are already normalized, removing/striping spaces.\n",
    "    pair_text = \" \".join(pair_str)\n",
    "    pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "    bbab[start : end] = pair_tokens # 0 for Unknown, by tokenizer trainig.\n",
    "\n",
    "    #--------------------- normalize\n",
    "    for col in _Cols_to_Always_Normalize:   # Odds_cols only now.\n",
    "        (mean, std, maximum) = normalization_parms[col]\n",
    "        start = BBAB_cols.index(col)\n",
    "        bbab[start] = (bbab[start] - mean) / std\n",
    "\n",
    "    #--------------------- columns for positional embedding\n",
    "    start = BBAB_cols.index(Date_cols[0])   #\n",
    "    date = bbab[start]\n",
    "    bbab[start] = (datetime.datetime.combine(date, datetime.time(0,0,0)) - config['baseDate']).days  # either positive or negative\n",
    "\n",
    "    #---------------------- bb only\n",
    "    start = BBAB_cols.index(BB_cols[0]); end = start + len(BB_cols)     # \n",
    "    bb = bbab[start : end]\n",
    "\n",
    "    return bbab, bb, label, date\n",
    "\n",
    "def getDateDetails(date):\n",
    "    baseYear = config['baseDate'].year\n",
    "    date_details = tf.Variable([date.year - baseYear, date.month, date.day, date.weekday()], dtype=tf.int32, trainable=False)\n",
    "    return date_details     # (4,)\n",
    "\n",
    "filler = tf.zeros_like([0] * len(BBAB_cols), dtype=tf.float32)\n",
    "\n",
    "def get_data_record(df_total, baseId, ids, tokenizer_team, normalization_parms, train_mode=True):\n",
    "    # try:\n",
    "        # base_bbab = list(df_grown.loc[df_grown['id'] == baseId, BBAB_cols])\n",
    "        if train_mode:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BBAB_cols].iloc[0, :])  # base_bbab follows BBAB. list\n",
    "        else:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BB_cols].iloc[0, :])  # base_bbab follows BB. list\n",
    "\n",
    "        base_bbab, base_bb, base_label, base_date = standardize_raw_bbab(base_bbab, tokenizer_team, normalization_parms, train_mode=train_mode)\n",
    "        # base_bbab, base_bb, base_label, base_date\n",
    "        # print('2', base_bbab)\n",
    "        baseId = tf.Variable(baseId, dtype=tf.int32, trainable=False)\n",
    "        base_bbab = tf.Variable(base_bbab, dtype=tf.float32, trainable=False)    # (len(BBAB_cols),)\n",
    "        base_bb = tf.Variable(base_bb, dtype=tf.float32, trainable=False)        # (len(BB_cols),)\n",
    "        base_label = tf.Variable(base_label, dtype=tf.float32, trainable=False)  # (len(_Label_cols),)\n",
    "        # print('3', base_bbab)\n",
    "        # Default sequence.\n",
    "        sequence = tf.transpose(tf.Variable([[]] * len(BBAB_cols), dtype=tf.float32, trainable=False))   # (0, len(BBAB_cols))\n",
    "        # sequence = np.array([[]] * len(BBAB_cols), dtype=config['np_float']).T\n",
    "        # print('3.5', sequence)\n",
    "        baseDateDetails = getDateDetails(base_date) # (4,)\n",
    "\n",
    "        concat = []\n",
    "        for id in ids:\n",
    "            bbab = list(df_total[df_total['id'] == id][BBAB_cols].iloc[0, :])   # bbab follows BBAB. list\n",
    "            # print('4', bbab)\n",
    "            bbab, _, _, _ = standardize_raw_bbab(bbab, tokenizer_team, normalization_parms, train_mode=train_mode)   # bbab follows BBAB. list\n",
    "            # check_normalization(bbab, normalization_parms)\n",
    "\n",
    "            bbab = tf.Variable(bbab, dtype=tf.float32, trainable=False)[tf.newaxis, :]       # (1, len(BBAB_cols))\n",
    "            # _bbab = bbab[0].numpy()\n",
    "            # check_normalization(_bbab, normalization_parms)\n",
    "\n",
    "            concat.append(bbab)     # concat doesn't create a new axis.\n",
    "\n",
    "        if len(concat) > 0:\n",
    "            sequence = tf.concat(concat, axis=0)    # (nSequence, len(BBAB_cols))\n",
    "            # if sequence.shape[0] > 0:\n",
    "            #     bbab = sequence[0].numpy()\n",
    "            #     check_normalization(bbab, normalization_parms)\n",
    "\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            patch = tf.stack([filler] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, patch], axis=0)     # concat doesn't create a new axis. (MAX_TOKENS, len(BBAB_cols))\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, len(BBAB_cols))\n",
    "        baseDateDetails = baseDateDetails[tf.newaxis, :]\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32, trainable=False) # (MAX_TOKENS,) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]    # (MAX_TOKENS, MAX_TOKENS)\n",
    "\n",
    "        return (baseId, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "\n",
    "def generate_dataset_uk(df_total, fixture_id_to_ids, tokenizer_team, normalization_parms, train_mode=True):\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for baseId, (tag, label, ids) in fixture_id_to_ids.items():\n",
    "            baseId = int(baseId)\n",
    "            baseId, sequence, base_bb, base_label, baseDateDetails, mask = get_data_record(df_total, baseId, ids, tokenizer_team, normalization_parms, train_mode=train_mode)\n",
    "            print(\"count: {}, baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "            # if count > 200: break\n",
    "            yield (baseId, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS))),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "   \n",
    "def X_versus_Y(baseId, sequence, base_bb, base_label, baseDateDetails, mask):\n",
    "    return (baseId, sequence, base_bb, baseDateDetails, mask), (base_label)\n",
    "\n",
    "# I found, in PositionalEmbedding, batch size ranges between 3, 4, 6 and 8, while they should be 4 or 8, except margial rows. Check it.\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def apply_train_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)       # Shuffle the training dataset.\n",
    "        .batch(train_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def apply_test_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-normalization' + \".json\")\n",
    "normalization_parms = get_normalization_params(df_grown)\n",
    "print(normalization_parms)\n",
    "data_helpers.SaveJsonData(normalization_parms, std_path)\n",
    "normalization_parms = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE:\n",
    "    ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-foundation-ds')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_grown, id_to_ids, tokenizer_team, normalization_parms)\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(ds)\n",
    "\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "take_size = total_size - starting_size - ending_size\n",
    "remaining_ds = ds.skip(starting_size)\n",
    "dataset = remaining_ds.take(take_size)          # starting | dataset | ending\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size - valid_size      # len(dataset) = train_size + valid_size + tast_size        NO back_size\n",
    "train_ds = dataset.take(train_size)                    # dataset[: train_size]\n",
    "remaining_ds = dataset.skip(train_size - valid_size)    # dataset[train_size - valid_size: ]\n",
    "back_ds = remaining_ds.take(valid_size)                # dataset[train_size - valid_size: train_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)            # dataset[train_size: ]\n",
    "valid_ds = remaining_ds.take(valid_size)               # dataset[train_size, train_size + valid_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)                # dataset[train_size + valid_size :]\n",
    "test_ds = remaining_ds.take(test_size)\n",
    "backtest_ds = valid_ds.concatenate(test_ds) # tf.data.Dataset.zip((valid_ds, test_ds))\n",
    "\n",
    "assert len(test_ds) == test_size\n",
    "assert dataset_size == len(train_ds) + len(valid_ds) + len(test_ds)\n",
    "\n",
    "print(\"total_size, dataset, train_ds, back_ds, valid_ds, test_ds, backtest_ds: \", \\\n",
    "      total_size, len(dataset), len(train_ds), len(back_ds), len(valid_ds), len(test_ds), len(backtest_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_baseDateDetails(ds):\n",
    "    baseId = baseDateDetails = None\n",
    "    for z in ds:\n",
    "        baseId, _, _, _, baseDateDetails, _, _ = z\n",
    "        break\n",
    "    return baseId.numpy(), baseDateDetails.numpy()\n",
    "# print(\"train_ds's first game: \", find_first_baseDateDetails(train_ds))\n",
    "# print(\"back_ds's first game: \", find_first_baseDateDetails(back_ds))\n",
    "# print(\"valid_ds's first game: \", find_first_baseDateDetails(valid_ds))\n",
    "# print(\"test_ds's first game: \", find_first_baseDateDetails(test_ds))\n",
    "\"\"\"\n",
    "train_ds's first game:  (1000001, array([[4, 8, 7, 5]]))\n",
    "back_ds's first game:  (1037717, array([[23,  2, 18,  5]]))\n",
    "valid_ds's first game:  (1039366, array([[23, 12, 22,  4]]))\n",
    "test_ds's first game:  (1041019, array([[24, 11,  2,  5]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batches = apply_train_pipeline(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    cnt = 5\n",
    "    for z in train_batches:\n",
    "        (baseId, sequence, base_bb, baseDateDetails, mask), (base_label) = z\n",
    "        cnt -= 1 \n",
    "        if cnt == 0: break\n",
    "    print(baseId.shape, sequence.shape, base_bb.shape, mask.shape, base_label.shape, baseDateDetails.shape)\n",
    "    sample_x = (sequence, base_bb, baseDateDetails, mask)\n",
    "    sample_y = (base_label)\n",
    "    # print(baseId.numpy(), base_bb.numpy(), baseDateDetails.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_batches = apply_test_pipeline(back_ds)\n",
    "valid_batches = apply_test_pipeline(valid_ds)\n",
    "test_batches = apply_test_pipeline(test_ds)\n",
    "backtest_batches = apply_test_pipeline(backtest_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):     # positions.shape = (batch, seq_len). d_model is a positive even integer.\n",
    "    # positions[b, :] = [dago_299, dago_298, ..., dago_0] = eg [6, 7, ..., 1611]\n",
    "\n",
    "    years = 25\n",
    "    quotient = 365 * years / d_model   # 325 * 25 is total days during 25 years since 2024 (the earliest data year). \n",
    "    fractional_pos = positions / quotient  # (batch, seq_len). max << d_model.  lower=0, upper<<d_model\n",
    "    depth = d_model/2   #\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth  # (depth,). [0/depth, 1/depth, ..., (depth-1)/depth]\n",
    "\n",
    "    # Why do we use this specific formula for depths, while any formula will work?\n",
    "    # depths will be multiplied by fractional_pos, which may have ever large number.\n",
    "    # We want the product to be limited, and depths should converge to zero to limit the product.\n",
    "    # If it cenverges to zero too fast, though, the generated pos_encoding will have little representative power as positional representation.\n",
    "    # Why, then, do we want to limit the product?\n",
    "    # The product visiting many different values will be enough for us. The solution is to let them go to zero, from a significant value.\n",
    "    BIG = d_model * 1.0 * 0.8     # let it be float.\n",
    "    depths = 1.0 / tf.pow(BIG, depths)        # 1 / [1, ..., BIG ** ((depth-1)/depth)] = [1, ..., >1/BIG]\n",
    "\n",
    "    # Why fractional_pos is multiplied linearly?\n",
    "    # Because (sin(A+a), cos(A+a)) is a rotation of (sin(A), cos(A)), no matter what depths is.\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] * depths  # [0, ..., <<d_model ] * [ 1, ..., >1/BIG]. \n",
    "    # Interleaving sin and cos is equivalent to seperated sin and cos.\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')  # Interleaving sin and cos. (batch, seq_len, d_model)\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)   # Seperated sin and cos. (batch, seq_len, d_model)\n",
    "    return pos_encoding  # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 15\n",
    "seq_len = 300\n",
    "d_model = 118\n",
    "positions = tf.Variable([[i for i in range(0, 365 * years, int(365 * years / seq_len))] for batch in range(100)], dtype=tf.float32, trainable=False)\n",
    "# positions = tf.ones((100, 200), dtype=tf.float32) * tf.range(200, dtype=tf.float32)\n",
    "pos_encoding = positional_encoding(positions, d_model=d_model)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class hParams:    \n",
    "    nDivisions = len(DIVIISONS) \n",
    "    division_embs = 4   # [0, 1, 2, 3]\n",
    "\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 4  # [0, 1, 2, 3] Maximum nGoals goals for a team in each of 1st and 2nd halfs. Extra goals will be clipped_by_value.\n",
    "    goal_embs = 4\n",
    "    nShoots = 21    # [0, ..., 20]\n",
    "    shoot_embs = 4 # for combination\n",
    "    nShootTs = 11   # [0, ..., 10]\n",
    "    shootT_embs = 4 # for combination\n",
    "    nCorners = 11   # [0, ..., 10]\n",
    "    corner_embs = 4 # for combination\n",
    "    nFauls = 21     # [0, ..., 20]\n",
    "    faul_embs = 2 # for combination\n",
    "    nYellows = 5    # [0, ..., 4]\n",
    "    yellow_embs = 2 # for combination\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "\n",
    "    # d_model DEPENDS...\n",
    "    # We want as small d_model as possible, because from which we can freely choose an actual d_model.\n",
    "    # Tests seem to indicate that larger d_model leads to training overfitting and validation underfitting.\n",
    "    \n",
    "    # Where goes ODDS_IN_DECODER ????????????????????\n",
    "\n",
    "    d_model = 1 * division_embs + 2 * team_embs     # 1 division, 2 teams\n",
    "    if ODDS_IN_ENCODER: d_model = d_model + 3 * NUMBER_BOOKIES  # + 3 odds * nBookies\n",
    "    d_encoder = d_decoder = 0\n",
    "    if EMBED_AB_COLS:\n",
    "        d_encoder = d_model + 1 * goal_embs + 1 * goal_embs + 1 * (shoot_embs + shootT_embs + corner_embs + faul_embs + yellow_embs)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    else:   # This mode, EMBED_AB_COLS = False, gives much smaller d_moddel, maybe avoiding overfitting.\n",
    "        d_encoder = d_model + 1 * len(AB_cols)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    d_model = max(d_encoder, d_decoder)     # (136, 118) for EMBED_AB_COLS, (126, 118) for False EMBED_AB_COLS\n",
    "    d_model += ADDITIONAL_D_MODEL               # Adjust for the model size and overfitting.\n",
    "    d_model = d_model + d_model % 2     # make it an even number.\n",
    "    print(\"d_model: \", d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "class PositionalEmbedding(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, hParams, isEncoder=True):\n",
    "        super().__init__()\n",
    "        self.isEncoder = isEncoder\n",
    "\n",
    "        # game\n",
    "        self.division_emb = tf.keras.layers.Embedding(hParams.nDivisions, hParams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.team_emb = tf.keras.layers.Embedding(hParams.nTeams, hParams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                # AB_cols\n",
    "                self.firstH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.secondH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shoot_emb = tf.keras.layers.Embedding(hParams.nShoots * hParams.nShoots, hParams.shoot_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shootT_emb = tf.keras.layers.Embedding(hParams.nShootTs * hParams.nShootTs, hParams.shootT_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.corner_emb = tf.keras.layers.Embedding(hParams.nCorners * hParams.nCorners, hParams.corner_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.faul_emb = tf.keras.layers.Embedding(hParams.nFauls * hParams.nFauls, hParams.faul_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.yellow_emb = tf.keras.layers.Embedding(hParams.nYellows * hParams.nYellows, hParams.yellow_embs, dtype=tf.float32, mask_zero=False)\n",
    "            else:\n",
    "                params = []\n",
    "                for col in AB_cols:\n",
    "                    params.append(normalization_parms[col])\n",
    "                self.AB_mormalization_params = tf.Variable(params, dtype=tf.float32, trainable=False)  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "\n",
    "        if not self.isEncoder:\n",
    "            if DECODE_BASE_DATE:\n",
    "                self.day_emb = tf.keras.layers.Embedding(31, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.month_emb = tf.keras.layers.Embedding(12, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.wday_emb = tf.keras.layers.Embedding(7, 2, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        if EXCLUDE_TRANSFORMER: pass\n",
    "        else: self.position_permuting = tf.keras.layers.Dense(hParams.d_model)\n",
    "\n",
    "        self.idx_Days = BB_cols.index('Date')\n",
    "\n",
    "    def representDateDetails(self, dateDetails):\n",
    "        # dateDetails: (batch, 1, 4)\n",
    "        bYears, bMonths, bDays, bWDays = tf.split(dateDetails, [1, 1, 1, 1], axis=-1)   # All should be of (batch, seq_len = 1, 1)\n",
    "        bYears = tf.cast(bYears, dtype=tf.float32)  # (batch, seq_len = 1, 1)\n",
    "        bDays = self.day_emb(bDays)[:, :, -1]       # (batch, seq_len = 1, embs = 2)\n",
    "        bMonths = self.month_emb(bMonths)[:, :, -1] # (batch, seq_len = 1, embs = 2)\n",
    "        bWDays = self.wday_emb(bWDays)[:, :, -1]    # (batch, seq_len = 1, embs = 2)\n",
    "        # w = tf.Variable(np.math.pi / 25, dtype=tf.float32, trainable=False)    # 25 years are covered by pi or a half circle.\n",
    "        w = np.math.pi / 25\n",
    "        bYearsCos = tf.math.cos(bYears * w)\n",
    "        bYearsSin = tf.math.sin(bYears * w)\n",
    "        bYears = tf.concat([bYearsCos, bYearsSin], axis=-1)   # (batch, seq_len = 1, 1+1 = 2)\n",
    "        return bYears, bMonths, bDays, bWDays\n",
    "\n",
    "    def combined_embeddings_of_double_columns(self, emb_layer, columns, nValues):\n",
    "        # Assume emb_layer = Embedding(nValues * nValues, embs, mask_zero=False)\n",
    "        cols = tf.cast(columns, dtype=tf.int32)\n",
    "        cols = tf.clip_by_value(cols, 0, nValues-1)\n",
    "        combi = cols[:, :, 0] * nValues + cols[:, :, 1]   # (batch, seq_len, 1). [0, ..., nValues * nValues - 1]\n",
    "        combi = emb_layer(combi)\n",
    "        return combi    # (batch, seq_len, 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        (sequence, base_bb, baseDateDetails, mask) = x # sob = sequence or base_bb\n",
    "        sequenceDays = sequence[:, :, self.idx_Days]  # (batch, seq_len)\n",
    "        baseDays = base_bb[:, :, self.idx_Days]   # (batch, 1)\n",
    "\n",
    "        # sequence follows BBAB, whereas base_bb follows \n",
    "        \n",
    "        # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        if self.isEncoder:\n",
    "            # ramainder: Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols  --- total 12 fields.\n",
    "            id, div, days, teams, odds, half_goals, full_goals, shoot, shootT, corner, faul, yellow\\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), \\\n",
    "                                  len(Shoot_cols), len(ShootT_cols), len(Corner_cols), len(Faul_cols), len(Yellow_cols)], axis=-1)\n",
    "            # All shape of (batch, sequence, own_cols), all tf.flaot32\n",
    "        else:\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "            # remainder: [] \n",
    "            # All shape of (batch, 1, own_cols), guess., all tf.float32\n",
    "        \n",
    "        div = self.division_emb(tf.cast(div, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=1, division_embs)\n",
    "        div = tf.reshape(div, [div.shape[0], div.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=1*division_embs) --- \n",
    "        teams = self.team_emb(tf.cast(teams, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=2, team_embs)\n",
    "        teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=2*team_embs) --- \n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                first_half_goals = self.combined_embeddings_of_double_columns(self.firstH_goal_emb, half_goals, hParams.nGoals)\n",
    "                second_half_goals = self.combined_embeddings_of_double_columns(self.secondH_goal_emb, full_goals - half_goals, hParams.nGoals)\n",
    "                shoot = self.combined_embeddings_of_double_columns(self.shoot_emb, shoot, hParams.nShoots)\n",
    "                shootT = self.combined_embeddings_of_double_columns(self.shootT_emb, shootT, hParams.nShootTs)\n",
    "                corner = self.combined_embeddings_of_double_columns(self.corner_emb, corner, hParams.nCorners)\n",
    "                faul = self.combined_embeddings_of_double_columns(self.faul_emb, faul, hParams.nFauls)\n",
    "                yellow = self.combined_embeddings_of_double_columns(self.yellow_emb, yellow, hParams.nYellows)\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "                else: concat = [div, teams, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "\n",
    "            else:   # normalize now\n",
    "                # AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "                AB_values = [half_goals, full_goals, shoot, shootT, corner, faul, yellow]   # all (batch, 2 cols)\n",
    "                AB_values = tf.concat(AB_values, axis=-1) # (batch, seq_len, num_AB_cols=14)\n",
    "                # self.AB_mormalization_params  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "                AB_values = (AB_values - self.AB_mormalization_params[:, 0]) / self.AB_mormalization_params[:, 1]\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, AB_values]\n",
    "                else: concat = [div, teams, AB_values]\n",
    "        else:\n",
    "            if DECODE_BASE_DATE:\n",
    "                bYears, bMonths, bDays, bWDays = self.representDateDetails(baseDateDetails)\n",
    "                if ODDS_IN_DECODER:  concat = [div, teams, odds, bYears, bMonths, bDays, bWDays]\n",
    "                else: concat = [div, teams, bYears, bMonths, bDays, bWDays]\n",
    "            else:\n",
    "                if ODDS_IN_DECODER: concat = [div, teams, odds]\n",
    "                else: concat = [div, teams]\n",
    "\n",
    "        concat = tf.concat(concat, axis=-1)\n",
    "        # print(\"PE. concat.shape: {}, hParams.d_model: {}\".format(concat.shape, hParams.d_model))\n",
    "        assert concat.shape[-1] <= hParams.d_model        \n",
    "\n",
    "        if EXCLUDE_TRANSFORMER: pass # concat: (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        else:\n",
    "            concat = self.position_permuting(concat)  # (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "            positions = tf.cast(baseDays - sequenceDays, dtype=tf.float32) if self.isEncoder else tf.cast(baseDays - baseDays, dtype=tf.float32) # the latter is a zero tensor.\n",
    "            # eg. positions[b, :] = Tensor([6, 7, ..., 1911]\n",
    "            pe = positional_encoding(positions, d_model=hParams.d_model) # (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "            # pe = pe / tf.math.sqrt(tf.cast(concat.shape[-1], tf.float32))   # Read \"Attention is all you need\"\n",
    "            concat = concat + pe\n",
    "        \n",
    "        if self.isEncoder:\n",
    "            mask = mask     # (batch, MAX_TOKEN, MAX_TOKEN), although (batch, 1, MAX_TOKEN) will propagate.\n",
    "        else:\n",
    "            # Decoder layers will find cross attention between Decoder concat and Encoder concat.\n",
    "            mask = mask[:, 0:concat.shape[1], :]    # concat: (batch, 1, MAX_TOKEN)   \n",
    "\n",
    "        return (concat, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    PE = PositionalEmbedding(hParams, isEncoder=True)\n",
    "    PE(sample_x); PE.summary()\n",
    "    @tf.function\n",
    "    def pos():\n",
    "        return PE(sample_x)\n",
    "    eSob, eMask = pos()\n",
    "    print(eSob.shape, eMask.shape )\n",
    "    del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    PE = PositionalEmbedding(hParams, isEncoder=False)\n",
    "    dSob, dMask = PE(sample_x)\n",
    "    print(dSob.shape, dMask.shape )\n",
    "    del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axis is normalized across. No inter-token operation.\n",
    "      self.add = tf.keras.layers.Add()  # Operate on the -1 axis.\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(query=x, key=context, value=context, attention_mask=mask, return_attention_scores=True)    \n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)  # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])\n",
    "      x = self.layer_norm(x)\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (batch, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_emb(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # (batch_size, max_tokens, d_model)\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "      self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=False)\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      if EXCLUDE_TRANSFORMER: pass\n",
    "      else:\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, dropout_rate=dropout_rate)\n",
    "            for _ in range(hParams.num_layers)]\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (batch, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_emb(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens), ca_cask: which tokens of context to mask out (with 0)\n",
    "      if EXCLUDE_TRANSFORMER: pass\n",
    "      else:\n",
    "        x = self.dropout(x)\n",
    "        for decoder_layer in self.dec_layers:\n",
    "          x  = decoder_layer(x, context, ca_mask)\n",
    "        self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      if EXCLUDE_TRANSFORMER:\n",
    "        self.all_one_context = tf.ones((BATCH_SIZE, MAX_TOKENS, hParams.d_model), dtype=tf.float32) # (batch, max_tokens, d_model)\n",
    "      else:\n",
    "        self.encoder = Encoder(hParams, dropout_rate=dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(hParams.d_model) #-------------- to modify\n",
    "      self.decoder = Decoder(hParams, dropout_rate=dropout_rate)\n",
    "\n",
    "    def call(self, x):\n",
    "      # inputs = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), baseDateDetails: (batch, 1, 4), mask: (batch, max_token, max_token)\n",
    "      if EXCLUDE_TRANSFORMER: context = self.all_one_context\n",
    "      else: context = self.encoder(x)  # (batch, max_tokens, d_model). Only sequence and mask are used.\n",
    "      x = self.decoder(x, context)  # (batch, 1, d_model).  Only base_bb, baseDateDetails, and mask are used.      \n",
    "      if EXCLUDE_TRANSFORMER: logits = x\n",
    "      else: logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    sample_transformer = Transformer(hParams)\n",
    "    y = sample_transformer(sample_x)\n",
    "    sample_transformer.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def fun():\n",
    "        y = sample_transformer(sample_x)\n",
    "        return y\n",
    "    y = fun()\n",
    "    sample_transformer.summary()\n",
    "    del sample_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Add_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, seed):\n",
    "      super().__init__()\n",
    "      self.dense =tf.keras.layers.Dense (dim, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=seed), bias_initializer=Zeros, activation='tanh')\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "      dense = self.dense(x)\n",
    "      x = self.add([x, dense])  # x.shape[-1] == dim\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "\n",
    "Zeros = tf.keras.initializers.Zeros()\n",
    "\n",
    "# Used for earlier versions that don't allow mixing bookies.\n",
    "class Adaptor(tf.keras.Model):\n",
    "  def __init__(self, nLayers, d_main, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    # total (nLayers + nLayers) dims = 2 * nLayers dims\n",
    "    dims = [d_main] * nLayers\n",
    "    layers = [Dense_Add_Norm(dims[id], id) for id in range(len(dims))]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "    self.initial = tf.keras.layers.Dense (d_main, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "    self.final = tf.keras.layers.Dense (d_output, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "  def call(self, x):  # (batch, d_model)\n",
    "    x = self.initial(x)\n",
    "    x = self.seq(x)   # (batch, d_model)\n",
    "    x = self.final(x) # (batch, nBookies * 3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan = Dense_Add_Norm(118 * 30, 23)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(dan)\n",
    "print(model(tf.ones((1,118 * 30), dtype=tf.float32)))\n",
    "print(model.summary())\n",
    "del dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = tf.Variable([normalization_parms[col][0] for col in Odds_cols], trainable=False)\n",
    "std_variation = tf.Variable([normalization_parms[col][1] for col in Odds_cols], trainable=False)\n",
    "print(std_mean.shape, std_variation.shape)\n",
    "\n",
    "# No, _Label_cols were not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1X2(tf.keras.Model):\n",
    "    softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.nQueries = nQueries\n",
    "        self.transformer = Transformer(hParams, dropout_rate=dropout_rate)\n",
    "        #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "        self.bookies = ['HDA' + str(b) for b in range(NUMBER_BOOKIES)]\n",
    "\n",
    "        if SIMPLIFY_ADAPTOR:\n",
    "            self.adaptor = tf.keras.layers.Dense(len(self.bookies) * self.nQueries)\n",
    "        else:\n",
    "            self.adaptor = Adaptor(ADAPTORS_LAYERS, hParams.d_model * ADAPTORS_WIDTH_FACTOR, self.nQueries * len(self.bookies))\n",
    "        return\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.transformer(x) # (batch, d_model)\n",
    "        stake_p = self.adaptor(x)   # (batch, nBookies * nQueries)\n",
    "        stake_p = tf.reshape(stake_p, [stake_p.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        stack = [stake_p[:, :, b] for b in range(len(self.bookies))]    # NNNNo way to swap axes efficiently.\n",
    "        stake_p = tf.stack(stack, axis=0)   # (nBookies, batch, nQueries)\n",
    "        if STAKE_ACTIVATION == 'softmax':\n",
    "            stake_p = tf.nn.softmax(stake_p)  # (nBookies, batch, nQueries)   #\n",
    "        elif STAKE_ACTIVATION == 'sigmoid':\n",
    "            stake_p = tf.math.sigmoid(stake_p * 5)  # the previous activation is tanh, ranging (-1, 1). Multiplier 5 will result in range (near 0, near 1)\n",
    "        elif STAKE_ACTIVATION == 'relu':\n",
    "            stake_p = tf.nn.relu(stake_p)\n",
    "        return stake_p\n",
    "    \n",
    "    def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "        # ftGoals:  (batch, 2)\n",
    "        ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "        h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "        h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "        return h\n",
    "\n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output  # (nBookies, batch, nQueries)\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        # Note: Do not normalize stake_p. It can learn whether to bet or not, as well as betting direction.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "\n",
    "        profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)    # (nBooies, batch)\n",
    "        mean_profit_per_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "\n",
    "        profit_backtest = tf.reduce_mean(mean_profit_per_game, axis=None)  # () \n",
    "        loss = - profit_backtest  # U.action.42\n",
    "    \n",
    "        return loss, mean_profit_per_game # (), negative average profit on a game on a bookie\n",
    "    \n",
    "    def backtest_event_wise(self, y, output, key_a, key_b):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: oh_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        def analyze(stake_p):\n",
    "            # sum_stake_p = tf.math.reduce_sum(stake_p, axis=-1)\n",
    "            norm = tf.norm(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "            profit_p = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / norm) - 1.0, stake_p), axis=-1)# (nBookies, batch)\n",
    "            profit_backtest = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1) # (nBookies, batch)\n",
    "            condition = tf.math.logical_and(key_a <= profit_p, profit_p <= key_b)   # (nBookies, batch), dtype=tf.bool\n",
    "            indices = tf.where(condition)   # (n, 2)    values: (bookieId, gameId)\n",
    "            profit_backtest = profit_backtest[condition]  # (n,)    values: (profit)\n",
    "            stake = stake_p[condition]  # (n, nQueries)\n",
    "            # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "            return indices, stake, profit_backtest\n",
    "\n",
    "        if VECTOR_BETTING:\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "        else:\n",
    "            bestQuery = tf.argmax(stake_p, axis=-1)     #   (nBookies, batch, nQueries)\n",
    "            stake_p = tf.one_hot(bestQuery, self.nQueries, dtype=tf.float32)   #   (nBookies, batch, nQueries)\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "\n",
    "        # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "        return indices, stake, profit_backtest  # Not normalized. i.e. not divided with the norm of stake_p\n",
    "    \n",
    "    def get_batch_backtest(self, y, output):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "        sum = tf.math.reduce_sum(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "        indicatorP = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / sum) - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        backtestP = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        indices = tf.where(indicatorP == indicatorP)    # (nBookies * batch, 2 [bookieId, rel_gameId])\n",
    "        indicatorP = tf.reshape(indicatorP, [-1])   # (nBookies * batch, )\n",
    "        backtestP = tf.reshape(backtestP, [-1])     # (nBookies * batch, )\n",
    "        stake_p = tf.reshape(stake_p, [-1, self.nQueries])\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        return indicatorP, backtestP, indices, stake_p\n",
    "    \n",
    "dummy_mean_profit_per_game = None\n",
    "class Model_Filter(Model_1X2):\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__(hParams, nQueries, dropout_rate=dropout_rate)\n",
    "        self.bce = tf.losses.BinaryCrossentropy(from_logits=False)\n",
    "        return\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.transformer(x) # (batch, d_model)\n",
    "        stake_p = self.adaptor(x)   # (batch, nBookies * nQueries)\n",
    "        stake_p = tf.reshape(stake_p, [stake_p.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        stack = [stake_p[:, :, b] for b in range(len(self.bookies))]    # NNNNo way to swap axes efficiently.\n",
    "        stake_p = tf.stack(stack, axis=0)   # (nBookies, batch, nQueries)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=-1) # (nBookies, batch)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=0) # (batch,)\n",
    "        stake_p = tf.math.sigmoid(stake_p * 5)          # (batch,)\n",
    "        return stake_p\n",
    "    \n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch,) \n",
    "        # output: (batch,)\n",
    "        loss = self.bce(y, output)\n",
    "        return loss, dummy_mean_profit_per_game\n",
    "\n",
    "def create_model_object(model_class):\n",
    "    try: model_c\n",
    "    except NameError: pass\n",
    "    else: del model_c\n",
    "    try: model_1x2\n",
    "    except NameError: pass\n",
    "    else: del model_1x2\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    model = model_class(hParams, nQueries=3, dropout_rate=DROPOUT)   # Do not create a reference and return directly.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = create_model_object(Model_1X2)\n",
    "\n",
    "if MODEL_TYPE_CHECK:\n",
    "    stake_p = model_1x2(sample_x, training=True)\n",
    "    @tf.function\n",
    "    def fun():\n",
    "        stake_p = model_1x2(sample_x, training=True)\n",
    "        loss, _ = model_1x2.loss(sample_y, stake_p)\n",
    "        return stake_p, loss\n",
    "    stake_p, loss = fun()\n",
    "    model_1x2.summary()\n",
    "    del model_1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_1x2, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_with_dataset(dataset, profit_keys):\n",
    "    profits = [-1.0] * len(profit_keys)\n",
    "    casts = [0] * len(profit_keys)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        profit_list, cast_list = model_1x2.backtest(y, outputs, profit_keys)\n",
    "\n",
    "        for p, c, id in zip(profit_list, cast_list, range(len(profit_keys))):\n",
    "            if c > 0:\n",
    "                profits[id] = (profits[id] * casts[id] + p * c) / (casts[id] + c)\n",
    "                casts[id] = casts[id] + c\n",
    "    # print('key', profit_back_mean, nBettingsTotal)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_event_wise_with_dataset(ds_batches, model, key_a, key_b):\n",
    "    indices = []; stakes = []; profits = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model(x, training=False)  #\n",
    "        new_indices, new_stakes, new_profits = model.backtest_event_wise(y, outputs, key_a, key_b)  # Tensor (n, 2 <bookieId, rel_gameId>), (n, nQueries), (n,)\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ..., [bookie, rel_game_id]]. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # [[0, 123], ..., [[bookie, gameId]]], len = n\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes\n",
    "\n",
    "        new_profits = list(new_profits.numpy())                 # [0.3, ...]\n",
    "        profits = profits + new_profits     # [1.3, ..., profit], len = n\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, stake, profit) for (bookie, gameId), stake, profit in zip(indices, stakes, profits)]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "    return interval_backtests     # [(bookie, gameId, profit) for ...]\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def get_backtest_and_indicator_profits(dataset):\n",
    "    backtestP = indicatorP = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        new_indicatorP, new_backtestP = model_1x2.get_backtest_and_indicator_profits(y, outputs)    # Tensor (nBookies * batch), (nBookies * batch)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())\n",
    "    return indicatorP, backtestP\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0):\n",
    "    backtestP = np.array(backtestP, dtype=np.float32)\n",
    "    indicatorP = np.array(indicatorP, dtype=np.float32)\n",
    "    indices = np.array(indices, dtype=np.int32)\n",
    "    stakes = np.array(stakes, dtype=np.float32)\n",
    "    idx = np.argsort(indicatorP)\n",
    "    indicatorP = indicatorP[idx]    # increasing order\n",
    "    backtestP = backtestP[idx]      # hope to be increading order\n",
    "    indices = indices[idx]\n",
    "    stakes = stakes[idx]\n",
    "\n",
    "    ab = []\n",
    "    for w in range(30, int(len(backtestP))):    # 30\n",
    "        back = np.convolve(backtestP, np.ones(w), mode='valid') - threshold\n",
    "        ab += [(back[i], i, i+w-1) for i in range(len(back))]   # [ i : i + w ] : w points. inclusive boundaries.\n",
    "    ab = [(p, a, b) for (p, a, b) in ab if p > 0]   # a, b : inclusive\n",
    "    ab.sort(reverse=False)  # False !   The larger the profit, the earlier it comes in ab.\n",
    "    AB = ab.copy()\n",
    "    for i in range(len(ab)):   # less valuable intervals are screened first.\n",
    "        interval_i = ab[i];  P, A, B = interval_i    # sure A < B\n",
    "        intersects = False\n",
    "        for j in range(i+1, len(ab)):\n",
    "            interval_j = ab[j]; p, a, b = interval_j    # sure a < b\n",
    "            if not (b < A or B < a): intersects = True; break\n",
    "        if intersects:  AB.remove(interval_i)   # interval_i is unique in AB\n",
    "    AB.sort(reverse=True); AB = AB[:5]\n",
    "    print(\"AB: \", AB)\n",
    "    [interval_profit, idx_a, idx_b] = AB[0]     # idx_a, idx_b : inclusive\n",
    "    keyPairs = [[indicatorP[a], indicatorP[b]] for (B, a, b) in AB]\n",
    "    (key_a, key_b) = keyPairs[0]\n",
    "\n",
    "    return indicatorP, backtestP, indices, stakes, interval_profit, key_a, key_b, idx_a, idx_b\n",
    "\n",
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def inference_step(x, odds, interval_a, interval_b):\n",
    "    stake_p = model_1x2(x, training=False)    # (nBookies, batch, nQueries)\n",
    "    nQueries = stake_p.shape[-1]\n",
    "    profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "    bet_bool = interval_a <= profit_p and profit_p >= interval_b    # (nBookies, batch)\n",
    "    bet_bool = tf.stack([bet_bool] * nQueries, axis=-1) # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.math.multiply(stake_p, tf.cast(bet_bool, dtype=tf.float32))   # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.reshape(stake_vector, [1, 0, 2])  # (batch, nBookies, nQueries)\n",
    "    return stake_vector\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def inference_with_dataset(dataset, interval_a, interval_b): \n",
    "    vectors = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)\n",
    "        stake_vector = inference_step(x, odds, interval_a, interval_b)  # (batch, nBookies, nQueries)\n",
    "        vectors.append(stake_vector)\n",
    "    \n",
    "    stake_vectors = tf.concat(vectors, axis=0)   # (batch, nBookies, nQueries)\n",
    "    return stake_vectors    # (batch, nBookies, nQueries)\n",
    "\n",
    "def get_dataset_backtest(model, dataset):\n",
    "    backtestP = indicatorP = indices = stakes = []\n",
    "\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model(x, training=False)\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        new_indicatorP, new_backtestP, new_indices, new_stakes = model.get_batch_backtest(y, outputs)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())  # + [1.2, ...], len = nBookies * batch\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())     # + [1.2, ...], len = nBookies * batch\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ...], len = nBookies * batch. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # + [[0, 1000123], ..., [[bookie, gameId]]], len = nBookies * batch\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes    # + [[sHWin, sDraw, sAWin], ...], len = nBooks * batch\n",
    "\n",
    "    indicatorP_permuted, backtestP_permuted, indices_permutated, stakes_permutated, interval_profit, interval_a, interval_b, idx_a, idx_b = \\\n",
    "        get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0)  # idx_a, idx_b : inclusive\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, list(stake), profit) for (bookie, gameId), stake, profit in \\\n",
    "        zip(indices_permutated[idx_a:idx_b+1], stakes_permutated[idx_a:idx_b+1], backtestP_permuted[idx_a:idx_b+1])]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "\n",
    "    return interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b\n",
    "\n",
    "class Adam_exponential(tf.keras.optimizers.Adam):\n",
    "    def __init__(self, initial_step, starting_rate, ex_step, ex_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-12):\n",
    "        self.step = tf.Variable(initial_step, dtype=tf.float32, trainable=False)\n",
    "        learning_rate = tf.compat.v1.train.exponential_decay(starting_rate, self.step, ex_step, ex_rate/starting_rate, staircase=False)\n",
    "        super().__init__(learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'initial_profit': -float('inf'), 'loss': [], 'val_loss_0': [], 'val_loss': [], 'learning_rate': [], 'recall': [], 'precision': [], 'time_taken': [],  'gauge': [], 'backtest_reports': []}\n",
    "    def set_initial_interval_profit(self, initail_inverval_profit):\n",
    "        self.history['initial_profit'] = initail_inverval_profit\n",
    "        self.save()\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.history? ---------------------- Check it.\n",
    "        self.__init__(self.filepath)\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = data_helpers.LoadJsonData(self.filepath)\n",
    "        if history is not None: self.history = history\n",
    "\n",
    "    def append(self, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, gauge, backtest_report):\n",
    "        self.history['loss'].append(self.round_sig(train_loss, 4))\n",
    "        self.history['val_loss_0'].append(self.round_sig(val_loss_0, 4))\n",
    "        self.history['val_loss'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['learning_rate'].append(learning_rate)\n",
    "        self.history['recall'].append(self.round_sig(recall, 4))\n",
    "        self.history['precision'].append(self.round_sig(precision, 4))\n",
    "        self.history['time_taken'].append(time_taken)\n",
    "        self.history['gauge'].append(gauge)\n",
    "        self.history['backtest_reports'].append(backtest_report)\n",
    "        self.save()\n",
    "\n",
    "    def get_zipped_history(self):\n",
    "        z = zip(self.history['loss'], self.history['val_loss_0'], self.history['val_loss'], self.history['learning_rate'], self.history['recall'], self.history['precision'], self.history['time_taken'])\n",
    "        return list(z)\n",
    "    # def append_backtests(self, epoch, key_a, key_b, interval_profit, backtest):\n",
    "    #     self.history['backtests'].append((epoch, key_a, key_b, interval_profit, backtest))\n",
    "    #     self.save()\n",
    "    def get_backtest_report(self, epoch):\n",
    "        return self.history['backtest_reports'][epoch]     # sure exists. epoch is selected.\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss_0'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['recall'])\n",
    "        assert len(self.history['loss']) == len(self.history['precision'])\n",
    "        assert len(self.history['loss']) == len(self.history['learning_rate'])\n",
    "        assert len(self.history['loss']) == len(self.history['time_taken'])\n",
    "        assert len(self.history['loss']) == len(self.history['gauge'])\n",
    "        assert len(self.history['loss']) == len(self.history['backtest_reports'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "    def get_max_gauge(self, epoch):\n",
    "        gauges = self.history['gauge']\n",
    "        return -float('inf') if (len(gauges) <= 0 or epoch <= 0) else max(gauges[:epoch])\n",
    "    def replace_gauge(self, epoch, gauge):\n",
    "        self.history['gauge'][epoch] = gauge;   self.save()\n",
    "    def show(self, ax):\n",
    "        ax.set_title(TEST_ID + \": loss history\")\n",
    "        ax.plot(self.history['loss'])\n",
    "        ax.plot(self.history['val_loss_0'])\n",
    "        ax.plot(self.history['val_loss'])\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch', loc='right')\n",
    "        ax.legend(['train_loss', 'val_loss_0', 'val_loss'], loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recall():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):    # (batch,)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    labeled_positives = tf.math.reduce_sum(label, axis=None)\n",
    "    recall = hit_positives / (labeled_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    predicted_positives = tf.math.reduce_sum(pred, axis=None)\n",
    "    precision = hit_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function    # Removing this decoration leads to GPU OOM!!!\n",
    "def train_step(model, optimizer, x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value, mean_profit_per_game = model.loss(y, outputs)    # (), (batch,)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    del tape    # new\n",
    "    return loss_value, mean_profit_per_game  # (), (batch,)\n",
    "\n",
    "@tf.function\n",
    "def find_loss_for_step(model, x, y):\n",
    "    outputs = model(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    # print('2', y.shape, outputs.shape)\n",
    "    loss_value, mean_profit_per_game = model.loss(y, outputs)\n",
    "    return loss_value, mean_profit_per_game\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def find_loss_for_dataset(model, ds_batches):\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32, trainable=False)   # The creation of tensor inside a tf function may be the problem.\n",
    "    n = 0\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        n += 1\n",
    "        loss, _ = find_loss_for_step(model, x, y)\n",
    "        val_loss = val_loss * (n-1) / n + loss / n   ###\n",
    "    return val_loss\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def find_recall_precision(model, ds_batches):\n",
    "    recall_object.reset(); precision_object.reset()\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); label = value\n",
    "        pred = model(x)\n",
    "        pred = tf.cast(pred > 0.5, dtype=tf.int32)\n",
    "        recall_object.update(label, pred); precision_object.update(label, pred)\n",
    "    return recall_object.result(), precision_object.result()\n",
    "\n",
    "def replace_baseLabel_with_profitable(model, ds_batches, threshold=0.0001):     # ds_batches: either train_batches or valid_batches\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = base_label\n",
    "            # print('3', sequence.shape, base_bb.shape, baseDateDetails.shape, mask.shape, y.shape)\n",
    "            _, mean_profit_per_game = find_loss_for_step(model, x, y)  # (), (batch,)\n",
    "            profitable = tf.cast(mean_profit_per_game >= threshold, dtype=tf.int32)\n",
    "            # print('1', baseId.shape, sequence.shape, base_bb.shape, baseDateDetails.shape, mask.shape, profitable.shape)\n",
    "            for i in range(baseId.shape[0]):\n",
    "                # create a new dataset format, to which to apply apply_train/test_pipeline_c(.). Note it's a different pipeline.\n",
    "                yield (baseId[i], sequence[i], base_bb[i], baseDateDetails[i], mask[i], profitable[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS)), tf.TensorShape(())),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def select_profitable_items(model, ds_batches, threshold=0.5):  # ds_batches: either train_batches or valid_batches\n",
    "    def generator():\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask)\n",
    "            output = model(x)   # (batch, ...)\n",
    "            for i in range(output.shape[0]):\n",
    "                if output[i] >= threshold:\n",
    "                    # back to the origitnal dataset format, to which to apply apply_train/test_pipeline(.)\n",
    "                    yield (baseId[i], sequence[i], base_bb[i], base_label[i], baseDateDetails[i], mask[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS))),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def analyze(model, ds_batches, checkpointPath):\n",
    "    model.load_weights(checkpointPath)\n",
    "    interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b = get_dataset_backtest(model, ds_batches)\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "    axes[0].plot(indicatorP_permuted); axes[0].grid(True)\n",
    "    axes[1].plot(backtestP_permuted); axes[1].grid(True)\n",
    "    w = 50; idx_1 = idx_a - int(w/2); idx_2 = idx_b - int(w/2)\n",
    "    back = np.convolve(np.array(backtestP_permuted), np.ones(w), mode='valid') / w\n",
    "    axes[2].plot(back, lw=0.5); axes[2].plot([idx_1, idx_1], plt.ylim(), 'r', lw=0.5); axes[2].plot([idx_2, idx_2], plt.ylim(), 'r', lw=0.5); axes[2].grid(True)\n",
    "    axes[3].plot(indicatorP_permuted, backtestP_permuted); axes[3].grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_interval_trajectory(model, dataset, checkpointPath, chunk_size_pct, step_counts):\n",
    "    length = len(dataset)\n",
    "    assert step_counts > 1\n",
    "    assert chunk_size_pct * step_counts >= 100\n",
    "    chunk_size = int(length * chunk_size_pct / 100)\n",
    "    step_size = int((length - chunk_size) / (step_counts - 1))\n",
    "    intervals = []\n",
    "    model.load_weights(checkpointPath)\n",
    "    for step in range(step_counts):\n",
    "        chunk_a = step_size * step\n",
    "        ds = dataset.skip(chunk_a)\n",
    "        ds = ds.take(chunk_size)\n",
    "        ds_batches = apply_test_pipeline(ds)\n",
    "        interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b = get_dataset_backtest(model, ds_batches)\n",
    "        intervals.append((interval_a, interval_b, interval_profit))\n",
    "    return intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_epoch(epoch, batch_id_seen, sub_epoch_loss, epoch_loss, samples_seen, learning_rate):\n",
    "    print(\"epoch: {}, batchId_seen: {}, sub_loss: {:.6}, loss: {:.6f}, samples_seen: {}, learning_rate: {:.5e}                  \".\n",
    "            format(epoch, batch_id_seen, float(sub_epoch_loss), float(epoch_loss), samples_seen, learning_rate), end='\\r')\n",
    "\n",
    "def accumulate(backtests, compound=False):\n",
    "    initial = 1.0; Div = 3\n",
    "    sum = initial; minS = sum; maxS = sum\n",
    "    for (bookie, gameId, stake, profit) in backtests:\n",
    "        sum_stake = 0.0\n",
    "        for s in stake: sum_stake += s\n",
    "        sum += (profit / sum_stake * ( sum if compound else initial*sum_stake/Div))\n",
    "        if sum < minS: minS = sum\n",
    "        if sum > maxS: maxS = sum\n",
    "        if sum < 0.2: break\n",
    "        # if sum > initial * 2: initial = sum   # this is a step-wise compound. as risky as simple compound.\n",
    "    return sum, minS, maxS\n",
    "\n",
    "def run_backtest(epoch, model, history, playback=False):     # Read-only\n",
    "    #-------------------- make a backtest virtually/actually\n",
    "    A_ds = valid_batches; len_A_ds = len(valid_ds)\n",
    "    B_ds = test_batches; len_B_ds = len(test_ds)\n",
    "\n",
    "    if playback:\n",
    "        interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests = \\\n",
    "            history.get_backtest_report(epoch)\n",
    "    else:   # idx_a, idx_b : inclusive\n",
    "        print(\"Wait...\", end='')\n",
    "        A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b = get_dataset_backtest(model, A_ds)\n",
    "        B_backtests = backtest_event_wise_with_dataset(B_ds, model, interval_a, interval_b)\n",
    "\n",
    "    # We have A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b, B_backtests\n",
    "\n",
    "    print(\"iterval_a: {:.8f}, interval_b: {:.8f}, idx_a: {}, idx_b: {}, A_interval_profit: {:.4f}\".format(interval_a, interval_b, idx_a, idx_b, A_interval_profit))\n",
    "    \n",
    "    #---- validation/verification\n",
    "    A_len_backtest = len(A_backtests)\n",
    "    B_len_backtest = len(B_backtests)\n",
    "    A_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in A_backtests]))\n",
    "    B_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in B_backtests]))\n",
    "    A_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in A_backtests]))\n",
    "    B_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in B_backtests]))\n",
    "    A_final_capital, A_bottom_capital, A_top_capital = accumulate(A_backtests, compound=False)\n",
    "    B_final_capital, B_bottom_capital, B_top_capital = accumulate(B_backtests, compound=False)\n",
    "    A_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in A_backtests]\n",
    "    B_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in B_backtests]\n",
    "\n",
    "    format = \"len_backtest/len_dataset: {}/{}, accProfit/accStake: {:.4f}/{:.4f}, capital (final/bottom/top): {:.4f}/{:.4f}/{:.4f}\"\n",
    "    A_format = \"A_test:: \" + format; B_format = \"B_test:: \" + format\n",
    "    print(A_format.format(A_len_backtest, len_A_ds, A_acc_profit, A_acc_stake, A_final_capital, A_bottom_capital, A_top_capital))\n",
    "    print(B_format.format(B_len_backtest, len_B_ds, B_acc_profit, B_acc_stake, B_final_capital, B_bottom_capital, B_top_capital))\n",
    "    print(\"A_backtest: \", A_backtestsToShow[:20])\n",
    "    print(\"B_backtest: \", B_backtestsToShow[:20])\n",
    "\n",
    "    backtest_report = (interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests)\n",
    "    gauge = B_acc_profit\n",
    "\n",
    "    return backtest_report, gauge\n",
    "\n",
    "# conclude_train_epoch(-1, 0, 0, 0, 0, playback=False)\n",
    "def conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, playback=False, gaugeTerm='val_loss'):\n",
    "    global focus_interval_id, focus_scores, focus_back, focus_valid, focus_test\n",
    "    print(\"epoch: {}, train_loss: {:.9f}, val_loss_0: {:.9f}, val_loss: {:.9f}, learning_rate: {:.5e}, recall/precision: {:.4f}/{:.4f}, time_taken: {:.1f} m\".format(epoch, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken))\n",
    "\n",
    "    backtest_report = ()\n",
    "    if gaugeTerm == 'val_loss': gauge = - val_loss\n",
    "    elif gaugeTerm == 'recall': gauge = recall\n",
    "    elif gaugeTerm == 'precision': gauge = precision\n",
    "\n",
    "    # backtest_report, gauge = run_backtest(epoch, model, history, playback)   #-------------------------------------------------\n",
    "\n",
    "    upgraded = False\n",
    "    #-------------------- get interval scores from arguments, create the best checkpoint if we have a highest ever score.\n",
    "    if gauge > history.get_max_gauge(epoch):\n",
    "        upgraded = True\n",
    "        # focus_interval_id, focus_scores, focus_back, focus_valid, focus_test = bets_interval_id, interval_scores, best_interval_back, best_interval_valid, best_interval_test\n",
    "        if not playback: model.save_weights(checkpointPathBest)\n",
    "        print(\"----------------------------------------------------------------------------------------------------- best checkpoint updated\")\n",
    "    if playback: history.replace_gauge(epoch, gauge)\n",
    "\n",
    "    #--------------------- Save, finally\n",
    "    if not playback:\n",
    "        model.save_weights(checkpointPath)\n",
    "        # self, train_loss, val_loss, learning_rate, recall, precision, time_taken, gauge, backtest_report\n",
    "        history.append(train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, gauge, backtest_report)\n",
    "\n",
    "    return upgraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history.json')\n",
    "history = history_class(historyPath); history.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 2))\n",
    "history.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "model_1x2 = create_model_object(Model_1X2)\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken in history.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='val_loss')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def train(epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False, gaugeTerm='val_loss', get_val_loss_0=False):\n",
    "    epochs = epochs; seen_upgrades = 0\n",
    "    for epoch in range(history.len(), history.len() + epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer.step.assign(history.len()); learning_rate = optimizer.lr.numpy()\n",
    "        n = 0; loss = tf.Variable(0.0, dtype=tf.float32, trainable=False); samples_seen = 0\n",
    "        m = 0; train_loss = 0.0\n",
    "        set_seed(epoch)        \n",
    "        train_batches = apply_train_pipeline(train_ds)      # the pipeline includes suffling.\n",
    "        for batch_id, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(train_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "            samples_seen += sequence.shape[0]\n",
    "            batch_loss, _ = train_step(model, optimizer, x, y)  # (), (batch,)\n",
    "            n += 1; loss = loss * (n-1)/n + batch_loss/n\n",
    "            m += 1; train_loss = train_loss * (m-1)/m + batch_loss/m\n",
    "            if batch_id % 50 == 0: show_sub_epoch(epoch, batch_id, loss, train_loss, samples_seen, learning_rate); n = 0; loss = 0.0\n",
    "\n",
    "        show_sub_epoch(epoch, batch_id, loss, train_loss, samples_seen, learning_rate)  # closing show\n",
    "        val_loss_0 = 1.0\n",
    "        if get_val_loss_0: val_loss_0 = find_loss_for_dataset(model, train_batches)\n",
    "        val_loss = find_loss_for_dataset(model, valid_batches)\n",
    "\n",
    "        recall = precision = -1.0\n",
    "        if get_f1: recall, precision = find_recall_precision(model, valid_batches); recall = recall.numpy(); precision = precision.numpy()\n",
    "        \n",
    "        # epoch, model, history, checkpointPath, checkpointPathBest, train_loss, val_loss, learning_rate, recall, precision, time_taken, playback=False\n",
    "        upgraded = conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, float(train_loss), float(val_loss_0), float(val_loss), learning_rate, recall, precision, (time.time()-start_time)/60, playback=False, gaugeTerm=gaugeTerm)\n",
    "        if upgraded: seen_upgrades += 1\n",
    "        if seen_upgrades >= nUpgrades: break\n",
    "\n",
    "    return seen_upgrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'PRETRAIN': # model_1x2 is trained to find profitable staking vectors for games, although average profitability is far below zero.\n",
    "    model_1x2 = create_model_object(Model_1X2)\n",
    "    try:    model_1x2.load_weights(checkpointPath).expect_partial(); print(\"model_1x2 loaded its previous checkpoint.\")\n",
    "    except: print(\"model_1x2 loaded its initial weights.\")    # The raw model_1x2 itself is the starting point.\n",
    "    optimizer = Adam_exponential(history.len(), STARTING_LEARNING_RATE, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.99, epsilon=1e-12)\n",
    "    # Goal: get checkpointPathBest checkpoint, which is used as the starting checkpoint of both TRAIN_C and FINETUNE.\n",
    "    nEpochs = 300; wanted_upgrades = 60\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_1x2, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1 = False, gaugeTerm='val_loss', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "    \n",
    "    OPERATION = 'TRAIN_C'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(model_1x2, test_batches, checkpointPathBest)    # baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_valid = back_ds.concatenate(valid_ds)\n",
    "intervals = get_interval_trajectory(model_1x2, back_valid, checkpointPathBest, chunk_size_pct=25, step_counts=4)\n",
    "intervals_a = [a for (a, _, _) in intervals]\n",
    "intervals_b = [b for (_, b, _) in intervals]\n",
    "intervals_profit = [p for (_, _, p) in intervals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def find_extrapolation(sequence, tail_data_len):\n",
    "    assert tail_data_len <= len(sequence)\n",
    "    x = np.array([[i] for i in range(tail_data_len)], dtype=np.float32)\n",
    "    y = np.array(sequence[- tail_data_len :], dtype=np.float32)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    extrapolation = model.predict(np.array([[tail_data_len]], dtype=np.float32))[0]\n",
    "    return extrapolation\n",
    "\n",
    "intervals_a.append(find_extrapolation(intervals_a, 2))\n",
    "intervals_b.append(find_extrapolation(intervals_b, 2))\n",
    "\n",
    "plt.plot(intervals_a, label='a')\n",
    "plt.plot(intervals_b, label='b')\n",
    "plt.plot(intervals_profit,label='p')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = intervals_a[-1]; b = intervals_b[-1]\n",
    "backtests = backtest_event_wise_with_dataset(test_batches, model_1x2, a, b)  # [(bookie, gameId, [stakeHomeWin, stakeDraw, stakeAwayWin], profit), ...]\n",
    "sum, minS, maxS = accumulate(backtests, compound=False)\n",
    "print('sum, minS, maxS: ', sum, minS, maxS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(backtests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_c')\n",
    "checkpointPathBest_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best_c')\n",
    "historyPath_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history_c.json')\n",
    "history_c = history_class(historyPath_c); history_c.load()\n",
    "\n",
    "model_1x2 = create_model_object(Model_1X2)\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken in history_c.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='precision')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 2))\n",
    "history_c.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'TRAIN_C':  # model_c is trained to find games for which model_1x2 can can find positively profitable staking vector.\n",
    "\n",
    "    def X_versus_Y_c(baseId, sequence, base_bb, baseDateDetails, mask, profitable):\n",
    "        return (baseId, sequence, base_bb, baseDateDetails, mask), (profitable)\n",
    "    def apply_train_pipeline_c(ds):\n",
    "        return (ds.shuffle(BUFFER_SIZE).batch(train_batch_size).map(X_versus_Y_c, tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "    def apply_test_pipeline_c(ds):\n",
    "        return (ds.shuffle(BUFFER_SIZE).batch(test_batch_size).map(X_versus_Y_c, tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "    def create_replace_baseLabel_with_profitable(path, model, ds_batches, threshold=0.0001):     \n",
    "        if os.path.exists(path):  ds = tf.data.Dataset.load(path)\n",
    "        else: ds = replace_baseLabel_with_profitable(model, ds_batches, threshold=threshold); tf.data.Dataset.save(ds, path);  ds = tf.data.Dataset.load(path)\n",
    "        return ds\n",
    "\n",
    "    model_1x2 = create_model_object(Model_1X2)\n",
    "    model_1x2.load_weights(checkpointPathBest)    # finetuning model_1x2 starts from the best pretrained model_1x2.\n",
    "    REPLACE_THRESHOLD = 0.00005\n",
    "    path_train_ds_with_profitable = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-train_ds_with_profitable')\n",
    "    train_ds_with_profitable = create_replace_baseLabel_with_profitable(path_train_ds_with_profitable, model_1x2, train_batches, threshold=REPLACE_THRESHOLD) # train_batches\n",
    "    print(\"train len: \", len(train_ds_with_profitable))\n",
    "    path_valid_ds_with_profitable = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-valid_ds_with_profitable')\n",
    "    valid_ds_with_profitable = create_replace_baseLabel_with_profitable(path_valid_ds_with_profitable, model_1x2, valid_batches, threshold=REPLACE_THRESHOLD) # valid_batches\n",
    "    print(\"valid len: \", len(valid_ds_with_profitable))\n",
    "    valid_batches_with_profitable = apply_test_pipeline_c(valid_ds_with_profitable)\n",
    "\n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    try: model_c.load_weights(checkpointPath_c); print(\"model_c loaded its previous checkpoint.\")\n",
    "    except: model_c.load_weights(checkpointPathBest); print(\"model_c loaded model_1x2's best pretrain checkpoint\")\n",
    "    # Goal: get checkpointPathBest_c checkpoint, which is used to filter datasets in favor of model_1x2\n",
    "    optimizer_c = Adam_exponential(history.len(), STARTING_LEARNING_RATE/5, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nEpochs = 20; wanted_upgrades = 5\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_c, optimizer_c, history_c, checkpointPath_c, checkpointPathBest_c, train_ds_with_profitable, valid_batches_with_profitable, apply_train_pipeline_c, get_f1=True, gaugeTerm='precision', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "\n",
    "    OPERATION = 'FINETUNE'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_f')\n",
    "checkpointPathBest_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best_f')\n",
    "historyPath_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history_f.json')\n",
    "history_f = history_class(historyPath_f); history_f.load()\n",
    "\n",
    "model_1x2 = create_model_object(Model_1X2)\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken in history_f.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_loss, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='val_loss')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_for_profitable_items(path, model, ds_batches, threshold=0.5):\n",
    "    if path is None: ds = select_profitable_items(model, ds_batches, threshold=threshold)\n",
    "    else:\n",
    "        if os.path.exists(path):  ds = tf.data.Dataset.load(path)\n",
    "        else: ds = select_profitable_items(model_c, ds_batches, threshold=threshold); tf.data.Dataset.save(ds, path);  ds = tf.data.Dataset.load(path)\n",
    "    return ds\n",
    "\n",
    "if OPERATION == 'FINETUNE': # model_1x2 is trained to find profitable staking vectors for games that are filtered in by model_c.\n",
    "   \n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "    FILTER_THRESHOLD = 0.5\n",
    "    path_train_ds_filtered = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-train_ds_filtered')\n",
    "    train_ds_filtered = filter_dataset_for_profitable_items(path_train_ds_filtered, model_c, train_batches, threshold=FILTER_THRESHOLD)    # train_batches\n",
    "    print(\"train len: \", len(train_ds_filtered))\n",
    "    path_valid_ds_filtered = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-valid_ds_filtered')\n",
    "    valid_ds_filtered = filter_dataset_for_profitable_items(path_valid_ds_filtered, model_c, valid_batches, threshold=FILTER_THRESHOLD)    # valid_batches\n",
    "    print(\"valid len: \", len(valid_ds_filtered))\n",
    "    valid_batches_filtered = apply_test_pipeline(valid_ds_filtered)\n",
    "\n",
    "    model_1x2 = create_model_object(Model_1X2)\n",
    "    try: model_1x2.load_weights(checkpointPath_f); print(\"model_1x2 loaded its previous checkpoint.\")\n",
    "    except: model_1x2.load_weights(checkpointPathBest); print(\"model_1x2 loaded its best pretrain checkpoints.\")\n",
    "    # Goal: get checkpointPathBest_f checkpoint, which is used as our final model. The final model needs the dataset filter model_c as seen in 'TEST' operation.\n",
    "    optimizer_f = Adam_exponential(history.len(), STARTING_LEARNING_RATE/10, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nEpochs = 10; wanted_upgrades = 3\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_1x2, optimizer_f, history_f, checkpointPath_f, checkpointPathBest_f, train_ds_filtered, valid_batches_filtered, apply_train_pipeline, get_f1 = False, gaugeTerm='val_loss', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "\n",
    "    OPERATION = 'TEST'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'TEST': # model_1x2, after pretrained and finetuned, is tested on a dataset that is filered in by model_c.\n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "\n",
    "    FILTER_THRESHOLD = 0.5\n",
    "    ds_filtered = filter_dataset_for_profitable_items(None, model_c, test_batches, threshold=0.5)\n",
    "    ds_filtered_batches = apply_test_pipeline(ds_filtered)\n",
    "    analyze(model_1x2, ds_filtered_batches, checkpointPathBest_f)  # compare with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_a = 0.7    # Choose onefiltered_test_ds\n",
    "interval_b = 0.8    # Choose one\n",
    "\n",
    "if not TRAIN_MODE:      # just sketch\n",
    "    # df_grown gets to be the existing df_grown, because the train_mode if False.\n",
    "    df_total, df_new = df_grown, df_new = data_helpers.get_grown_and_new_from_football_data(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, train_mode = TRAIN_MODE, skip=False)\n",
    "    df_white = df_new\n",
    "    df_black = data_helpers.read_excel(path)\n",
    "\n",
    "    df_total = df_total; df_search = df_new\n",
    "    additional_id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, df_total, df_search, chooseDivs=chooseDivs)\n",
    "    \n",
    "    ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset-inference')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_total, additional_id_to_ids, tokenizer_team, normalization_parms, train_mode=False)  #-----------------                                                                          #-----------------\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    ds_inference = apply_test_pipeline(ds)\n",
    "    stake_vectors = inference_with_dataset(ds_inference, interval_a, interval_b) # (batch, nBookies, nQueries)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
