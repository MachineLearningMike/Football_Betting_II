{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist\n",
    "1. Make sure your \"Regional Format\" is set to \"English (United Kingdom)\" before Excel opens files that contain English date-like objects,\n",
    "   if you want them to be parsed as English datetime. English (United States) regional format will convert them to American Datetime as possible.\n",
    "   You have no way to prevent Excel from converting date-like strings or objects to datetime when opening a file. \n",
    "\n",
    "    DECODE_BASE_DATE = False\n",
    "    \n",
    "    softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from config import config\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "OPERATION = 'PRETRAIN'  # 'PRETRAIN'  'TRAIN_C'   'FINETUNE'   'TEST'\n",
    "\n",
    "TEST_ID = 'SM.13'    \n",
    "BASE_TEST_ID = ''  # should be either '' or an existing TEST_ID.\n",
    "DATA_BITS = 32              #------------------ 16 doesn't work.\n",
    "\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0\n",
    "TRAIN_PERCENT= 94   # chronically the earliest part over the databased past rows.  In the end of this part, there is BACK part with the same size as VALID part.\n",
    "VALID_PERCENT = 4   # chronically the next part to train part over the databased past rows\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT # chronically the last part over the databased past rows. This part will be added with post-database rows, either past or known coming\n",
    "\n",
    "BUFFER_SIZE = 35000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "TEAM_EMBS = 50  #\n",
    "DECODE_BASE_DATE = False\n",
    "EMBED_AB_COLS = True    # True: Pls choose a small LR so that we have plenty of train epochs and the embedded values have enough chance to seek their proper places.\n",
    "ADDITIONAL_D_MODEL = 0   #------------ Try increase it when underfitting.\n",
    "\n",
    "TRANSFORMER_LAYERS = 6\n",
    "TRANSFORMER_HEADS = 60\n",
    "DROPOUT = 0.0\n",
    "ADAPTORS_LAYERS = 10\n",
    "\n",
    "# This is an exponential curve that hits STARTING_LEARNING_RATE at step zero and EXAMPLE_LEARNING_RATE at step EXAMPLE_LEARNING_STEP.\n",
    "# lr(step) = STARTING_LEARNING_RATE * pow( pow(EXAMPLE_LEARNING_RATE/STARTING_LEARNING_RATE, 1/EXAMPLE_LEARNING_STEP), step )\n",
    "STARTING_LEARNING_RATE = 2e-6\n",
    "EXAMPLE_LEARNING_RATE = STARTING_LEARNING_RATE * 0.1\n",
    "EXAMPLE_LEARNING_STEP = 30  \n",
    "\n",
    "INTERVAL_VALUE_BOTTOM = -150\n",
    "INTERVAL_VALUE_TOP = 20\n",
    "\n",
    "RESET_HISTORY = False\n",
    "MIN_PROFIT = -1.0\n",
    "VECTOR_BETTING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = 'England'\n",
    "NUMBER_BOOKIES = (3-1)  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "BOOKIE_TO_EXCLUDE = ['BW']    # 'BWIN' odds don't show up since mid Febrary 2025. This may reduce the effective NUMBER_BOOKIES.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds makers are not archived for the league since 2013.\n",
    "\n",
    "\"\"\"\n",
    "#-------------------- England ---------------------\n",
    "Time range of data: 2004/2005 - 2024/2025\n",
    "Leagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\n",
    "!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\"\"\"\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For serendipity\n",
    "seed = 23\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "\n",
    "# Make sure Odds_cols comes first !!!\n",
    "if EMBED_AB_COLS: _Cols_to_Standardize = Odds_cols\n",
    "else: _Cols_to_Standardize = Odds_cols + AB_cols\n",
    "\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BBAB_cols)\n",
    "print(BB_cols)\n",
    "print(_Label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_Cols_to_Standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_size():\n",
    "    size = 0\n",
    "    for cols in _Cols_to_Standardize:\n",
    "        size += len(cols)\n",
    "    return size\n",
    "\n",
    "def get_standardization_params(df_grown):\n",
    "    \n",
    "    def get_mean_and_std(col):\n",
    "        array = df_grown[col]\n",
    "        array = np.array(array)\n",
    "        return (array.mean(), array.std(), np.max(array))\n",
    "    \n",
    "    params = {}\n",
    "    for col in _Cols_to_Standardize:\n",
    "        params[col] = get_mean_and_std(col)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def check_standardization(bbab, std_params):\n",
    "    for col in _Cols_to_Standardize:\n",
    "        start = BBAB_cols.index(col)\n",
    "        (mean, std, maximum) = std_params[col]\n",
    "        if -mean/std > bbab[start] + 1e-5:\n",
    "            print('standardization error 1', col, -mean/std, bbab[start])\n",
    "        if max(bbab[start]) > (maximum - mean) / std + 1e-5:\n",
    "            print('standardization error 2', col, bbab[start], (maximum - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/\" + COUNTRY\n",
    "data_helpers.assign_seasonal_filenames(countryDirPath)\n",
    "df_grown, df_new = data_helpers.get_grown_and_new_from_football_data(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, train_mode = TRAIN_MODE, skip=True)\n",
    "if df_grown is not None: print(\"df_grown: \", df_grown.shape)\n",
    "if df_new is not None: print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "def createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens):\n",
    "        tokenizer = Tokenizer(models.WordLevel(unk_token=unknown_token))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "        tokenizer.train(corpus_files, trainer=trainer)\n",
    "        tokenizer.decoder = decoders.WordPiece(prefix=\" \")\n",
    "        return tokenizer\n",
    "\n",
    "def creat_team_tokenizer_uk(df_grown):\n",
    "    teams = list(set(list(df_grown['HomeTeam']) + list(df_grown['AwayTeam'])))\n",
    "    teams_string = [str(team) for team in teams]\n",
    "    teams_string = [re.sub(r\"\\s\", \"_\", item) for item in teams_string]    # replace spaces with a '_'\n",
    "    teams_text = \" \".join(teams_string)\n",
    "\n",
    "    corpus_file = os.path.join(countryDirPath, '_tokenizers', 'team_ids_text_uk.txt')\n",
    "    f = open(corpus_file, \"w+\", encoding=\"utf-8\")\n",
    "    f.write(teams_text)\n",
    "    f.close()\n",
    "\n",
    "    corpus_files = [corpus_file]\n",
    "    unknown_token = config['unknown_token']\n",
    "    special_tokens = [unknown_token] ################### + [\"[HOME]\", \"[AWAY]\"]\n",
    "    vocab_size = len(teams_string) + len(special_tokens)\n",
    "\n",
    "    tokenizer_team = createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)\n",
    "    return tokenizer_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df_grown)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= build_id_to_ids.ipynb\n",
    "\n",
    "QUALITY = 75\n",
    "\n",
    "targetLength = 300          # A target game will be explained by maximum 'targetLength' count of past games.\n",
    "minCurrent = 1e-7\n",
    "sinceDaysAgo = 365 * 20     # A target game will be explained by past games that took place since 'sinceDaysAgo' days ago. \n",
    "qualityPct = QUALITY        #\n",
    "conductance365 = 0.9        # 0.9 comes from test.\n",
    "chooseDivs=False\n",
    "\n",
    "id_to_ids_filename = str(targetLength) + '-' + str(minCurrent) + '-' + str(sinceDaysAgo) + '-' + str(qualityPct) + '-' + str(conductance365) + '-' + str(chooseDivs)\n",
    "id_to_ids_filePath = os.path.join(countryDirPath, '_id_to_ids', id_to_ids_filename + \".json\")\n",
    "\n",
    "id_to_ids_existing = None\n",
    "id_to_ids_existing = data_helpers.LoadJsonData(id_to_ids_filePath)\n",
    "tf_total = df_grown; df_search = df_grown\n",
    "id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, tf_total, df_search, chooseDivs=chooseDivs)\n",
    "data_helpers.SaveJsonData(id_to_ids, id_to_ids_filePath)\n",
    "\n",
    "print(len(id_to_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_raw_bbab(bbab, tokenizer_team, std_params, train_mode=True):\n",
    "\n",
    "    label = []\n",
    "    if train_mode:\n",
    "        #---------------------- label, before changing bbab\n",
    "        for col in _Label_cols:\n",
    "            start = BBAB_cols.index(col)\n",
    "            label.append(bbab[start])\n",
    "\n",
    "    #----------------------- \n",
    "    start = BBAB_cols.index(Div_cols[0])\n",
    "    Div = bbab[start]\n",
    "    bbab[start] = DIVIISONS.index(Div)  # Assumes no n/a\n",
    "\n",
    "    start = BBAB_cols.index(Team_cols[0]); end = BBAB_cols.index(Team_cols[-1]) + 1\n",
    "    pair_str = [str(team) for team in bbab[start : end]]    # Team names are already normalized, removing/striping spaces.\n",
    "    pair_text = \" \".join(pair_str)\n",
    "    pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "    bbab[start : end] = pair_tokens # 0 for Unknown, by tokenizer trainig.\n",
    "\n",
    "    #--------------------- standardize\n",
    "    for col in _Cols_to_Standardize:    #   _Cols_to_Standardize has some of BB_cols.\n",
    "        if not train_mode and col not in BB_cols: continue\n",
    "\n",
    "        (mean, std, maximum) = std_params[col]\n",
    "        # assert 0 <= bbab[start]\n",
    "        start = BBAB_cols.index(col)\n",
    "        bbab[start] = (bbab[start] - mean) / std\n",
    "        # assert - mean/std <= bbab[start]\n",
    "        # assert max(bbab[start : end]) <= (maximum - mean) / std\n",
    "        # print('std', bbab[start : end])\n",
    "\n",
    "    #--------------------- columns for positional embedding\n",
    "    start = BBAB_cols.index(Date_cols[0])   #\n",
    "    date = bbab[start]\n",
    "    bbab[start] = (datetime.datetime.combine(date, datetime.time(0,0,0)) - config['baseDate']).days  # either positive or negative\n",
    "\n",
    "    #---------------------- bb only\n",
    "    start = BBAB_cols.index(BB_cols[0]); end = start + len(BB_cols)     # \n",
    "    bb = bbab[start : end]\n",
    "\n",
    "    return bbab, bb, label, date\n",
    "\n",
    "def getDateDetails(date):\n",
    "    baseYear = config['baseDate'].year\n",
    "    date_details = tf.Variable([date.year - baseYear, date.month, date.day, date.weekday()], dtype=tf.int32)\n",
    "    return date_details     # (4,)\n",
    "\n",
    "filler = tf.zeros_like([0] * len(BBAB_cols), dtype=tf.float32)\n",
    "\n",
    "def get_data_record(df_total, baseId, ids, tokenizer_team, std_params, train_mode=True):\n",
    "    # try:\n",
    "        # base_bbab = list(df_grown.loc[df_grown['id'] == baseId, BBAB_cols])\n",
    "        if train_mode:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BBAB_cols].iloc[0, :])  # base_bbab follows BBAB. list\n",
    "        else:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BB_cols].iloc[0, :])  # base_bbab follows BB. list\n",
    "\n",
    "        base_bbab, base_bb, base_label, base_date = normalize_raw_bbab(base_bbab, tokenizer_team, std_params, train_mode=train_mode)\n",
    "        # base_bbab, base_bb, base_label, base_date\n",
    "        # print('2', base_bbab)\n",
    "        baseId = tf.Variable(baseId, dtype=tf.int32)\n",
    "        base_bbab = tf.Variable(base_bbab, dtype=tf.float32)    # (len(BBAB_cols),)\n",
    "        base_bb = tf.Variable(base_bb, dtype=tf.float32)        # (len(BB_cols),)\n",
    "        base_label = tf.Variable(base_label, dtype=tf.float32)  # (len(_Label_cols),)\n",
    "        # print('3', base_bbab)\n",
    "        # Default sequence.\n",
    "        sequence = tf.transpose(tf.Variable([[]] * len(BBAB_cols), dtype=tf.float32))   # (0, len(BBAB_cols))\n",
    "        # sequence = np.array([[]] * len(BBAB_cols), dtype=config['np_float']).T\n",
    "        # print('3.5', sequence)\n",
    "        baseDateDetails = getDateDetails(base_date) # (4,)\n",
    "\n",
    "        concat = []\n",
    "        for id in ids:\n",
    "            bbab = list(df_total[df_total['id'] == id][BBAB_cols].iloc[0, :])   # bbab follows BBAB. list\n",
    "            # print('4', bbab)\n",
    "            bbab, _, _, _ = normalize_raw_bbab(bbab, tokenizer_team, std_params, train_mode=train_mode)   # bbab follows BBAB. list\n",
    "            # check_standardization(bbab, std_params)\n",
    "\n",
    "            bbab = tf.Variable(bbab, dtype=tf.float32)[tf.newaxis, :]       # (1, len(BBAB_cols))\n",
    "            # _bbab = bbab[0].numpy()\n",
    "            # check_standardization(_bbab, std_params)\n",
    "\n",
    "            concat.append(bbab)     # concat doesn't create a new axis.\n",
    "\n",
    "        if len(concat) > 0:\n",
    "            sequence = tf.concat(concat, axis=0)    # (nSequence, len(BBAB_cols))\n",
    "            # if sequence.shape[0] > 0:\n",
    "            #     bbab = sequence[0].numpy()\n",
    "            #     check_standardization(bbab, std_params)\n",
    "\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            patch = tf.stack([filler] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, patch], axis=0)     # concat doesn't create a new axis. (MAX_TOKENS, len(BBAB_cols))\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, len(BBAB_cols))\n",
    "        baseDateDetails = baseDateDetails[tf.newaxis, :]\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) # (MAX_TOKENS,) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]    # (MAX_TOKENS, MAX_TOKENS)\n",
    "\n",
    "        return (baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org)\n",
    "\n",
    "\n",
    "def generate_dataset_uk(df_total, fixture_id_to_ids, tokenizer_team, std_params, train_mode=True):\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for baseId, (tag, label, ids) in fixture_id_to_ids.items():\n",
    "            baseId = int(baseId)\n",
    "            baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org = get_data_record(df_total, baseId, ids, tokenizer_team, std_params, train_mode=train_mode)\n",
    "            print(\"count: {}, baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "            # if count > 200: break\n",
    "            yield (baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS)), tf.TensorShape(())),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "    \n",
    "   \n",
    "def X_versus_Y(baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org):\n",
    "    return (baseId, sequence, base_bb, baseDateDetails, mask), (base_label)\n",
    "\n",
    "# I found, in PositionalEmbedding, batch size ranges between 3, 4, 6 and 8, while they should be 4 or 8, except margial rows. Check it.\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def apply_train_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)       # Shuffle the training dataset.\n",
    "        .batch(train_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def apply_test_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-normalization' + \".json\")\n",
    "std_params = get_standardization_params(df_grown)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if TRAIN_MODE:\n",
    "#     ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset')\n",
    "\n",
    "#     if os.path.exists(ds_path):\n",
    "#         ds = tf.data.Dataset.load(ds_path)\n",
    "#     else:\n",
    "#         ds = generate_dataset_uk(df_grown, id_to_ids, tokenizer_team, std_params)\n",
    "#         tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "#         ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "#     len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseIds = [int(id) for (id, value) in id_to_ids.items()]\n",
    "print(min(baseIds), max(baseIds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE:\n",
    "    ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "        lastPart = ds.skip(len(ds)-1).take(1)\n",
    "        lastElem = None; \n",
    "        for elem in lastPart: lastElem = elem; break\n",
    "        (lastId, _, base_bb, _, _, _, _) = lastElem\n",
    "        lastId = lastId.numpy(); base_bb = base_bb.numpy()\n",
    "        assert lastId == max(df_grown['id'])\n",
    "        print(\"max_id in the existing dataset is {}\". format(lastId))\n",
    "        # print(\"For your information: base_bb = \", base_bb)\n",
    "        new_id_to_ids = {key: value for (key, value) in id_to_ids.items() if lastId < int(key)}\n",
    "        if len(new_id_to_ids) > 0:\n",
    "            print(\"{} A dataset of new records are being generated...\")\n",
    "            ds_new = generate_dataset_uk(df_grown, new_id_to_ids, tokenizer_team, std_params)\n",
    "            print(\"The dateset is being appended to the exsiting dataset...\")\n",
    "            ds = ds.concatenate(ds_path) # tf.data.Dataset.zip((ds, ds_new))\n",
    "            tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "            ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "        else:\n",
    "            print(\"No new records generated.\")\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_grown, id_to_ids, tokenizer_team, std_params)\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(ds)\n",
    "\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "take_size = total_size - starting_size - ending_size\n",
    "remaining_ds = ds.skip(starting_size)\n",
    "dataset = remaining_ds.take(take_size)          # starting | dataset | ending\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size - valid_size      # len(dataset) = train_size + valid_size + tast_size        NO back_size\n",
    "train_ds = dataset.take(train_size)                    # dataset[: train_size]\n",
    "remaining_ds = dataset.skip(train_size - valid_size)    # dataset[train_size - valid_size: ]\n",
    "back_ds = remaining_ds.take(valid_size)                # dataset[train_size - valid_size: train_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)            # dataset[train_size: ]\n",
    "valid_ds = remaining_ds.take(valid_size)               # dataset[train_size, train_size + valid_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)                # dataset[train_size + valid_size :]\n",
    "test_ds = remaining_ds.take(test_size)\n",
    "backtest_ds = valid_ds.concatenate(test_ds) # tf.data.Dataset.zip((valid_ds, test_ds))\n",
    "\n",
    "assert len(test_ds) == test_size\n",
    "assert dataset_size == len(train_ds) + len(valid_ds) + len(test_ds)\n",
    "\n",
    "print(\"total_size, dataset, train_ds, back_ds, valid_ds, test_ds, backtest_ds: \", \\\n",
    "      total_size, len(dataset), len(train_ds), len(back_ds), len(valid_ds), len(test_ds), len(backtest_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_baseDateDetails(ds):\n",
    "    baseId = baseDateDetails = None\n",
    "    for z in ds:\n",
    "        baseId, _, _, _, baseDateDetails, _, _ = z\n",
    "        break\n",
    "    return baseId.numpy(), baseDateDetails.numpy()\n",
    "# print(\"train_ds's first game: \", find_first_baseDateDetails(train_ds))\n",
    "# print(\"back_ds's first game: \", find_first_baseDateDetails(back_ds))\n",
    "# print(\"valid_ds's first game: \", find_first_baseDateDetails(valid_ds))\n",
    "# print(\"test_ds's first game: \", find_first_baseDateDetails(test_ds))\n",
    "\"\"\"\n",
    "train_ds's first game:  (1000001, array([[4, 8, 7, 5]]))\n",
    "back_ds's first game:  (1037717, array([[23,  2, 18,  5]]))\n",
    "valid_ds's first game:  (1039366, array([[23, 12, 22,  4]]))\n",
    "test_ds's first game:  (1041019, array([[24, 11,  2,  5]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batches = apply_train_pipeline(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 5\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, baseDateDetails, mask), (base_label) = z\n",
    "    cnt -= 1 \n",
    "    if cnt == 0: break\n",
    "print(baseId.shape, sequence.shape, base_bb.shape, mask.shape, base_label.shape, baseDateDetails.shape)\n",
    "sample_x = (sequence, base_bb, baseDateDetails, mask)\n",
    "sample_y = (base_label)\n",
    "# print(baseId.numpy(), base_bb.numpy(), baseDateDetails.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_batches = apply_test_pipeline(back_ds)\n",
    "valid_batches = apply_test_pipeline(valid_ds)\n",
    "test_batches = apply_test_pipeline(test_ds)\n",
    "backtest_batches = apply_test_pipeline(backtest_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):     # positions.shape = (batch, seq_len). d_model is a positive even integer.\n",
    "    # positions[b, :] = [dago_299, dago_298, ..., dago_0] = eg [6, 7, ..., 1611]\n",
    "\n",
    "    years = 25\n",
    "    quotient = 365 * years / d_model   # 325 * 25 is total days during 25 years since 2024 (the earliest data year). \n",
    "    fractional_pos = positions / quotient  # (batch, seq_len). max << d_model.  lower=0, upper<<d_model\n",
    "    depth = d_model/2   #\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth  # (depth,). [0/depth, 1/depth, ..., (depth-1)/depth]\n",
    "\n",
    "    # Why do we use this specific formula for depths, while any formula will work?\n",
    "    # depths will be multiplied by fractional_pos, which may have ever large number.\n",
    "    # We want the product to be limited, and depths should converge to zero to limit the product.\n",
    "    # If it cenverges to zero too fast, though, the generated pos_encoding will have little representative power as positional representation.\n",
    "    # Why, then, do we want to limit the product?\n",
    "    # The product visiting many different values will be enough for us. The solution is to let them go to zero, from a significant value.\n",
    "    BIG = d_model * 1.0 * 0.8     # let it be float.\n",
    "    depths = 1.0 / tf.pow(BIG, depths)        # 1 / [1, ..., BIG ** ((depth-1)/depth)] = [1, ..., >1/BIG]\n",
    "\n",
    "    # Why fractional_pos is multiplied linearly?\n",
    "    # Because (sin(A+a), cos(A+a)) is a rotation of (sin(A), cos(A)), no matter what depths is.\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] * depths  # [0, ..., <<d_model ] * [ 1, ..., >1/BIG]. \n",
    "    # Interleaving sin and cos is equivalent to seperated sin and cos.\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')  # Interleaving sin and cos. (batch, seq_len, d_model)\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)   # Seperated sin and cos. (batch, seq_len, d_model)\n",
    "    return pos_encoding  # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 15\n",
    "seq_len = 300\n",
    "d_model = 184\n",
    "positions = tf.Variable([[i for i in range(0, 365 * years, int(365 * years / seq_len))] for batch in range(100)], dtype=tf.float32)\n",
    "# positions = tf.ones((100, 200), dtype=tf.float32) * tf.range(200, dtype=tf.float32)\n",
    "pos_encoding = positional_encoding(positions, d_model=d_model)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class hParams:    \n",
    "    nDivisions = len(DIVIISONS)\n",
    "    division_embs = 4\n",
    "\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 4  # [0, 1, 2, 3] Maximum nGoals goals for a team in each of 1st and 2nd halfs. Extra goals will be clipped_by_value.\n",
    "    goal_embs = 4\n",
    "    nShoots = 21    # [0, ..., 20]\n",
    "    shoot_embs = 4 # for combination\n",
    "    nShootTs = 11   # [0, ..., 10]\n",
    "    shootT_embs = 4 # for combination\n",
    "    nCorners = 11   # [0, ..., 10]\n",
    "    corner_embs = 4 # for combination\n",
    "    nFauls = 21     # [0, ..., 20]\n",
    "    faul_embs = 2 # for combination\n",
    "    nYellows = 5    # [0, ..., 4]\n",
    "    yellow_embs = 2 # for combination\n",
    "    nReds = 2       # [0, 1]\n",
    "    red_embs = 2\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "\n",
    "    # d_model DEPENDS...\n",
    "    # We want as small d_model as possible, because from which we can freely choose an actual d_model.\n",
    "    # Tests seem to indicate that larger d_model leads to training overfitting and validation underfitting.\n",
    "    d_model = 1 * division_embs + 2 * team_embs + 3 * NUMBER_BOOKIES\n",
    "    d_encoder = d_decoder = 0\n",
    "    if EMBED_AB_COLS:\n",
    "        d_encoder = d_model + 1 * goal_embs + 1 * goal_embs + 1 * (shoot_embs + shootT_embs + corner_embs + faul_embs + yellow_embs + red_embs)\n",
    "        d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "    else:   # This mode, EMBED_AB_COLS = False, gives much smaller d_moddel, maybe avoiding overfitting.\n",
    "        d_encoder = d_model + 2 * 8     # 2 * 8 : bb_cols\n",
    "        d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "    d_model = max(d_encoder, d_decoder)     # (136, 118) for EMBED_AB_COLS, (126, 118) for False EMBED_AB_COLS\n",
    "    d_model += ADDITIONAL_D_MODEL               # Adjust for the model size and overfitting.\n",
    "    d_model = d_model + d_model % 2     # make it an even number.\n",
    "    print(\"d_model: \", d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, hParams, isEncoder=True):\n",
    "        super().__init__()\n",
    "        self.isEncoder = isEncoder\n",
    "\n",
    "        # game\n",
    "        self.division_emb = tf.keras.layers.Embedding(hParams.nDivisions, hParams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.team_emb = tf.keras.layers.Embedding(hParams.nTeams, hParams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "        if EMBED_AB_COLS:\n",
    "            # AB_cols\n",
    "            self.firstH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.secondH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.shoot_emb = tf.keras.layers.Embedding(hParams.nShoots * hParams.nShoots, hParams.shoot_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.shootT_emb = tf.keras.layers.Embedding(hParams.nShootTs * hParams.nShootTs, hParams.shootT_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.corner_emb = tf.keras.layers.Embedding(hParams.nCorners * hParams.nCorners, hParams.corner_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.faul_emb = tf.keras.layers.Embedding(hParams.nFauls * hParams.nFauls, hParams.faul_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.yellow_emb = tf.keras.layers.Embedding(hParams.nYellows * hParams.nYellows, hParams.yellow_embs, dtype=tf.float32, mask_zero=False)\n",
    "            self.red_emb = tf.keras.layers.Embedding(hParams.nReds * hParams.nReds, hParams.red_embs, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        # base_date\n",
    "        self.day_emb = tf.keras.layers.Embedding(31, 2, dtype=tf.float32, mask_zero=False)\n",
    "        self.month_emb = tf.keras.layers.Embedding(12, 2, dtype=tf.float32, mask_zero=False)\n",
    "        self.wday_emb = tf.keras.layers.Embedding(7, 2, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        self.position_permuting_encoding = tf.keras.layers.Dense(hParams.d_model)\n",
    "        self.position_permuting_decoding = tf.keras.layers.Dense(hParams.d_model)\n",
    "\n",
    "        self.idx_Days = BB_cols.index('Date')\n",
    "\n",
    "    def representDateDetails(self, dateDetails):\n",
    "        # dateDetails: (batch, 1, 4)\n",
    "        bYears, bMonths, bDays, bWDays = tf.split(dateDetails, [1, 1, 1, 1], axis=-1)   # All should be of (batch, seq_len = 1, 1)\n",
    "        bYears = tf.cast(bYears, dtype=tf.float32)  # (batch, seq_len = 1, 1)\n",
    "        bDays = self.day_emb(bDays)[:, :, -1]       # (batch, seq_len = 1, embs = 2)\n",
    "        bMonths = self.month_emb(bMonths)[:, :, -1] # (batch, seq_len = 1, embs = 2)\n",
    "        bWDays = self.wday_emb(bWDays)[:, :, -1]    # (batch, seq_len = 1, embs = 2)\n",
    "        # w = tf.Variable(np.math.pi / 25, dtype=tf.float32)    # 25 years are covered by pi or a half circle.\n",
    "        w = np.math.pi / 25\n",
    "        bYearsCos = tf.math.cos(bYears * w)\n",
    "        bYearsSin = tf.math.sin(bYears * w)\n",
    "        bYears = tf.concat([bYearsCos, bYearsSin], axis=-1)   # (batch, seq_len = 1, 1+1 = 2)\n",
    "        return bYears, bMonths, bDays, bWDays\n",
    "\n",
    "    def combined_embeddings_of_double_columns(self, emb_layer, columns, nValues):\n",
    "        # Assume emb_layer = Embedding(nValues * nValues, embs, mask_zero=False)\n",
    "        cols = tf.cast(columns, dtype=tf.int32)\n",
    "        cols = tf.clip_by_value(cols, 0, nValues-1)\n",
    "        combi = cols[:, :, 0] * nValues + cols[:, :, 1]   # (batch, seq_len, 1). [0, ..., nValues * nValues - 1]\n",
    "        combi = emb_layer(combi)\n",
    "        return combi    # (batch, seq_len, 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        (sequence, base_bb, baseDateDetails, mask) = x # sob = sequence or base_bb\n",
    "        sequenceDays = sequence[:, :, self.idx_Days]  # (batch, seq_len)\n",
    "        baseDays = base_bb[:, :, self.idx_Days]   # (batch, 1)\n",
    "\n",
    "        # sequence follows BBAB, whereas base_bb follows \n",
    "        \n",
    "        # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        if self.isEncoder:\n",
    "            # ramainder: Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols  --- total 12 fields.\n",
    "            id, div, days, teams, odds, half_goals, full_goals, shoot, shootT, corner, faul, yellow, red \\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), \\\n",
    "                                  len(Shoot_cols), len(ShootT_cols), len(Corner_cols), len(Faul_cols), len(Yellow_cols), len(Red_cols),], axis=-1)\n",
    "            # All shape of (batch, sequence, own_cols), all tf.flaot32\n",
    "        else:\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "            # remainder: [] \n",
    "            # All shape of (batch, 1, own_cols), guess., all tf.float32\n",
    "\n",
    "        div = self.division_emb(tf.cast(div, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=1, division_embs)\n",
    "        div = tf.reshape(div, [div.shape[0], div.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=1*division_embs) --- \n",
    "        teams = self.team_emb(tf.cast(teams, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=2, team_embs)\n",
    "        teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=2*team_embs) --- \n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                first_half_goals = self.combined_embeddings_of_double_columns(self.firstH_goal_emb, half_goals, hParams.nGoals)\n",
    "                second_half_goals = self.combined_embeddings_of_double_columns(self.secondH_goal_emb, full_goals - half_goals, hParams.nGoals)\n",
    "                shoot = self.combined_embeddings_of_double_columns(self.shoot_emb, shoot, hParams.nShoots)\n",
    "                shootT = self.combined_embeddings_of_double_columns(self.shootT_emb, shootT, hParams.nShootTs)\n",
    "                corner = self.combined_embeddings_of_double_columns(self.corner_emb, corner, hParams.nCorners)\n",
    "                faul = self.combined_embeddings_of_double_columns(self.faul_emb, shoot, hParams.nFauls)\n",
    "                yellow = self.combined_embeddings_of_double_columns(self.yellow_emb, yellow, hParams.nYellows)\n",
    "                red = self.combined_embeddings_of_double_columns(self.red_emb, red, hParams.nReds)\n",
    "\n",
    "                concat = [div, teams, odds, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow, red]\n",
    "            else:\n",
    "                concat = [div, teams, odds, half_goals, full_goals, shoot, shootT, corner, faul, yellow, red]\n",
    "        else:\n",
    "            if DECODE_BASE_DATE:\n",
    "                bYears, bMonths, bDays, bWDays = self.representDateDetails(baseDateDetails)\n",
    "                concat = [div, teams, odds, bYears, bMonths, bDays, bWDays]\n",
    "            else:\n",
    "                concat = [div, teams, odds]\n",
    "\n",
    "        concat = tf.concat(concat, axis=-1)\n",
    "        assert concat.shape[-1] <= hParams.d_model  \n",
    "        \n",
    "        if self.isEncoder:\n",
    "            concat = self.position_permuting_encoding(concat)  # (batch, MAX_TOKENS, <= hParams.d_model)\n",
    "        else:\n",
    "            concat = self.position_permuting_decoding(concat)  # (batch, 1, <= hParams.d_model)\n",
    "\n",
    "        positions = tf.cast(baseDays - sequenceDays, dtype=tf.float32) if self.isEncoder else tf.cast(baseDays - baseDays, dtype=tf.float32) # the latter is a zero tensor.\n",
    "        # eg. positions[b, :] = Tensor([6, 7, ..., 1911]\n",
    "        pe = positional_encoding(positions, d_model=hParams.d_model) # (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        pe = pe / tf.math.sqrt(tf.cast(concat.shape[-1], tf.float32))   # Read \"Attention is all you need\"\n",
    "        concat = concat + pe\n",
    "\n",
    "        if self.isEncoder:\n",
    "            mask = mask     # (batch, MAX_TOKEN, MAX_TOKEN)\n",
    "        else:\n",
    "            # Decoder layers will find cross attention between Decoder concat and Encoder concat.\n",
    "            mask = mask[:, 0:concat.shape[1], :]    # concat: (batch, 1, MAX_TOKEN)   \n",
    "\n",
    "        return (concat, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hParams, isEncoder=True)\n",
    "eSob, eMask = PE(sample_x)\n",
    "print(eSob.shape, eMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hParams, isEncoder=False)\n",
    "dSob, dMask = PE(sample_x)\n",
    "print(dSob.shape, dMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axis is normalized across. No inter-token operation.\n",
    "      self.add = tf.keras.layers.Add()  # Operate on the -1 axis.\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(query=x, key=context, value=context, attention_mask=mask, return_attention_scores=True)    \n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(query=x, value=x, key=x, attention_mask=mask)\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(query=x, value=x, key=x, use_causal_mask = True)  # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])\n",
    "      x = self.layer_norm(x)\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=True)\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (batch, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_emb(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # (batch_size, max_tokens, d_model)\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, *, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "      self.causal_self_attention = CausalSelfAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.cross_attention = CrossAttention(num_heads=num_heads, key_dim=d_model, dropout=dropout_rate)\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=False)\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (batch, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_emb(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens), ca_cask: which tokens of context to mask out (with 0)\n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hParams, dropout_rate=dropout_rate)\n",
    "      self.decoder = Decoder(hParams, dropout_rate=dropout_rate)\n",
    "      self.final_layer = tf.keras.layers.Dense(hParams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, x):\n",
    "      # inputs = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), baseDateDetails: (batch, 1, 4), mask: (batch, max_token, max_token)\n",
    "      context = self.encoder(x)  # (batch, max_tokens, d_model). Only sequence and mask are used.\n",
    "      x = self.decoder(x, context)  # (batch, 1, d_model).  Only base_bb, baseDateDetails, and mask are used.\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(hParams)\n",
    "y = sample_transformer(sample_x)\n",
    "\n",
    "sample_transformer.summary()\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomNormal = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.05, seed=523)\n",
    "Zeros = tf.keras.initializers.Zeros()\n",
    "\n",
    "# Used for earlier versions that don't allow mixing bookies.\n",
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    # total (nLayers + nLayers) dims = 2 * nLayers dims\n",
    "    dims = [hParams.d_model * 2] * nLayers\n",
    "    dims = dims + [hParams.d_model * 2 + round( (d_output - hParams.d_model * 2) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dims[id], kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=id), bias_initializer=Zeros, activation='tanh') for id in range(len(dims))]\n",
    "    Layers = []\n",
    "    for id in range(len(layers)):\n",
    "      if id % 3 == 0:\n",
    "        Layers.append(tf.keras.layers.LayerNormalization())\n",
    "      Layers.append(layers[id])\n",
    "    self.seq = tf.keras.Sequential(Layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = tf.Variable([std_params[col][0] for col in Odds_cols]) # Odds_cols comes first in _Cols_to_Standardize\n",
    "std_variation = tf.Variable([std_params[col][1] for col in Odds_cols])\n",
    "print(std_mean.shape, std_variation.shape)\n",
    "\n",
    "# No, _Label_cols were not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1X2(tf.keras.Model):\n",
    "    softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.nQueries = nQueries\n",
    "        self.transformer = Transformer(hParams, dropout_rate=dropout_rate)\n",
    "        #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "        self.bookies = ['HDA' + str(b) for b in range(NUMBER_BOOKIES)]\n",
    "        self.baseAdaptors = [Adaptor(ADAPTORS_LAYERS, self.nQueries) for _ in self.bookies]\n",
    "        return\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.transformer(x) # (batch, d_model)\n",
    "        stake_p = [adaptor(x) for adaptor in self.baseAdaptors]  # [(batch, nQueries)] * nBookies\n",
    "        stake_p = tf.stack(stake_p, axis=0)   # (nBookies, batch, nQueries)\n",
    "        stake_p = tf.nn.softmax(stake_p)  # (nBookies, batch, nQueries)   #\n",
    "        # stake_p = tf.math.sigmoid(stake_p * 5)  # the previous activation is tanh, ranging (-1, 1). Multiplier 5 will result in range (near 0, near 1)\n",
    "        return stake_p\n",
    "    \n",
    "    def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "        # ftGoals:  (batch, 2)\n",
    "        ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "        h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "        h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "        return h\n",
    "\n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output  # (nBookies, batch, nQueries)\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        # Note: Do not normalize stake_p. It can learn whether to bet or not, as well as betting direction.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "\n",
    "        profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)    # (nBooies, batch)\n",
    "        mean_profit_per_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "\n",
    "        profit_backtest = tf.reduce_mean(mean_profit_per_game, axis=None)  # () \n",
    "        loss = - profit_backtest  # U.action.42\n",
    "    \n",
    "        return loss, mean_profit_per_game # (), negative average profit on a game on a bookie\n",
    "    \n",
    "    def backtest_event_wise(self, y, output, key_a, key_b):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: oh_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        def analyze(stake_p):\n",
    "            # sum_stake_p = tf.math.reduce_sum(stake_p, axis=-1)\n",
    "            norm = tf.norm(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "            profit_p = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / norm) - 1.0, stake_p), axis=-1)# (nBookies, batch)\n",
    "            profit_backtest = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1) # (nBookies, batch)\n",
    "            condition = tf.math.logical_and(key_a <= profit_p, profit_p <= key_b)   # (nBookies, batch), dtype=tf.bool\n",
    "            indices = tf.where(condition)   # (n, 2)    values: (bookieId, gameId)\n",
    "            profit_backtest = profit_backtest[condition]  # (n,)    values: (profit)\n",
    "            stake = stake_p[condition]  # (n, nQueries)\n",
    "            # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "            return indices, stake, profit_backtest\n",
    "\n",
    "        if VECTOR_BETTING:\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "        else:\n",
    "            bestQuery = tf.argmax(stake_p, axis=-1)     #   (nBookies, batch, nQueries)\n",
    "            stake_p = tf.one_hot(bestQuery, self.nQueries, dtype=tf.float32)   #   (nBookies, batch, nQueries)\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "\n",
    "        # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "        return indices, stake, profit_backtest  # Not normalized. i.e. not divided with the norm of stake_p\n",
    "    \n",
    "    def get_batch_backtest(self, y, output):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "        sum = tf.math.reduce_sum(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "        indicatorP = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / sum) - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        backtestP = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        indices = tf.where(indicatorP == indicatorP)    # (nBookies * batch, 2 [bookieId, rel_gameId])\n",
    "        indicatorP = tf.reshape(indicatorP, [-1])   # (nBookies * batch, )\n",
    "        backtestP = tf.reshape(backtestP, [-1])     # (nBookies * batch, )\n",
    "        stake_p = tf.reshape(stake_p, [-1, self.nQueries])\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        return indicatorP, backtestP, indices, stake_p\n",
    "    \n",
    "class Classifier(Model_1X2):\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__(hParams, nQueries, dropout_rate=dropout_rate)\n",
    "        self.bce = tf.losses.BinaryCrossentropy(from_logits=False)\n",
    "        return\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.transformer(x) # (batch, d_model)\n",
    "        stake_p = [adaptor(x) for adaptor in self.baseAdaptors]  # [(batch, nQueries)] * nBookies\n",
    "        stake_p = tf.stack(stake_p, axis=0)   # (nBookies, batch, nQueries)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=-1) # (nBookies, batch)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=0) # (batch,)\n",
    "        stake_p = tf.math.sigmoid(stake_p * 5)          # (batch,)\n",
    "        return stake_p\n",
    "    \n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch,) \n",
    "        # output: (batch,)\n",
    "        loss = self.bce(y, output)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = Model_1X2(hParams, nQueries=3, dropout_rate=DROPOUT)\n",
    "\n",
    "stake_p = model_1x2(sample_x, training=True)\n",
    "loss, _ = model_1x2.loss(sample_y, stake_p)\n",
    "\n",
    "model_1x2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_1x2, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_with_dataset(dataset, profit_keys):\n",
    "    profits = [MIN_PROFIT] * len(profit_keys)\n",
    "    casts = [0] * len(profit_keys)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        profit_list, cast_list = model_1x2.backtest(y, outputs, profit_keys)\n",
    "\n",
    "        for p, c, id in zip(profit_list, cast_list, range(len(profit_keys))):\n",
    "            if c > 0:\n",
    "                profits[id] = (profits[id] * casts[id] + p * c) / (casts[id] + c)\n",
    "                casts[id] = casts[id] + c\n",
    "    # print('key', profit_back_mean, nBettingsTotal)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_event_wise_with_dataset(dataset, model, key_a, key_b):\n",
    "    indices = []; stakes = []; profits = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model(x, training=False)  #\n",
    "        new_indices, new_stakes, new_profits = model.backtest_event_wise(y, outputs, key_a, key_b)  # Tensor (n, 2 <bookieId, rel_gameId>), (n, nQueries), (n,)\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ..., [bookie, rel_game_id]]. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # [[0, 123], ..., [[bookie, gameId]]], len = n\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes\n",
    "\n",
    "        new_profits = list(new_profits.numpy())                 # [0.3, ...]\n",
    "        profits = profits + new_profits     # [1.3, ..., profit], len = n\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, stake, profit) for (bookie, gameId), stake, profit in zip(indices, stakes, profits)]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "    return interval_backtests     # [(bookie, gameId, profit) for ...]\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def get_backtest_and_indicator_profits(dataset):\n",
    "    backtestP = indicatorP = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        new_indicatorP, new_backtestP = model_1x2.get_backtest_and_indicator_profits(y, outputs)    # Tensor (nBookies * batch), (nBookies * batch)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())\n",
    "    return indicatorP, backtestP\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0):\n",
    "    backtestP = np.array(backtestP, dtype=np.float32)\n",
    "    indicatorP = np.array(indicatorP, dtype=np.float32)\n",
    "    indices = np.array(indices, dtype=np.int32)\n",
    "    stakes = np.array(stakes, dtype=np.float32)\n",
    "    idx = np.argsort(indicatorP)\n",
    "    indicatorP = indicatorP[idx]    # increasing order\n",
    "    backtestP = backtestP[idx]      # hope to be increading order\n",
    "    indices = indices[idx]\n",
    "    stakes = stakes[idx]\n",
    "\n",
    "    ab = []\n",
    "    for w in range(30, int(len(backtestP))):    # 30\n",
    "        back = np.convolve(backtestP, np.ones(w), mode='valid') - threshold\n",
    "        ab += [(back[i], i, i+w-1) for i in range(len(back))]   # [ i : i + w ] : w points. inclusive boundaries.\n",
    "    ab = [(p, a, b) for (p, a, b) in ab if p > 0]   # a, b : inclusive\n",
    "    ab.sort(reverse=False)  # False !   The larger the profit, the earlier it comes in ab.\n",
    "    AB = ab.copy()\n",
    "    for i in range(len(ab)):   # less valuable intervals are screened first.\n",
    "        interval_i = ab[i];  P, A, B = interval_i    # sure A < B\n",
    "        intersects = False\n",
    "        for j in range(i+1, len(ab)):\n",
    "            interval_j = ab[j]; p, a, b = interval_j    # sure a < b\n",
    "            if not (b < A or B < a): intersects = True; break\n",
    "        if intersects:  AB.remove(interval_i)   # interval_i is unique in AB\n",
    "    AB.sort(reverse=True); AB = AB[:5]\n",
    "    print(\"AB: \", AB)\n",
    "    [interval_profit, idx_a, idx_b] = AB[0]     # idx_a, idx_b : inclusive\n",
    "    keyPairs = [[indicatorP[a], indicatorP[b]] for (B, a, b) in AB]\n",
    "    (key_a, key_b) = keyPairs[0]\n",
    "\n",
    "    return indicatorP, backtestP, indices, stakes, interval_profit, key_a, key_b, idx_a, idx_b\n",
    "\n",
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def inference_step(x, odds, interval_a, interval_b):\n",
    "    stake_p = model_1x2(x, training=False)    # (nBookies, batch, nQueries)\n",
    "    nQueries = stake_p.shape[-1]\n",
    "    profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "    bet_bool = interval_a <= profit_p and profit_p >= interval_b    # (nBookies, batch)\n",
    "    bet_bool = tf.stack([bet_bool] * nQueries, axis=-1) # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.math.multiply(stake_p, tf.cast(bet_bool, dtype=tf.float32))   # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.reshape(stake_vector, [1, 0, 2])  # (batch, nBookies, nQueries)\n",
    "    return stake_vector\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def inference_with_dataset(dataset, interval_a, interval_b): \n",
    "    vectors = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)\n",
    "        stake_vector = inference_step(x, odds, interval_a, interval_b)  # (batch, nBookies, nQueries)\n",
    "        vectors.append(stake_vector)\n",
    "    \n",
    "    stake_vectors = tf.concat(vectors, axis=0)   # (batch, nBookies, nQueries)\n",
    "    return stake_vectors    # (batch, nBookies, nQueries)\n",
    "\n",
    "def get_dataset_backtest(dataset, model):\n",
    "    backtestP = indicatorP = indices = stakes = []\n",
    "\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model(x, training=False)\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        new_indicatorP, new_backtestP, new_indices, new_stakes = model.get_batch_backtest(y, outputs)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())  # + [1.2, ...], len = nBookies * batch\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())     # + [1.2, ...], len = nBookies * batch\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ...], len = nBookies * batch. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # + [[0, 1000123], ..., [[bookie, gameId]]], len = nBookies * batch\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes    # + [[sHWin, sDraw, sAWin], ...], len = nBooks * batch\n",
    "\n",
    "    indicatorP_permuted, backtestP_permuted, indices_permutated, stakes_permutated, interval_profit, interval_a, interval_b, idx_a, idx_b = \\\n",
    "        get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0)  # idx_a, idx_b : inclusive\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, list(stake), profit) for (bookie, gameId), stake, profit in \\\n",
    "        zip(indices_permutated[idx_a:idx_b+1], stakes_permutated[idx_a:idx_b+1], backtestP_permuted[idx_a:idx_b+1])]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "\n",
    "    return interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b\n",
    "\n",
    "class Adam_exponential(tf.keras.optimizers.Adam):\n",
    "    def __init__(self, initial_step, starting_rate, ex_step, ex_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-12):\n",
    "        self.step = tf.Variable(initial_step, dtype=tf.float32, trainable=False)\n",
    "        learning_rate = tf.compat.v1.train.exponential_decay(starting_rate, self.step, ex_step, ex_rate/starting_rate, staircase=False)\n",
    "        super().__init__(learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'initial_profit': -float('inf'), 'loss': [], 'val_loss': [], 'recall': [], 'precision': [], 'learning_rate': [], 'time_taken': [],  'gauge': [], 'backtest_reports': []}\n",
    "    def set_initial_interval_profit(self, initail_inverval_profit):\n",
    "        self.history['initial_profit'] = initail_inverval_profit\n",
    "        self.save()\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.history? ---------------------- Check it.\n",
    "        self.__init__(self.filepath)\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = data_helpers.LoadJsonData(self.filepath)\n",
    "        if history is not None: self.history = history\n",
    "    def append(self, train_loss, val_loss, recall, precision, learning_rate, time_taken, gauge, backtest_report):\n",
    "        self.history['loss'].append(self.round_sig(train_loss, 4))\n",
    "        self.history['val_loss'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['recall'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['precision'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['learning_rate'].append(learning_rate)\n",
    "        self.history['time_taken'].append(time_taken)\n",
    "        self.history['gauge'].append(gauge)\n",
    "        self.history['backtest_reports'].append(backtest_report)\n",
    "        self.save()\n",
    "    # def append_backtests(self, epoch, key_a, key_b, interval_profit, backtest):\n",
    "    #     self.history['backtests'].append((epoch, key_a, key_b, interval_profit, backtest))\n",
    "    #     self.save()\n",
    "    def get_backtest_report(self, epoch):\n",
    "        return self.history['backtest_reports'][epoch]     # sure exists. epoch is selected.\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['recall'])\n",
    "        assert len(self.history['loss']) == len(self.history['precision'])\n",
    "        assert len(self.history['loss']) == len(self.history['learning_rate'])\n",
    "        assert len(self.history['loss']) == len(self.history['time_taken'])\n",
    "        assert len(self.history['loss']) == len(self.history['gauge'])\n",
    "        assert len(self.history['loss']) == len(self.history['backtest_reports'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "    def get_max_gauge(self, epoch):\n",
    "        profits = self.history['gauge']\n",
    "        return -float('inf') if (len(profits) <= 0 or epoch <= 0) else max(profits[:epoch])\n",
    "    def replace_gauge(self, epoch, gauge):\n",
    "        self.history['gauge'][epoch] = gauge;   self.save()\n",
    "    def show(self, ax):\n",
    "        ax.set_title(TEST_ID + \": loss history\")\n",
    "        ax.plot(self.history['loss'])\n",
    "        ax.plot(self.history['val_loss'])\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch', loc='right')\n",
    "        ax.legend(['train_loss', 'val_loss'], loc='lower left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history.json')\n",
    "\n",
    "history = history_class(historyPath)\n",
    "\n",
    "def removeFile(path):\n",
    "    files = glob.glob(path + \"*\")   # \"*.*\" may not work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    return\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    removeFile(checkpointPath)\n",
    "    removeFile(checkpointPathBest)\n",
    "    history.reset()\n",
    "\n",
    "history.load()\n",
    "\n",
    "print(\"history: \", history.history['loss'])\n",
    "\n",
    "if BASE_TEST_ID != '':      # whether training or infering. BASE_TEST_ID can even be this one.\n",
    "    try: \n",
    "        model_1x2.load_weights(CheckpointPathBestBase)\n",
    "        print('The best model of ' + BASE_TEST_ID + ' loaded.')\n",
    "    except:\n",
    "        print('Failed to load the best model of ' + BASE_TEST_ID)\n",
    "else:\n",
    "    if TRAIN_MODE:          # not the best.\n",
    "            try: \n",
    "                model_1x2.load_weights(checkpointPath)\n",
    "                print('The latest model of ' + TEST_ID + ' loaded.')\n",
    "            except:\n",
    "                print('Failed to load the latest model of ' + TEST_ID)\n",
    "    else:                   # best one\n",
    "        try: \n",
    "            model_1x2.load_weights(checkpointPathBest)\n",
    "            print('The best model of ' + TEST_ID + ' loaded.')\n",
    "        except:\n",
    "            print('Failed to load the best model of ' + TEST_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recall():\n",
    "  def __init__(self, name='recall', **kwargs):\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):    # (batch,)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    labeled_positives = tf.math.reduce_sum(label, axis=None)\n",
    "    recall = hit_positives / (labeled_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', **kwargs):\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    predicted_positives = tf.math.reduce_sum(pred, axis=None)\n",
    "    precision = hit_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, optimizer, x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value, mean_profit_per_game = model.loss(y, outputs)    # (), (batch,)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value, mean_profit_per_game  # (), (batch,)\n",
    "\n",
    "@tf.function\n",
    "def find_loss_for_step(model, x, y):\n",
    "    outputs = model(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value, mean_profit_per_game = model.loss(y, outputs)\n",
    "    return loss_value, mean_profit_per_game\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def find_loss_for_dataset(model, ds_batches):\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)   # The creation of tensor inside a tf function may be the problem.\n",
    "    n = 0\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        n += 1\n",
    "        loss, _ = find_loss_for_step(model, x, y)\n",
    "        val_loss = val_loss * (n-1) / n + loss / n   ###\n",
    "    return val_loss\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def find_recall_precision_for_dataset(model, ds_batches):\n",
    "    recall.reset(); precision.reset()\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); label = value\n",
    "        pred = model(x)\n",
    "        pred = tf.cast(pred > 0.5, dtype=tf.int32)\n",
    "        recall.update(label, pred); precision.update(label, pred)\n",
    "    return recall.result(), precision.result()\n",
    "\n",
    "def replace_baseLabel_with_profitable(model, ds_batches, threshold=0.0001):\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "            _, mean_profit_per_game = find_loss_for_step(model, x, y)  # (), (batch,)\n",
    "            profitable = tf.cast(mean_profit_per_game >= threshold, dtype=tf.int32)\n",
    "            for i in range(baseId.shape[0]):\n",
    "                yield (baseId[i], sequence[i], base_bb[i], baseDateDetails[i], mask[i], profitable[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS)), tf.TensorShape(())),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def select_profitable_items(model, ds_batches, threshold=0.5):\n",
    "    def generator():\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "            output = model(x)   # (batch, ...)\n",
    "            for i in range(output.shape[0]):\n",
    "                if output[i] >= threshold: yield (baseId[i], sequence[i], base_bb[i], baseDateDetails[i], mask[i], y[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS)), tf.TensorShape(())),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def analyze(model, checkpointPath, ds_batches):\n",
    "    model.load_weights(checkpointPath)\n",
    "    interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b = get_dataset_backtest(ds_batches, model)\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "    axes[0].plot(indicatorP_permuted); axes[0].grid(True)\n",
    "    axes[1].plot(backtestP_permuted); axes[1].grid(True)\n",
    "    w = 50; idx_1 = idx_a - int(w/2); idx_2 = idx_b - int(w/2)\n",
    "    back = np.convolve(np.array(backtestP_permuted), np.ones(w), mode='valid') / w\n",
    "    axes[2].plot(back, lw=0.5); axes[2].plot([idx_1, idx_1], plt.ylim(), 'r', lw=0.5); axes[2].plot([idx_2, idx_2], plt.ylim(), 'r', lw=0.5); axes[2].grid(True)\n",
    "    axes[3].plot(indicatorP_permuted, backtestP_permuted); axes[3].grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_interval_id = focus_scores = focus_back = focus_valid = focus_test = None\n",
    "\n",
    "def show_steps(epoch, step, loss, samples_seen, learning_rate):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hParams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {:.6}, samples_seen: {}, learning_rate: {:.5e}                  \".\n",
    "            format(epoch, step, float(loss), samples_seen, learning_rate), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def accumulate(backtests, compound=False):\n",
    "    initial = 1.0; Div = 3\n",
    "    sum = initial; minS = sum; maxS = sum\n",
    "    for (bookie, gameId, stake, profit) in backtests:\n",
    "        sum_stake = 0.0\n",
    "        for s in stake: sum_stake += s\n",
    "        sum += (profit / sum_stake * ( sum if compound else initial*sum_stake/Div))\n",
    "        if sum < minS: minS = sum\n",
    "        if sum > maxS: maxS = sum\n",
    "        if sum < 0.2: break\n",
    "        # if sum > initial * 2: initial = sum   # this is a step-wise compound. as risky as simple compound.\n",
    "    return sum, minS, maxS\n",
    "\n",
    "def run_backtest(epoch, model, playback=False):     # Read-only\n",
    "    #-------------------- make a backtest virtually/actually\n",
    "    A_ds = valid_batches; len_A_ds = len(valid_ds)\n",
    "    B_ds = test_batches; len_B_ds = len(test_ds)\n",
    "\n",
    "    if playback:\n",
    "        interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests = \\\n",
    "            history.get_backtest_report(epoch)\n",
    "    else:   # idx_a, idx_b : inclusive\n",
    "        print(\"Wait...\", end='')\n",
    "        A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b = get_dataset_backtest(A_ds, model)\n",
    "        B_backtests = backtest_event_wise_with_dataset(B_ds, model, interval_a, interval_b)\n",
    "\n",
    "    # We have A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b, B_backtests\n",
    "\n",
    "    print(\"iterval_a: {:.8f}, interval_b: {:.8f}, idx_a: {}, idx_b: {}, A_interval_profit: {:.4f}\".format(interval_a, interval_b, idx_a, idx_b, A_interval_profit))\n",
    "    \n",
    "    #---- validation/verification\n",
    "    A_len_backtest = len(A_backtests)\n",
    "    B_len_backtest = len(B_backtests)\n",
    "    A_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in A_backtests]))\n",
    "    B_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in B_backtests]))\n",
    "    A_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in A_backtests]))\n",
    "    B_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in B_backtests]))\n",
    "    A_final_capital, A_bottom_capital, A_top_capital = accumulate(A_backtests, compound=False)\n",
    "    B_final_capital, B_bottom_capital, B_top_capital = accumulate(B_backtests, compound=False)\n",
    "    A_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in A_backtests]\n",
    "    B_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in B_backtests]\n",
    "\n",
    "    format = \"len_backtest/len_dataset: {}/{}, accProfit/accStake: {:.4f}/{:.4f}, capital (final/bottom/top): {:.4f}/{:.4f}/{:.4f}\"\n",
    "    A_format = \"A_test:: \" + format; B_format = \"B_test:: \" + format\n",
    "    print(A_format.format(A_len_backtest, len_A_ds, A_acc_profit, A_acc_stake, A_final_capital, A_bottom_capital, A_top_capital))\n",
    "    print(B_format.format(B_len_backtest, len_B_ds, B_acc_profit, B_acc_stake, B_final_capital, B_bottom_capital, B_top_capital))\n",
    "    print(\"A_backtest: \", A_backtestsToShow[:20])\n",
    "    print(\"B_backtest: \", B_backtestsToShow[:20])\n",
    "\n",
    "    backtest_report = (interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests)\n",
    "    gauge = B_acc_profit\n",
    "\n",
    "    return backtest_report, gauge\n",
    "\n",
    "# conclude_train_epoch(-1, 0, 0, 0, 0, playback=False)\n",
    "def conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, train_loss, val_loss, recall, precision, learning_rate, time_taken, playback=False):\n",
    "    global focus_interval_id, focus_scores, focus_back, focus_valid, focus_test\n",
    "    print(\"EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\")\n",
    "    print(\"epoch: {}, train_loss: {:.9f}, val_loss: {:.9f}, recall/precision: {:.4f}/{:.4f}, learning_rate: {:.5e}, time_taken: {:.1f}\".format(epoch, train_loss, val_loss, recall, precision, learning_rate, time_taken))\n",
    "\n",
    "    backtest_report = (); gauge = - val_loss  # May be changed below. Larger gauses are better.  \n",
    "    # backtest_report, gauge = run_backtest(epoch, model, playback)   #-------------------------------------------------\n",
    "\n",
    "    upgraded = False\n",
    "    #-------------------- get interval scores from arguments, create the best checkpoint if we have a highest ever score.\n",
    "    if gauge > history.get_max_gauge(epoch):\n",
    "        upgraded = True\n",
    "        # focus_interval_id, focus_scores, focus_back, focus_valid, focus_test = bets_interval_id, interval_scores, best_interval_back, best_interval_valid, best_interval_test\n",
    "        if not playback: model.save_weights(checkpointPathBest)\n",
    "        print(\"----------------------------------------------------------------------------------------------------- best checkpoint updated\")\n",
    "    if playback: history.replace_gauge(epoch, gauge)\n",
    "\n",
    "    #--------------------- Save, finally\n",
    "    if not playback:\n",
    "        model.save_weights(checkpointPath)\n",
    "        history.append(train_loss, val_loss, recall, precision, learning_rate, time_taken, gauge, backtest_report)\n",
    "\n",
    "    return upgraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 2))\n",
    "history.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "epoch = 0\n",
    "for train_loss, val_loss, recall, precision, learning_rate, time_taken \\\n",
    "    in zip(history.history['loss'], history.history['val_loss'], history.history['recall'], history.history['precision'], history.history['learning_rate'], history.history['time_taken']):\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss, recall, precision, learning_rate, time_taken, playback = True)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def train(epochs, model, optimizer, history, checkpointPath, checkpointPathBest, dataset, apply_train_pipeline, get_f1=False):\n",
    "    epochs = epochs\n",
    "    for epoch in range(history.len(), history.len() + epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer.step.assign(history.len())\n",
    "        learning_rate = optimizer.lr.numpy()\n",
    "        n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "        m = 0; train_loss = 0.0\n",
    "        \n",
    "        ds_batches = apply_train_pipeline(dataset)      # the pipeline includes suffle.\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "            samples_seen += sequence.shape[0]\n",
    "            batch_loss, _ = train_step(model, optimizer, x, y)  # (), (batch,)\n",
    "            n += 1; loss = loss * (n-1)/n + batch_loss/n\n",
    "            m += 1; train_loss = train_loss * (m-1)/m + batch_loss/m\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                show_steps(epoch, step, loss, samples_seen, learning_rate)\n",
    "                n = 0; loss = 0.0\n",
    "\n",
    "        show_steps(epoch, step, loss, samples_seen, learning_rate)  # closing show\n",
    "\n",
    "        val_loss = find_loss_for_dataset(model, valid_batches)\n",
    "\n",
    "        recall = precision = 0.0\n",
    "        if get_f1:\n",
    "            recall, precision = find_recall_precision_for_dataset(model, valid_batches)\n",
    "            recall = recall.numpy(); precision = precision.numpy()\n",
    "        \n",
    "        upgraded = conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, float(train_loss), float(val_loss), learning_rate, recall, precision, (time.time()-start_time)/60, playback=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'PRETRAIN':\n",
    "\n",
    "    history = history_class(historyPath); history.load()\n",
    "    optimizer = Adam_exponential(history.len(), STARTING_LEARNING_RATE, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "\n",
    "    train(50, model_1x2, optimizer, history, checkpointPath, checkpointPathBest, train_ds, apply_train_pipeline, get_f1 = False)    #--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(model_1x2, checkpointPathBest, test_batches)    # baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c = Classifier(hParams, nQueries=3, dropout_rate=DROPOUT)\n",
    "\n",
    "checkpointPath_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_c')\n",
    "checkpointPathBest_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best_c')\n",
    "historyPath_c = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history_c.json')\n",
    "\n",
    "if OPERATION == 'TRAIN_C':\n",
    "\n",
    "    def X_versus_Y_c(baseId, sequence, base_bb, baseDateDetails, mask, profitable):\n",
    "        return (baseId, sequence, base_bb, baseDateDetails, mask), (profitable)\n",
    "\n",
    "    def apply_train_pipeline_c(ds):\n",
    "        return (ds.shuffle(BUFFER_SIZE).batch(train_batch_size).map(X_versus_Y_c, tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "    history_c = history_class(historyPath_c); history_c.load()\n",
    "    optimizer_c = Adam_exponential(history.len(), 1e-8, 30, 0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "\n",
    "    path_train_ds_with_profitable = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-train_ds_with_profitable')\n",
    "    if os.path.exists(path_train_ds_with_profitable):\n",
    "        train_ds_with_profitable = tf.data.Dataset.load(path_train_ds_with_profitable)\n",
    "    else:\n",
    "        train_ds_with_profitable = replace_baseLabel_with_profitable(model_c, train_batches, threshold=0.0001)\n",
    "        tf.data.Dataset.save(train_ds_with_profitable, path_train_ds_with_profitable)\n",
    "        train_ds_with_profitable = tf.data.Dataset.load(path_train_ds_with_profitable)\n",
    "\n",
    "    try: model_c.load_weights(checkpointPath_c)\n",
    "    except: model_c.load_weights(checkpointPathBest)    # We want to finetune the best model_1x2 to be initial model_c.\n",
    "\n",
    "    train(50, model_c, optimizer_c, history_c, checkpointPath_c, checkpointPathBest_c, train_ds_with_profitable, apply_train_pipeline_c, get_f1=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_f')\n",
    "checkpointPathBest_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best_f')\n",
    "historyPath_f = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history_f.json')\n",
    "\n",
    "if OPERATION == 'FINETUNE':\n",
    "\n",
    "    history_f = history_class(historyPath_f); history_f.load()\n",
    "    optimizer_f = Adam_exponential(history.len(), 1e-8, 30, 0.05, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "\n",
    "    path_train_ds_classified = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-train_ds_with_classified')\n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "    if os.path.exists(path_train_ds_with_profitable):\n",
    "        train_ds_classified = tf.data.Dataset.load(path_train_ds_classified)\n",
    "    else:\n",
    "        train_ds_classified = select_profitable_items(model_c, train_batches, threshold=0.5)\n",
    "        tf.data.Dataset.save(train_ds_classified, path_train_ds_classified)\n",
    "        train_ds_classified = tf.data.Dataset.load(path_train_ds_classified)\n",
    "\n",
    "    try: model_1x2.load_weights(checkpointPath_f)\n",
    "    except: model_1x2.load_weights(checkpointPathBest)    # We start finetuing from the best model\n",
    "\n",
    "    if TRAIN_MODE: train(50, model_1x2, optimizer_f, history_f, checkpointPath_f, checkpointPathBest_f, train_ds_classified, apply_train_pipeline, get_f1 = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'TEST':\n",
    "    \n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "    test_ds_classified = select_profitable_items(model_c, test_batches, threshold=0.5)\n",
    "    test_ds_classified_batches = apply_test_pipeline(test_ds_classified)\n",
    "    analyze(model_1x2, checkpointPathBest_f, test_ds_classified_batches)  # compare with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_a = 0.7    # Choose oneclassified_test_ds\n",
    "interval_b = 0.8    # Choose one\n",
    "\n",
    "if not TRAIN_MODE:      # just sketch\n",
    "    # df_grown gets to be the existing df_grown, because the train_mode if False.\n",
    "    df_total, df_new = df_grown, df_new = data_helpers.get_grown_and_new_from_football_data(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, train_mode = TRAIN_MODE, skip=False)\n",
    "    df_white = df_new\n",
    "    df_black = data_helpers.read_excel(path)\n",
    "\n",
    "    df_total = df_total; df_search = df_new\n",
    "    additional_id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, df_total, df_search, chooseDivs=chooseDivs)\n",
    "    \n",
    "    ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset-inference')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_total, additional_id_to_ids, tokenizer_team, std_params, train_mode=False)  #-----------------                                                                          #-----------------\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    ds_inference = apply_test_pipeline(ds)\n",
    "    stake_vectors = inference_with_dataset(ds_inference, interval_a, interval_b) # (batch, nBookies, nQueries)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
