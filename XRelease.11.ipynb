{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checklist\n",
    "1. Make sure your \"Regional Format\" is set to \"English (United Kingdom)\" before Excel opens files that contain English date-like objects,\n",
    "   if you want them to be parsed as English datetime. English (United States) regional format will convert them to American Datetime as possible.\n",
    "   You have no way to prevent Excel from converting date-like strings or objects to datetime when opening a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as ps\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "\n",
    "from config import config\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "\n",
    "TEST_ID = 'X.11'\n",
    "BASE_TEST_ID = ''  # should be either '' or an existing TEST_ID.\n",
    "DATA_BITS = 32              #------------------ 16 doesn't work.\n",
    "\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0          # 0 -> 6\n",
    "TRAIN_PERCENT= 94   # chronically the earliest part in the databased past rows.  In the end of this part, there is BACK part with the same size as VALID part.\n",
    "VALID_PERCENT = 4   # chronically the next part to train part in the databased past rows\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT # chronically the last part in the databased past rows. This part will be added with post-databasing rows, either past or known coming\n",
    "\n",
    "BUFFER_SIZE = 35000\n",
    "BATCH_SIZE = 4  #-----------------------------------\n",
    "DIM_TRANSFORMER = 190   # Experimental. Check it.\n",
    "LEARNING_RATE = 1e-6\n",
    "TEAM_EMBS = 50  #\n",
    "DROPOUT = 0.0               # 0.1 -> 0.0\n",
    "TRANSFORMER_LAYERS = 6      # \n",
    "TRANSFORMER_HEADS = 60\n",
    "ADAPTORS_LAYERS = 10\n",
    "RESET_HISTORY = False\n",
    "MIN_PROFIT = -1.0\n",
    "PROFIT_KEYS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COUNTRY = 'England'\n",
    "NUMBER_BOOKIES = 3  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds are not archived for the league since 2013.\n",
    "\n",
    "#-------------------- England ---------------------\n",
    "# Time range of data: 2004/2005 - 2024/2025\n",
    "# Leagues - Premiere, League 1, League 2, Championship, Conference\n",
    "# !!! Conference will be omitted because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "# Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "# William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "# William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "# William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "# William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "# William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "# BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "_Cols_List_to_Embbed = [Div_cols, Team_cols, Half_Goal_cols, Full_Goal_cols, Result_cols, Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_to_Unembbed = [Odds_cols]\n",
    "_Cols_List_to_Standardize = [Odds_cols] #  They are embedded now , Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_for_Label = [Full_Goal_cols, Odds_cols]\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BBAB_cols)\n",
    "print(BB_cols)\n",
    "print(_Label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(_Cols_List_to_Standardize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_std_size():\n",
    "    size = 0\n",
    "    for cols in _Cols_List_to_Standardize:\n",
    "        size += len(cols)\n",
    "    return size\n",
    "\n",
    "def get_standardization_params(df_grown):\n",
    "    \n",
    "    def get_mean_and_std(cols):\n",
    "        array = df_grown[cols]\n",
    "        array = np.array(array)\n",
    "        return (array.mean(), array.std(), np.max(array))\n",
    "    \n",
    "    params = {}\n",
    "    for cols in _Cols_List_to_Standardize:\n",
    "        params[cols[0]] = get_mean_and_std(cols)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def check_standardization(bbab, std_params):\n",
    "    for cols in _Cols_List_to_Standardize:\n",
    "        start = BBAB_cols.index(cols[0]); end = start + len(cols)\n",
    "        (mean, std, maximum) = std_params[cols[0]]\n",
    "        if -mean/std > min(bbab[start : end]) + 1e-5:\n",
    "            print('standardization error 1', cols[0], -mean/std, bbab[start : end])\n",
    "        if max(bbab[start : end]) > (maximum - mean) / std + 1e-5:\n",
    "            print('standardization error 2', cols[0], bbab[start : end], (maximum - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/\" + COUNTRY\n",
    "data_helpers.assign_seasonal_filenames(countryDirPath)\n",
    "df_grown, df_new = data_helpers.get_grown_extra_from_football_data_co_uk(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, train_mode = TRAIN_MODE, skip=True)\n",
    "if df_grown is not None: print(\"df_grown: \", df_grown.shape)\n",
    "if df_new is not None: print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "def createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens):\n",
    "        tokenizer = Tokenizer(models.WordLevel(unk_token=unknown_token))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "        tokenizer.train(corpus_files, trainer=trainer)\n",
    "        tokenizer.decoder = decoders.WordPiece(prefix=\" \")\n",
    "        return tokenizer\n",
    "\n",
    "def creat_team_tokenizer_uk(df_grown):\n",
    "    teams = list(set(list(df_grown['HomeTeam']) + list(df_grown['AwayTeam'])))\n",
    "    teams_string = [str(team) for team in teams]\n",
    "    teams_string = [re.sub(r\"\\s\", \"_\", item) for item in teams_string]    # replace spaces with a '_'\n",
    "    teams_text = \" \".join(teams_string)\n",
    "\n",
    "    corpus_file = os.path.join(countryDirPath, '_tokenizers', 'team_ids_text_uk.txt')\n",
    "    f = open(corpus_file, \"w+\", encoding=\"utf-8\")\n",
    "    f.write(teams_text)\n",
    "    f.close()\n",
    "\n",
    "    corpus_files = [corpus_file]\n",
    "    unknown_token = config['unknown_token']\n",
    "    special_tokens = [unknown_token] ################### + [\"[HOME]\", \"[AWAY]\"]\n",
    "    vocab_size = len(teams_string) + len(special_tokens)\n",
    "\n",
    "    tokenizer_team = createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)\n",
    "    return tokenizer_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df_grown)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hParams:    \n",
    "    nDivisions = len(DIVIISONS)\n",
    "    division_embs = 4\n",
    "\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    \n",
    "    nGoals  = 4  # 0 for 0 goals not for Unknown. Maximum nGoals goals for a team in each of 1st and 2nd halfs. Extra goals will be clipped_by_value.\n",
    "    goal_embs = 4\n",
    "    \n",
    "    nResult = 3    # HWin, Draw, AWin\n",
    "    result_embs = 2 # for combination\n",
    "    \n",
    "    nShoots = 21    # [0, ..., 20]\n",
    "    shoot_embs = 4 # for combination\n",
    "    \n",
    "    nShootTs = 11   # [0, ..., 10]\n",
    "    shootT_embs = 4 # for combination\n",
    "    \n",
    "    nCorners = 11   # [0, ..., 10]\n",
    "    corner_embs = 4 # for combination\n",
    "    \n",
    "    nFauls = 21     # [0, ..., 20]\n",
    "    faul_embs = 2 # for combination\n",
    "    \n",
    "    nYellows = 5    # [0, ..., 4]\n",
    "    yellow_embs = 2 # for combination\n",
    "    \n",
    "    nReds = 2       # [0, 1]\n",
    "    red_embs = 2\n",
    "    \n",
    "    d_model = DIM_TRANSFORMER\n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "    # d_model = team_emb_size * 2 + country_emb_size * 3 + odds_size + outcome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================= build_id_to_ids.ipynb\n",
    "\n",
    "QUALITY = 75\n",
    "\n",
    "targetLength = 300          # A target game will be explained by maximum 'targetLength' count of past games.\n",
    "minCurrent = 1e-7\n",
    "sinceDaysAgo = 365 * 20     # A target game will be explained by past games that took place since 'sinceDaysAgo' days ago. \n",
    "qualityPct = QUALITY        #\n",
    "conductance365 = 0.9        # 0.9 comes from test.\n",
    "chooseDivs=False\n",
    "\n",
    "id_to_ids_filename = str(targetLength) + '-' + str(minCurrent) + '-' + str(sinceDaysAgo) + '-' + str(qualityPct) + '-' + str(conductance365) + '-' + str(chooseDivs)\n",
    "id_to_ids_filePath = os.path.join(countryDirPath, '_id_to_ids', id_to_ids_filename + \".json\")\n",
    "\n",
    "id_to_ids_existing = None\n",
    "id_to_ids_existing = data_helpers.LoadJsonData(id_to_ids_filePath)\n",
    "tf_total = df_grown; df_search = df_grown\n",
    "id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, tf_total, df_search, chooseDivs=chooseDivs)\n",
    "data_helpers.SaveJsonData(id_to_ids, id_to_ids_filePath)\n",
    "\n",
    "print(len(id_to_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_bbab = list(df_grown.loc[df_grown['id'] == min(df_grown['id']), BBAB_cols].iloc[0, :])\n",
    "# print(base_bbab)\n",
    "\n",
    "def normalize_raw_bbab(bbab, tokenizer_team, std_params):\n",
    "\n",
    "    \"\"\"\n",
    "        BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        AB_cols = Half_Goal_cols + Full_Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        This is how the data returned by this function is consumed. Note this function doesn't alter the order of columns.\n",
    "       \n",
    "        if self.isEncoder:\n",
    "            # Extract odds to remove them\n",
    "            id, div, days, teams, odds, half_goals, full_goals, result, remainder \\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), len(Result_cols), -1], axis=-1)\n",
    "            # All shape of (batch, sequence, own_columns)\n",
    "        else:\n",
    "            # Extract odds to remove them\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "    \"\"\"\n",
    "\n",
    "    #---------------------- label, before changing bbab\n",
    "    label = []\n",
    "    for cols in _Cols_List_for_Label:\n",
    "        start = BBAB_cols.index(cols[0]); end = BBAB_cols.index(cols[-1]) + 1\n",
    "        label += bbab[start : end]\n",
    "\n",
    "    #----------------------- columns to embed: Div, HomeTeam, AwayTeam, HTHG, HTAG, FTHG, FTAG, HTR, FTR\n",
    "    start = BBAB_cols.index(Div_cols[0])\n",
    "    Div = bbab[start]\n",
    "    bbab[start] = DIVIISONS.index(Div)  # Assumes no n/a\n",
    "\n",
    "    start = BBAB_cols.index(Team_cols[0]); end = BBAB_cols.index(Team_cols[-1]) + 1\n",
    "    pair_str = [str(team) for team in bbab[start : end]]    # Team names are already normalized, removing/striping spaces.\n",
    "    pair_text = \" \".join(pair_str)\n",
    "    pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "    bbab[start : end] = pair_tokens # 0 for Unknown, by tokenizer trainig.\n",
    "\n",
    "    start = BBAB_cols.index(Goal_cols[0]); end = BBAB_cols.index(Goal_cols[-1]) + 1\n",
    "    bbab[start: end] = bbab[start : end]   # Goals themselves are good tokens, assuming there is no Unknown. mask_zero = False\n",
    "    \n",
    "    start = BBAB_cols.index(Result_cols[0]); end = BBAB_cols.index(Result_cols[-1]) + 1\n",
    "    bbab[start : end] = [ ['H', 'D', 'A'].index(result) for result in bbab[start : end]]    # Assumes no n/a\n",
    "\n",
    "    #--------------------- standardize\n",
    "    for cols in _Cols_List_to_Standardize:\n",
    "        start = BBAB_cols.index(cols[0]); end = BBAB_cols.index(cols[-1]) + 1\n",
    "        (mean, std, maximum) = std_params[cols[0]]\n",
    "        assert 0 <= min(bbab[start : end])\n",
    "        bbab[start : end] = [ (item - mean) / std for item in bbab[start : end] ]\n",
    "        assert - mean/std <= min(bbab[start : end])\n",
    "        assert max(bbab[start : end]) <= (maximum - mean) / std\n",
    "        # print('std', bbab[start : end])\n",
    "\n",
    "    #--------------------- columns for positional embedding\n",
    "    start = BBAB_cols.index(Date_cols[0])\n",
    "    date = bbab[start]\n",
    "    bbab[start] = (datetime.datetime.combine(date, datetime.time(0,0,0)) - config['baseDate']).days  # either positive or negative\n",
    "\n",
    "    #---------------------- bb only\n",
    "    start = BBAB_cols.index(BB_cols[0]); end = start + len(BB_cols)\n",
    "    bb = bbab[start : end]\n",
    "\n",
    "    #----------------------- return\n",
    "    \"\"\"\n",
    "    Div, Team, and Result are digitalized.\n",
    "    Date is converted to days since config['baseDate'])\n",
    "\n",
    "    bbab follows BBAB. list\n",
    "    bb follows BB. list\n",
    "    label follows fused _Cols_List_for_Label. list\n",
    "    date: single field\n",
    "    \"\"\"\n",
    "    return bbab, bb, label, date\n",
    "\n",
    "def getDateDetails(date):\n",
    "    baseYear = config['baseDate'].year\n",
    "    date_details = tf.Variable([date.year - baseYear, date.month, date.day, date.weekday()], dtype=tf.int32)\n",
    "    return date_details     # (4,)\n",
    "\n",
    "def get_data_record(df_total, baseId, ids, tokenizer_team, std_params):\n",
    "    # try:\n",
    "        # base_bbab = list(df_grown.loc[df_grown['id'] == baseId, BBAB_cols])\n",
    "        base_bbab = list(df_total[df_total['id'] == baseId][BBAB_cols].iloc[0, :])  # base_bbab follows BBAB. list\n",
    "        base_bbab, base_bb, base_label, base_date = normalize_raw_bbab(base_bbab, tokenizer_team, std_params)\n",
    "        # print('2', base_bbab)\n",
    "        baseId = tf.Variable(baseId, dtype=tf.int32)\n",
    "        base_bbab = tf.Variable(base_bbab, dtype=tf.float32)    # (len(BBAB_cols),)\n",
    "        base_bb = tf.Variable(base_bb, dtype=tf.float32)        # (len(BB_cols),)\n",
    "        base_label = tf.Variable(base_label, dtype=tf.float32)  # (len(_Label_cols),)\n",
    "        # print('3', base_bbab)\n",
    "        # Default sequence.\n",
    "        sequence = tf.transpose(tf.Variable([[]] * len(BBAB_cols), dtype=tf.float32))   # (0, len(BBAB_cols))\n",
    "        # sequence = np.array([[]] * len(BBAB_cols), dtype=config['np_float']).T\n",
    "        # print('3.5', sequence)\n",
    "        baseDateDetails = getDateDetails(base_date) # (4,)\n",
    "\n",
    "        concat = []\n",
    "        for id in ids:\n",
    "            bbab = list(df_total[df_total['id'] == id][BBAB_cols].iloc[0, :])   # bbab follows BBAB. list\n",
    "            # print('4', bbab)\n",
    "            bbab, _, _, _ = normalize_raw_bbab(bbab, tokenizer_team, std_params)   # bbab follows BBAB. list\n",
    "            # check_standardization(bbab, std_params)\n",
    "\n",
    "            bbab = tf.Variable(bbab, dtype=tf.float32)[tf.newaxis, :]       # (1, len(BBAB_cols))\n",
    "            # _bbab = bbab[0].numpy()\n",
    "            # check_standardization(_bbab, std_params)\n",
    "\n",
    "            concat.append(bbab)     # concat doesn't create a new axis.\n",
    "\n",
    "        if len(concat) > 0:\n",
    "            sequence = tf.concat(concat, axis=0)    # (nSequence, len(BBAB_cols))\n",
    "            # if sequence.shape[0] > 0:\n",
    "            #     bbab = sequence[0].numpy()\n",
    "            #     check_standardization(bbab, std_params)\n",
    "\n",
    "        # print('6', sequence)\n",
    "        \"\"\"\n",
    "        baseId: (). tf.int32\n",
    "        base_bbab follows BBAB. (len(BBAB_cols),). tf.float32\n",
    "        sequence[i, :] follows BBAB. (nSequence, len(BBAB_cols)). tf.float32\n",
    "        base_bb follows BB. (len(BB_cols),). tf.float32\n",
    "        base_label follows _Cols_List_for_Label. (len(_Label_cols),). tf.float32\n",
    "        baseDateDetails: tensor (yy, m, d, wd)\n",
    "        \"\"\"\n",
    "        return (baseId, base_bbab, sequence, base_bb, base_label, baseDateDetails)\n",
    "    # except:\n",
    "    #     raise Exception(\"Failed to get_BBAB for baseId = {}\".format(baseId))   \n",
    "\n",
    "def generate_dataset_uk(df_total, fixture_id_to_ids, tokenizer_team, std_params, df_new=None, train_mode=True):\n",
    "    if train_mode:\n",
    "        def generator():\n",
    "            count = 0\n",
    "            for baseId, (tag, label, ids) in fixture_id_to_ids.items():\n",
    "                baseId = int(baseId)\n",
    "                # print('0', baseId, ids)\n",
    "                (baseId, _, sequence, base_bb, base_label, baseDateDetails) = get_data_record(df_total, baseId, ids, tokenizer_team, std_params)\n",
    "                print(\"count: {}, baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "                \n",
    "                # if count > 500: break\n",
    "\n",
    "                yield (baseId, sequence, base_bb, base_label, baseDateDetails)\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32),\n",
    "            output_shapes=(tf.TensorShape(()), tf.TensorShape((None, len(BBAB_cols))), tf.TensorShape((len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((4,))),\n",
    "            args=()\n",
    "        )\n",
    "        return ds\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        def generator():\n",
    "            count = 0\n",
    "            for baseId, (tag, label, ids) in fixture_id_to_ids.items():\n",
    "                baseId = int(baseId)\n",
    "                # print('0', baseId, ids)\n",
    "                (baseId, _, sequence, base_bb, base_label, baseDateDetails) = get_data_record(df_total, baseId, ids, tokenizer_team, std_params)\n",
    "                print(\"count: {}, baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "                \n",
    "                # if count > 500: break\n",
    "\n",
    "                yield (baseId, sequence, base_bb, base_label, baseDateDetails)\n",
    "\n",
    "        ds = tf.data.Dataset.from_generator(\n",
    "            generator,\n",
    "            output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32),\n",
    "            output_shapes=(tf.TensorShape(()), tf.TensorShape((None, len(BBAB_cols))), tf.TensorShape((len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((4,))),\n",
    "            args=()\n",
    "        )\n",
    "        return ds\n",
    "\n",
    "def get_patch_bbab_tensor_uk(df_total, tokenizer_team, std_params):\n",
    "    # The first games bbab will be used as a patch bbab.\n",
    "    (baseId, base_bbab, sequence, base_bb, base_label, baseDateDetails) = get_data_record(df_grown, min(list(df_total['id'])), [], tokenizer_team, std_params)\n",
    "    patch_bbab = tf.zeros_like(base_bbab, dtype=tf.float32) # All embedding fields will be zero, which means Unknown for all but goal fields.\n",
    "    return patch_bbab\n",
    "\n",
    "\n",
    "std_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-normalization' + \".json\")\n",
    "std_params = get_standardization_params(df_grown)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)\n",
    "\n",
    "patch_bbab_tensor = get_patch_bbab_tensor_uk(df_grown, tokenizer_team, std_params)\n",
    "\n",
    "def normalize_dataset_row(baseId, sequence, base_bb, base_label, baseDateDetails):\n",
    "    \"\"\"\n",
    "    # return from get_data_record(.)\n",
    "    baseId: (). tf.int32\n",
    "    base_bbab follows BBAB. (len(BBAB_cols),). tf.float32\n",
    "    sequence[i, :] follows BBAB. (nSequence, len(BBAB_cols)). tf.float32\n",
    "    base_bb follows BB. (len(BB_cols),). tf.float32\n",
    "    base_label follows _Cols_List_for_Label. (len(_Label_cols),). tf.float32\n",
    "    baseDateDetails: tensor (yy, m, d, wd)\n",
    "\n",
    "    # call from normalize_dataset(.)\n",
    "    inp=[baseId, sequence, base_bb, base_label],\n",
    "    Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            # patch_bbab_tensor has the same type as base_bbab, or as sequence[i, :]\n",
    "            patch = tf.stack([patch_bbab_tensor] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, patch], axis=0)     # concat doesn't create a new axis. (MAX_TOKENS, len(BBAB_cols))\n",
    "        # print(\"sequence 1\", sequence.shape)\n",
    "        # sequence[:, 2] = base[2] - sequence[:, 2]   # get delta days.\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, len(BBAB_cols))\n",
    "        baseDateDetails = baseDateDetails[tf.newaxis, :]\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) # (MAX_TOKENS,) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]    # (MAX_TOKENS, MAX_TOKENS)\n",
    "        # print('normalize', sequence.shape, base.shape, mask.shape, mask)\n",
    "        # seq_len_org = tf.Variable(seq_len_org, dtype=tf.int32)\n",
    "        \"\"\"\n",
    "        baseId: (). tf.int32\n",
    "        sequence: (MAX_TOKENS, len(BBAB_cols)). tf.float32\n",
    "        base_bb: (seq_len = 1, len(BBAB_cols)). tf.float32\n",
    "        base_label: (len(_Label_cols),). tf.float32\n",
    "        mask: (MAX_TOKENS, MAX_TOKENS). tf.int32\n",
    "        seq_len_org: (). tf.int64 ? tf.int32    #-------------------------------------- Check it, though tf.int32 works.\n",
    "        \"\"\"\n",
    "        return (baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org)\n",
    "    except:\n",
    "        print('normalize_dataset_row exception')\n",
    "        print('norm 1', sequence.shape, base_bb.shape, base_label.shape, mask.shape, nMissings)\n",
    "        print('norm 2', baseId, sequence, base_label, mask, nMissings)\n",
    "        # return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "\n",
    "def normalize_dataset(ds, train_mode=True):\n",
    "    if train_mode:\n",
    "        return (     \n",
    "            ds.map(lambda baseId, sequence, base_bb, base_label, baseDateDetails: tf.py_function(  \n",
    "                func=normalize_dataset_row,\n",
    "                inp=[baseId, sequence, base_bb, base_label, baseDateDetails],\n",
    "                Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "            )\n",
    "    else:\n",
    "        return (     \n",
    "            ds.map(lambda baseId, sequence, base_bb, base_label, baseDateDetails: tf.py_function(  \n",
    "                func=normalize_dataset_row,\n",
    "                inp=[baseId, sequence, base_bb, base_label, baseDateDetails],\n",
    "                Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "            )    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_params = get_standardization_params(df_grown)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset')\n",
    "\n",
    "if os.path.exists(ds_path):\n",
    "    ds = tf.data.Dataset.load(ds_path)\n",
    "else:\n",
    "    ds = generate_dataset_uk(df_grown, id_to_ids, tokenizer_team, std_params, df_new=df_new)\n",
    "    ds = normalize_dataset(ds)\n",
    "    tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "    ds = tf.data.Dataset.load(ds_path)      # Wierd, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(ds)\n",
    "\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "take_size = total_size - starting_size - ending_size\n",
    "remaining_ds = ds.skip(starting_size)\n",
    "dataset = remaining_ds.take(take_size)          # starting | dataset | ending\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size - valid_size      # len(dataset) = train_size + valid_size + tast_size        NO back_size\n",
    "\n",
    "train_ds = dataset.take(train_size)                    # dataset[: train_size]\n",
    "remaining_ds = dataset.skip(train_size - valid_size)    # dataset[train_size - valid_size: ]\n",
    "\n",
    "back_ds = remaining_ds.take(valid_size)                # dataset[train_size - valid_size: train_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)            # dataset[train_size: ]\n",
    "\n",
    "valid_ds = remaining_ds.take(valid_size)               # dataset[train_size, train_size + valid_size]\n",
    "test_ds = remaining_ds.skip(valid_size)                # dataset[train_size + valid_size :]\n",
    "\n",
    "assert len(test_ds) == test_size\n",
    "\n",
    "assert dataset_size == len(train_ds) + len(valid_ds) + len(test_ds)\n",
    "\n",
    "print(total_size, len(dataset), len(train_ds), len(back_ds), len(valid_ds), len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_bbab_tensor = get_patch_bbab_tensor_uk(df_grown, tokenizer_team, std_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(baseId, sequence, base_bb, base_label, baseDateDetails, mask, seq_len_org):\n",
    "    # target = tf.one_hot(tf.squeeze(tf.cast(base_bbab[:, :, -1], dtype=tf.int32), axis=-1), hParams.target_onehot_size)\n",
    "    \"\"\"\n",
    "    baseId: (). tf.int32\n",
    "    sequence: (MAX_TOKENS, len(BBAB_cols)). tf.float32\n",
    "    base_bb: (seq_len = 1, len(BBAB_cols)). tf.float32\n",
    "    base_label: (len(_Label_cols),). tf.float32\n",
    "    baseDateDetails: tensor [yy, m, d, wd]\n",
    "    mask: (MAX_TOKENS, MAX_TOKENS). tf.int32\n",
    "    seq_len_org: (). tf.int64 ? tf.int32    #-------------------------------------- Check it, though tf.int32 works.\n",
    "    \"\"\"\n",
    "    return (baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org)     # (X, Y)\n",
    "\n",
    "# I found, in PositionalEmbedding, batch size ranges between 3, 4, 6 and 8, while they should be 4 or 8, except margial rows. Check it.\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def apply_train_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)       # Shuffle the training dataset.\n",
    "        .batch(train_batch_size)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def apply_test_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batches = apply_train_pipeline(train_ds)\n",
    "len(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 3\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org) = z\n",
    "    cnt -= 1\n",
    "    if cnt == 0: break\n",
    "print(baseId.shape, sequence.shape, base_bb.shape, mask.shape, base_label.shape, baseDateDetails.shape, seq_len_org.shape)\n",
    "sample_x = (sequence, base_bb, baseDateDetails, mask)\n",
    "sample_y = (base_label, seq_len_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_batches = apply_test_pipeline(back_ds)\n",
    "len(back_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_batches = apply_test_pipeline(valid_ds)\n",
    "len(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = apply_test_pipeline(test_ds)\n",
    "len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(positions, d_model):     # positions.shape = (batch, seq_len). d_model is a positive even integer.\n",
    "    # positions[b, :] = [dago_299, dago_298, ..., dago_0] = eg [6, 7, ..., 1611]\n",
    "\n",
    "    years = 25\n",
    "    quotient = 365 * years / d_model   # 325 * 25 is total days during 25 years since 2024 (the earliest data year). \n",
    "    fractional_pos = positions / quotient  # (batch, seq_len). max << d_model.  lower=0, upper<<d_model\n",
    "    depth = d_model/2   #\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth  # (depth,). [0/depth, 1/depth, ..., (depth-1)/depth]\n",
    "\n",
    "    # Why do we use this specific formula for depths, while any formula will work?\n",
    "    # depths will be multiplied by fractional_pos, which may have ever large number.\n",
    "    # We want the product to be limited, and depths should converge to zero to limit the product.\n",
    "    # If it cenverges to zero too fast, though, the generated pos_encoding will have little representative power as positional representation.\n",
    "    # Why, then, do we want to limit the product?\n",
    "    # The product visiting many different values will be enough for us. The solution is to let them go to zero, from a significant value.\n",
    "    BIG = d_model * 1.0 * 0.8     # let it be float.\n",
    "    depths = 1.0 / tf.pow(BIG, depths)        # 1 / [1, ..., BIG ** ((depth-1)/depth)] = [1, ..., >1/BIG]\n",
    "\n",
    "    # Why fractional_pos is multiplied linearly?\n",
    "    # Because (sin(A+a), cos(A+a)) is a rotation of (sin(A), cos(A)), no matter what depths is.\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] * depths  # [0, ..., <<d_model ] * [ 1, ..., >1/BIG]. \n",
    "    # Interleaving sin and cos is equivalent to seperated sin and cos.\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')  # Interleaving sin and cos. (batch, seq_len, d_model)\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)   # Seperated sin and cos. (batch, seq_len, d_model)\n",
    "    return pos_encoding  # (batch, seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = 15\n",
    "seq_len = 300\n",
    "d_model = 184\n",
    "positions = tf.Variable([[i for i in range(0, 365 * years, int(365 * years / seq_len))] for batch in range(100)], dtype=tf.float32)\n",
    "# positions = tf.ones((100, 200), dtype=tf.float32) * tf.range(200, dtype=tf.float32)\n",
    "pos_encoding = positional_encoding(positions, d_model=d_model)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "          base_bbab follows BBAB\n",
    "          sequence[i, :] follows BBAB\n",
    "          base_bb follows BB\n",
    "          base_label follows _Cols_List_for_Label\n",
    "          All tf.float32.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hParams, isEncoder=True):\n",
    "        super().__init__()\n",
    "        self.isEncoder = isEncoder\n",
    "        self.division_emb = tf.keras.layers.Embedding(hParams.nDivisions, hParams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.team_emb = tf.keras.layers.Embedding(hParams.nTeams, hParams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        \n",
    "        self.shoot_emb = tf.keras.layers.Embedding(hParams.nShoots * hParams.nShoots, hParams.shoot_embs, dtype=tf.float32, mask_zero=False)\n",
    "        self.shootT_emb = tf.keras.layers.Embedding(hParams.nShootTs * hParams.nShootTs, hParams.shootT_embs, dtype=tf.float32, mask_zero=False)\n",
    "        self.corner_emb = tf.keras.layers.Embedding(hParams.nCorners * hParams.nCorners, hParams.corner_embs, dtype=tf.float32, mask_zero=False)\n",
    "        self.faul_emb = tf.keras.layers.Embedding(hParams.nFauls * hParams.nFauls, hParams.faul_embs, dtype=tf.float32, mask_zero=False)\n",
    "        self.yellow_emb = tf.keras.layers.Embedding(hParams.nYellows * hParams.nYellows, hParams.yellow_embs, dtype=tf.float32, mask_zero=False)\n",
    "        self.red_emb = tf.keras.layers.Embedding(hParams.nReds * hParams.nReds, hParams.red_embs, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        self.firstH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn goals 0\n",
    "        self.secondH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn goals 0\n",
    "        self.result_emb = tf.keras.layers.Embedding(hParams.nResult, hParams.result_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.day_emb = tf.keras.layers.Embedding(31, 2, dtype=tf.float32, mask_zero=False)\n",
    "        self.month_emb = tf.keras.layers.Embedding(12, 2, dtype=tf.float32, mask_zero=False)\n",
    "        self.wday_emb = tf.keras.layers.Embedding(7, 2, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        self.d_model = hParams.d_model\n",
    "        # print(self.d_model)\n",
    "        self.position_permuting_dense = tf.keras.layers.Dense(self.d_model)\n",
    "\n",
    "        self.idx_Days = BB_cols.index('Date')\n",
    "\n",
    "    def representDateDetails(self, dateDetails):\n",
    "        # dateDetails: (batch, 1, 4)\n",
    "        bYears, bMonths, bDays, bWDays = tf.split(dateDetails, [1, 1, 1, 1], axis=-1)   # All should be of (batch, seq_len = 1, 1)\n",
    "        bYears = tf.cast(bYears, dtype=tf.float32)  # (batch, seq_len = 1, 1)\n",
    "        bDays = self.day_emb(bDays)[:, :, -1]       # (batch, seq_len = 1, embs = 2)\n",
    "        bMonths = self.month_emb(bMonths)[:, :, -1] # (batch, seq_len = 1, embs = 2)\n",
    "        bWDays = self.wday_emb(bWDays)[:, :, -1]    # (batch, seq_len = 1, embs = 2)\n",
    "        # w = tf.Variable(np.math.pi / 25, dtype=tf.float32)    # 25 years are covered by pi or a half circle.\n",
    "        w = np.math.pi / 25\n",
    "        bYearsCos = tf.math.cos(bYears * w)\n",
    "        bYearsSin = tf.math.sin(bYears * w)\n",
    "        bYears = tf.concat([bYearsCos, bYearsSin], axis=-1)   # (batch, seq_len = 1, 1+1 = 2)\n",
    "        return bYears, bMonths, bDays, bWDays\n",
    "\n",
    "    def combined_embeddings_of_double_columns(self, emb_layer, columns, nValues):\n",
    "        # Assume emb_layer = Embedding(nValues * nValues, embs, mask_zero=False)\n",
    "        cols = tf.cast(columns, dtype=tf.int32)\n",
    "        cols = tf.clip_by_value(cols, 0, nValues-1)\n",
    "        combi = cols[:, :, 0] * nValues + cols[:, :, 1]   # (batch, seq_len, 1). [0, ..., nValues * nValues - 1]\n",
    "        combi = emb_layer(combi)\n",
    "        return combi    # (batch, seq_len, 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        # returns from prepare_batch(.):\n",
    "        baseId: (). tf.int32\n",
    "        sequence: (MAX_TOKENS, len(BBAB_cols)). tf.float32\n",
    "        base_bb: (seq_len = 1, len(BBAB_cols)). tf.float32\n",
    "        base_label: (len(_Label_cols),). tf.float32\n",
    "        baseDateDetails: tensor [yyyy-2020, m, d, wd]\n",
    "        mask: (MAX_TOKENS, MAX_TOKENS). tf.int32\n",
    "        seq_len_org: (). tf.int64 ? tf.int32    #-------------------------------------- Check it, though tf.int32 works.\n",
    "      \n",
    "        return (baseId, sequence, base_bb, mask), (base_label, seq_len_org)     # (X, Y)\n",
    "\n",
    "        prepare_batch(.) prepends the 'batch' axix.\n",
    "\n",
    "        BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        AB_cols = Half_Goal_cols + Full_Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "        \"\"\"\n",
    "\n",
    "        (sequence, base_bb, baseDateDetails, mask) = x # sob = sequence or base_bb\n",
    "        sequenceDays = sequence[:, :, self.idx_Days]  # (batch, seq_len)\n",
    "        baseDays = base_bb[:, :, self.idx_Days]   # (batch, 1)\n",
    "\n",
    "        # sequence follows BBAB, whereas base_bb follows \n",
    "        \n",
    "        # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        if self.isEncoder:\n",
    "            # ramainder: Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols  --- total 12 fields.\n",
    "            id, div, days, teams, odds, half_goals, full_goals, result, shoot, shootT, corner, faul, yellow, red \\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), len(Result_cols), \\\n",
    "                                  len(Shoot_cols), len(ShootT_cols), len(Corner_cols), len(Faul_cols), len(Yellow_cols), len(Red_cols),], axis=-1)\n",
    "            # All shape of (batch, sequence, own_cols), all tf.flaot32\n",
    "        else:\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "            # remainder: [] \n",
    "            # All shape of (batch, 1, own_cols), guess., all tf.float32\n",
    "\n",
    "        div = self.division_emb(tf.cast(div, dtype=tf.int32))   # (batch, sequence, columns=1, division_embs)\n",
    "        div = tf.reshape(div, [div.shape[0], div.shape[1], -1]) # (batch, sequence, extended_columns=1*division_embs) --- \n",
    "        teams = self.team_emb(tf.cast(teams, dtype=tf.int32))   # (batch, sequence, columns=2, team_embs)\n",
    "        teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1]) # (batch, sequence, extended_columns=2*team_embs) --- \n",
    "        if self.isEncoder:\n",
    "            first_half_goals = self.combined_embeddings_of_double_columns(self.firstH_goal_emb, half_goals, hParams.nGoals)\n",
    "            second_half_goals = self.combined_embeddings_of_double_columns(self.secondH_goal_emb, full_goals - half_goals, hParams.nGoals)\n",
    "            result = self.result_emb(tf.cast(result, dtype=tf.int32))   # (batch, sequence, columns=2, result_embs)\n",
    "            result = tf.reshape(result, [result.shape[0], result.shape[1], -1])   # (batch, sequence, extended_columns=2*result_embs) --- \n",
    "            shoot = self.combined_embeddings_of_double_columns(self.shoot_emb, shoot, hParams.nShoots)\n",
    "            shootT = self.combined_embeddings_of_double_columns(self.shootT_emb, shootT, hParams.nShootTs)\n",
    "            corner = self.combined_embeddings_of_double_columns(self.corner_emb, corner, hParams.nCorners)\n",
    "            faul = self.combined_embeddings_of_double_columns(self.faul_emb, shoot, hParams.nFauls)\n",
    "            yellow = self.combined_embeddings_of_double_columns(self.yellow_emb, yellow, hParams.nYellows)\n",
    "            red = self.combined_embeddings_of_double_columns(self.red_emb, red, hParams.nReds)\n",
    "        \n",
    "        # --------- Modify this block. Let concat be of any size, even or odd. -----------------\n",
    "        if self.isEncoder:\n",
    "            concat = [div, teams, first_half_goals, second_half_goals, result, odds, shoot, shootT, corner, faul, yellow, red]\n",
    "        else:\n",
    "            bYears, bMonths, bDays, bWDays = self.representDateDetails(baseDateDetails)\n",
    "            concat = [div, teams, odds, remainder, bYears, bMonths, bDays, bWDays]\n",
    "\n",
    "        concat = tf.concat(concat, axis=-1)           # (batch, sequence, SOME_LENGTH that is less that hParam.d_model)\n",
    "        assert concat.shape[-1] <= self.d_model  \n",
    "\n",
    "        \"\"\"\n",
    "        Will this disturb the transformer?  Sure no.\n",
    "        Transformer learns to combine x[batch, position, field] across 'positions' with the weights being the attention between positions, and as well \n",
    "        combine across 'field' with its feed-foward layers. \n",
    "        This line learns to combine x[batch, position, field] across 'field' helping to overcome the inequality, over 'field' axis, of the PE alorithm.\n",
    "        The PE algorithm actually encodes x in both axies - 'position' and 'field'. But the PE values diminish rapidly for larger field indices.\n",
    "        This means the PE algo doesn't take equal care of x values with different field indices.\n",
    "        This line learns to combine x values across different 'field' indices. It learns which 'field' indices should the PE algo take how much care.\n",
    "        \"\"\"\n",
    "        concat = self.position_permuting_dense(concat)  # (batch, sequence, hParams.d_model)\n",
    "\n",
    "        positions = tf.cast(baseDays - sequenceDays, dtype=tf.float32) if self.isEncoder else tf.cast(baseDays - baseDays, dtype=tf.float32) # the latter is a zero tensor.\n",
    "        # eg. positions[b, :] = Tensor([6, 7, ..., 1911]\n",
    "        pe = positional_encoding(positions, d_model=concat.shape[-1]) # (batch, seq_len = MAX_TOKENS, hParams.d_model)\n",
    "        pe = pe / tf.math.sqrt(tf.cast(concat.shape[-1], tf.float32))   # Read \"Attention is all you need\"\n",
    "        concat = concat + pe\n",
    "\n",
    "        if self.isEncoder:\n",
    "            mask = mask\n",
    "        else:\n",
    "            mask = mask[:, 0:concat.shape[1], :]\n",
    "\n",
    "        return (concat, mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hParams, isEncoder=True)\n",
    "eSob, eMask = PE(sample_x)\n",
    "print(eSob.shape, eMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hParams, isEncoder=False)\n",
    "dSob, dMask = PE(sample_x)\n",
    "print(dSob.shape, dMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axix is normalized across. No inter-token operatoin.\n",
    "      self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(\n",
    "          query=x,\n",
    "          key=context,\n",
    "          value=context,\n",
    "          attention_mask=mask,\n",
    "          return_attention_scores=True)\n",
    "    \n",
    "      # Cache the attention scores for plotting later.\n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          attention_mask=mask)    # intentional inter-token operation\n",
    "      x = self.add([x, attn_output])  # token-wise\n",
    "      x = self.layernorm(x)         # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          use_causal_mask = True)     # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])  # mask-agnostic\n",
    "      x = self.layernorm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),    # across -1 axis\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])  # mask-agnostic\n",
    "      x = self.layer_norm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=True)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hParams.d_model,\n",
    "                      num_heads=hParams.num_heads,\n",
    "                      dff=hParams.d_model * 4,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (token, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_emb(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "\n",
    "      self.causal_self_attention = CausalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "      \n",
    "      self.cross_attention = CrossAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "\n",
    "      # Cache the last attention scores for plotting later\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "\n",
    "      self.pos_emb = PositionalEmbedding(hParams, isEncoder=False)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads,\n",
    "                      dff=hParams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (batch, 1, 4), x[3]: (token, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_emb(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens)     \n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hParams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.decoder = Decoder(hParams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(hParams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, inputs):\n",
    "      # inputs = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), baseDateDetails: (batch, 1, 4), mask: (batch, max_token, max_token)\n",
    "      x = self.encoder(inputs)  # (batch, max_tokens, d_model)\n",
    "      x = self.decoder(inputs, x)  # (batch, 1, d_model)\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(hParams)\n",
    "y = sample_transformer(sample_x)\n",
    "\n",
    "sample_transformer.summary()\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for earlier versions that don't allow mixing bookies.\n",
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hParams.d_model * 2] * nLayers\n",
    "    dims = dims + [hParams.d_model * 2 + round( (d_output - hParams.d_model * 2) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, kernel_initializer=tf.keras.initializers.LecunUniform(), activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x\n",
    "  \n",
    "class MixedAdaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hParams.d_model * 2] * nLayers\n",
    "    dims = dims + [hParams.d_model * 2 + round( (d_output - hParams.d_model * 2) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, kernel_initializer=tf.keras.initializers.LecunUniform(), activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x\n",
    "\n",
    "class BaseAdaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hParams.d_model] * nLayers + [hParams.d_model + round( (d_output - hParams.d_model) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x\n",
    "\n",
    "class OhAdaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [d_output] * nLayers\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1X2(tf.keras.Model):\n",
    "    softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.nQueries = nQueries\n",
    "        self.transformer = Transformer(hParams, dropout_rate=dropout_rate)\n",
    "        #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "        self.bookies = ['HDA' + str(b) for b in range(NUMBER_BOOKIES)]\n",
    "        self.baseAdaptors = [Adaptor(ADAPTORS_LAYERS, self.nQueries) for _ in self.bookies]\n",
    "        return\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.transformer(x)\n",
    "        stake_p = [adaptor(x) for adaptor in self.baseAdaptors]  # [(batch, nQueries)] * nBookies\n",
    "        stake_p = tf.stack(stake_p, axis=0)   # (nBookies, batch, nQueries)\n",
    "        stake_p = Model_1X2.softmax(stake_p)  # (nBookies, batch, nQueries)\n",
    "        return stake_p\n",
    "    \n",
    "    def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "        # ftGoals:  (batch, 2)\n",
    "        ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "        h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "        h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "        return h\n",
    "\n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        \n",
    "        profit_backtest = tf.reduce_mean(tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1), axis=None)  # () \n",
    "        loss = - profit_backtest  # U.action.42\n",
    "    \n",
    "        return loss # (), negative average profit on a game on a bookie\n",
    "    \n",
    "    def backtest(self, y, output, profit_keys):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: oh_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        profit_backtest = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "        profit_list = []; cast_list = []\n",
    "        profit = MIN_PROFIT\n",
    "        for profit_key in profit_keys:\n",
    "            best_profit_backtest = profit_backtest[profit_p >= profit_key]  # [(bookie, game) where profit_p >= key]\n",
    "            cast = best_profit_backtest.shape[-1]\n",
    "            if cast > 0: profit = tf.math.reduce_mean(best_profit_backtest)\n",
    "            profit_list.append(float(profit))\n",
    "            cast_list.append(cast)\n",
    "        return profit_list, cast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = Model_1X2(hParams, 3, dropout_rate=DROPOUT)\n",
    "\n",
    "stake_p = model_1x2(sample_x, training=True)\n",
    "loss = model_1x2.loss(sample_y[0], stake_p)\n",
    "\n",
    "profits, casts = model_1x2.backtest(sample_y[0], stake_p, PROFIT_KEYS)\n",
    "# print(profit_list)\n",
    "# print(nBettings_list)\n",
    "\n",
    "model_1x2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model_1x2, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss_uk(label, y_pred):\n",
    "  # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch, 1)), y_pred: (batch, 3)\n",
    "  y_true = label[0]   # one_hot: (batch, 3)\n",
    "  seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "  mask = y_true != 0 \n",
    "  loss = loss_object(y_true, y_pred)\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask) # eq. sum_loss / batch\n",
    "  return loss\n",
    "\n",
    "\n",
    "class recall():\n",
    "  def __init__(self, name='recall', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    # print('recall', y_true.shape, y_pred.shape, seq_len_mask.shape)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    # print('recall', true_positives.numpy())\n",
    "    possible_positives = tf.math.reduce_sum(y_true)\n",
    "    recall_keras = true_positives / (possible_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall_keras.numpy() / self.n\n",
    "\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.math.reduce_sum(y_pred)\n",
    "    precision_keras = true_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision_keras.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def backtest_step(x, y, profit_keys):\n",
    "    outputs = model_1x2(x, training=False)  #\n",
    "    profits, casts = model_1x2.backtest(y, outputs, profit_keys)\n",
    "    # print('key', profit_back_mean_per_betting, nBettings)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def backtest_with_dataset(dataset, profit_keys):\n",
    "    profits = [MIN_PROFIT] * len(profit_keys)\n",
    "    casts = [0] * len(profit_keys)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = base_label\n",
    "        profit_list, cast_list = backtest_step(x, y, profit_keys)\n",
    "\n",
    "        for p, c, id in zip(profit_list, cast_list, range(len(profit_keys))):\n",
    "            if c > 0:\n",
    "                profits[id] = (profits[id] * casts[id] + p * c) / (casts[id] + c)\n",
    "                casts[id] = casts[id] + c\n",
    "    # print('key', profit_back_mean, nBettingsTotal)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def inference_step(x, odds, interval_a, interval_b):\n",
    "    stake_p = model_1x2(x, training=False)    # (nBookies, batch, nQueries)\n",
    "    nQueries = stake_p.shape[-1]\n",
    "    profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "    bet_bool = interval_a <= profit_p and profit_p >= interval_b    # (nBookies, batch)\n",
    "    bet_bool = tf.stack([bet_bool] * nQueries, axis=-1) # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.math.multiply(stake_p, tf.cast(bet_bool, dtype=tf.float32))   # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.reshape(stake_vector, [1, 0, 2])  # (batch, nBookies, nQueries)\n",
    "    return stake_vector\n",
    "\n",
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def inference_with_dataset(dataset, interval_a, interval_b): \n",
    "    vectors = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)\n",
    "        stake_vector = inference_step(x, odds, interval_a, interval_b)  # (batch, nBookies, nQueries)\n",
    "        vectors.append(stake_vector)\n",
    "    \n",
    "    stake_vectors = tf.concat(vectors, axis=0)   # (batch, nBookies, nQueries)\n",
    "    return stake_vectors    # (batch, nBookies, nQueries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'loss': [], 'val_loss': [], 'learning_rate': []}\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.history? ---------------------- Check it.\n",
    "        self.history = {'loss': [], 'val_loss': [], 'learning_rate': []}\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = data_helpers.LoadJsonData(self.filepath)\n",
    "        if history is not None:\n",
    "            self.history = history\n",
    "    def append(self, loss, val_loss, learning_rate):\n",
    "        self.history['loss'].append(self.round_sig(float(loss), 4))\n",
    "        self.history['val_loss'].append(self.round_sig(float(val_loss), 4))\n",
    "        self.history['learning_rate'].append(learning_rate)\n",
    "        self.save()\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['learning_rate'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_latest_item(self):\n",
    "        return (self.history['loss'][-1], self.history['val_loss'][-1], self.history['learning_rate'][-1])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "\n",
    "    def show(self, ax):\n",
    "        ax.set_title(TEST_ID + \": loss history\")\n",
    "        ax.plot(self.history['loss'])\n",
    "        ax.plot(self.history['val_loss'])\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend(['train_loss', 'val_loss'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            # return x\n",
    "            return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, class_name, profit_keys, data_batches, nDataRows, nBookies, filepath):\n",
    "        self.class_name = class_name\n",
    "        self.profit_keys = profit_keys\n",
    "        self.data_batches = data_batches\n",
    "        self.nTests = nDataRows * nBookies\n",
    "        self.filepath = filepath\n",
    "        self.profits = {str(key): [] for key in self.profit_keys}\n",
    "        self.casts = {str(key): [] for key in self.profit_keys}\n",
    "        self.profits_round = self.casts_round = None\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData([self.profits, self.casts], self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.profits and self.casts? ------------------------ Check it.\n",
    "        self.profits = {str(key): [] for key in self.profit_keys}\n",
    "        self.casts = {str(key): [] for key in self.profit_keys}\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        test = data_helpers.LoadJsonData(self.filepath)\n",
    "        if test is not None:\n",
    "            [self.profits, self.casts] = test\n",
    "    def getLen(self, dict):\n",
    "        length = None\n",
    "        try:\n",
    "            for key, value in dict.items():\n",
    "                if length is None:\n",
    "                    length = len(value)\n",
    "                else:\n",
    "                    assert len(value) == length\n",
    "            return length\n",
    "        except:\n",
    "            raise Exception(\"Un-uniform length in  distribution\")      \n",
    "\n",
    "    def len(self):\n",
    "        assert len(self.profits) == len(self.profit_keys)\n",
    "        assert len(self.casts) == len(self.profit_keys)\n",
    "        length = self.getLen(self.profits)\n",
    "        assert self.getLen(self.casts) == length\n",
    "        return length\n",
    "\n",
    "    def append(self, profits, casts):\n",
    "        _ = self.len()     # for all the asserts.\n",
    "        assert len(profits) == len(self.profit_keys)\n",
    "        assert len(casts) == len(self.profit_keys)\n",
    "        for item_list, item in zip(self.profits.values(), profits):\n",
    "            item_list.append(item)\n",
    "\n",
    "        assert len(casts) == len(self.profit_keys)\n",
    "        for item_list, item in zip(self.casts.values(), casts):\n",
    "            item_list.append(item)\n",
    "\n",
    "        self.save()\n",
    "\n",
    "    def get_best_product(self, profits, casts):\n",
    "        best_product = -float('inf') # MIN_PROFIT * 1e6\n",
    "        for (p, n) in zip(profits, casts):\n",
    "            if p * n > best_product:\n",
    "                best_product = p * n\n",
    "        return best_product\n",
    "    \n",
    "    def get_best_product2(self):\n",
    "        return self.get_best_product(self.profits_round, self.casts_round)\n",
    "\n",
    "    def get_existing_best_product(self):\n",
    "        all_profits = []\n",
    "        for item_list in self.profits.values():\n",
    "            all_profits += item_list\n",
    "        all_casts = []\n",
    "        for item_list in self.casts.values():\n",
    "            all_casts += item_list\n",
    "        return self.get_best_product(all_profits, all_casts)\n",
    "    \n",
    "    def find_profit_cast_series(self):\n",
    "        nSeries = len(tuple(self.profits.values())[0])\n",
    "        for v in self.profits.values():\n",
    "            assert len(v) == nSeries\n",
    "        for v in self.casts.values():\n",
    "            assert len(v) == nSeries\n",
    "        \n",
    "        profit_series =  [[v[serial] for v in self.profits.values()] for serial in range(nSeries)] # [ [ profit for _ in profit_keys ] ] * nSeries\n",
    "        cast_series =  [[v[serial] for v in self.casts.values()] for serial in range(nSeries)] # [ [ cast for _ in profit_keys ] ] * nSeries\n",
    "        return profit_series, cast_series\n",
    "       \n",
    "    def find_total_profit_groups(self):       \n",
    "        profit_series, cast_series = self.find_profit_cast_series()\n",
    "        total_profit_groups = []\n",
    "        for profits, casts in zip(profit_series, cast_series):\n",
    "            profit_groups = self.find_profit_groups(profits, casts, sort=False)\n",
    "            total_profit_groups.append(profit_groups)   # [ [ (product, profit, cast, id_string) for nGroups ] ] * nSeries\n",
    "        return total_profit_groups\n",
    "    \n",
    "    def track_profit_groups(self, total_profit_groups):\n",
    "        # total_profit_groups: [ [ (product, profit, cast, id_string) for nGroups ] ] * nSeries\n",
    "        profit_groups_track = None\n",
    "        nSeries = len(total_profit_groups)\n",
    "        if nSeries > 0:\n",
    "            nGroups = len(total_profit_groups[0])\n",
    "            for profit_groups in total_profit_groups:\n",
    "                assert len(profit_groups) == nGroups\n",
    "            profit_groups_track = [[total_profit_groups[series][profit_group] for series in range(nSeries)] for profit_group in range(nGroups)]\n",
    "        return profit_groups_track  # [ [ (product, profit, cast, id) for _ in range(nSeries)] for _ in range(nGroups) ]\n",
    "        # profit_groups_track = { profit_groups_track[group][0][3] : [(profit, cast) for _, profit, cast, _ in profit_groups_track[group]] for group in range(nGroups) }\n",
    "        # return profit_groups_track  # { id : [ (profit, cast) for _ in range(nSeries)] for _ in range(nGroups) }\n",
    "    \n",
    "    def find_profit_groups(self, profits, casts, sort=True):\n",
    "        result = []\n",
    "        for n1 in range(len(self.profit_keys)):\n",
    "            for n2 in range(n1, len(self.profit_keys)): \n",
    "                # n2 >= n1. profit_keys[n2] >= profit_keys[n1], casts[n2] <= casts[n1]\n",
    "                if n1 == n2:\n",
    "                    result.append((profits[n1] * casts[n1], profits[n1], casts[n1], str(self.profit_keys[n1])+\"-\"))\n",
    "                else:\n",
    "                    cast3 = casts[n1] - casts[n2]\n",
    "                    if cast3 > 0:\n",
    "                        profit3 = (profits[n1] * casts[n1] - profits[n2] * casts[n2]) / cast3\n",
    "                    else:\n",
    "                        profit3 = MIN_PROFIT\n",
    "                    result.append( (profit3 * cast3, profit3, cast3, str(self.profit_keys[n1])+\"-\"+str(self.profit_keys[n2])))\n",
    "        if sort: result.sort(reverse=True)\n",
    "        return result\n",
    "    \n",
    "    def find_profit_groups2(self, sort=True):\n",
    "        return self.find_profit_groups(self.profits_round, self.casts_round, sort)\n",
    "    \n",
    "    def find_profit_groups_elements(self, profits, casts, sort=True):\n",
    "        result = []\n",
    "        for n1 in range(len(self.profit_keys)-1):\n",
    "            n2 = n1 + 1\n",
    "            cast3 = casts[n1] - casts[n2]\n",
    "            if cast3 > 0:\n",
    "                profit3 = (profits[n1] * casts[n1] - profits[n2] * casts[n2]) / cast3\n",
    "            else:\n",
    "                profit3 = MIN_PROFIT\n",
    "            result.append( (profit3 * cast3, profit3, cast3, str(self.profit_keys[n1])+\"-\"+str(self.profit_keys[n2])))\n",
    "        if sort: result.sort(reverse=True)\n",
    "        return result\n",
    "    \n",
    "    def find_profit_groups_elements2(self, sort=True):\n",
    "        return self.find_profit_groups_elements(self.profits_round, self.casts_round, sort)\n",
    "    \n",
    "    def print_profit_groups(self, groups, count):\n",
    "        # groups: [ (product, profit, cast, interval) ] * n\n",
    "        print(self.class_name, end=', ')\n",
    "        for (product, profit, cast, interval) in groups:\n",
    "            print(\"[{:.5f}, {:.4f}, {}, {}]\".format(product, profit, cast, interval), end=', ')\n",
    "            count -= 1\n",
    "            if count <= 0:\n",
    "                print(); break\n",
    "            \n",
    "    # def show_profit_distribution(self):\n",
    "\n",
    "\n",
    "    def show_profit_groups(self, minProduct=0.0):\n",
    "        total_profit_groups = self.find_total_profit_groups()   # [ [ (product, profit, cast, group_id) for nGroups ] ] * nSeries\n",
    "        if len(total_profit_groups) < 1: return\n",
    "\n",
    "        profit_groups_track = self.track_profit_groups(total_profit_groups) # [ [ (product, profit, cast, group_id) for _ in range(nSeries)] for _ in range(nGroups) ]\n",
    "\n",
    "        nGroups = len(total_profit_groups[0])\n",
    "        for profit_groups in total_profit_groups:\n",
    "            assert len(profit_groups) == nGroups\n",
    "        profit_groups_track = { profit_groups_track[group][0][3] : [(profit, cast) for _, profit, cast, _ in profit_groups_track[group]] for group in range(nGroups) }\n",
    "        # { group_id : [ (profit, cast) for _ in range(nSeries)] for _ in range(nGroups) }\n",
    "        \n",
    "        minCasts = self.nTests; maxCasts = 0\n",
    "        minProfit = 50.0; maxProfit = MIN_PROFIT\n",
    "        for key, value in profit_groups_track.items():\n",
    "            casts = [cast for _, cast in value]\n",
    "            profits = [profit for profit, _ in value]\n",
    "            # if profits[-1] * casts[-1] > minProduct:\n",
    "            if key.endswith('-'):\n",
    "                if minCasts > min(casts): minCasts = min(casts)\n",
    "                if maxCasts < max(casts): maxCasts = max(casts)\n",
    "                if minProfit > min(profits): minProfit = min(profits)\n",
    "                if maxProfit < max(profits): maxProfit = max(profits)\n",
    "\n",
    "        step = 5; x = np.arange(minCasts, maxCasts + step, step).reshape(-1, 1)\n",
    "        step = 0.0005; y = np.arange(minProfit, maxProfit + step, step).reshape(-1, 1)\n",
    "        X, Y = np.meshgrid(x, y)    # (n, m)\n",
    "        XY = np.stack((X, Y), axis=-1)  # (n, m, 2)\n",
    "        Z = XY[:, :, 0] * XY[:, :, 1]   # (n, m)\n",
    "\n",
    "        if Z.shape[0] >= 2 and Z.shape[1] >= 2:\n",
    "            sLevels = (0) #, 1, 2, 3, 4, 5) if GUI.loss == 'mean_squared_error' else (0,)\n",
    "            sColors = ['r'] # GUI.colors[: len(sLevels)]\n",
    "            nContours = 80\n",
    "            plt.figure(figsize=(12,8))\n",
    "            CS0 = plt.contourf(X, Y, Z, nContours, cmap=plt.cm.bone, origin='lower')\n",
    "            CS = plt.contour(X, Y, Z, CS0.levels, colors=('k'), origin='lower', linewidths=.2)\n",
    "            plt.contour(X, Y, Z, sLevels, colors=sColors, origin='lower', linewidths=.5)    \n",
    "            plt.clabel(CS, fmt='%1.1f', colors='c', fontsize=8, inline=True)\n",
    "\n",
    "        for key, value in profit_groups_track.items():\n",
    "            casts = [cast for _, cast in value]\n",
    "            profits = [profit for profit, _ in value]\n",
    "            # if profits[-1] * casts[-1] > minProduct:\n",
    "            if key.endswith('-'):\n",
    "                plt.plot(casts, profits, label=key, marker='o', lw=0.5)\n",
    "                plt.plot(casts[-1], profits[-1], marker='o', color='k')\n",
    "                plt.plot(casts[-1], profits[-1], marker='x', color='w')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def show(self, ax):\n",
    "        colors = ['black', 'firebrick', 'darkgreen', 'c', 'blue', 'blueviolet', 'magenta', 'maroon', \"yellowgreen\", 'cadetblue', 'purple', 'c', 'blue']\n",
    "\n",
    "        gmin = MIN_PROFIT - 1.0; gmax = MIN_PROFIT\n",
    "        all_profits = []\n",
    "        for item_list in self.profits.values():\n",
    "            all_profits += item_list\n",
    "        if len(all_profits) > 0:\n",
    "            gmin = min(all_profits); gmax = max(all_profits)\n",
    "\n",
    "        _min = 0.0; _max = self.nTests        \n",
    "        # _min = 0.0; _max = 1.0\n",
    "        # all_nBettings = []\n",
    "        # for item_list in self.nBettings.values():\n",
    "        #     all_nBettings += item_list\n",
    "        # if len(all_nBettings) > 0:\n",
    "        #     _min = min(all_nBettings); _max = max(all_nBettings)\n",
    "        \n",
    "        legends = []\n",
    "        for item_list, color, key in zip(self.profits.values(), colors[:len(self.profit_keys)], self.profit_keys):\n",
    "            # print(item_list, color, key)\n",
    "            ax.plot(item_list, color=color, linewidth=0.7)\n",
    "            legends.append(\"> \" + str(key))\n",
    "        # print(legends)\n",
    "\n",
    "        for item_list, color in zip(self.casts.values(), colors[:len(self.profit_keys)]):\n",
    "            item_list = [ (item-_min)/(_max-_min+1e-9) * (gmax-gmin) + gmin for item in item_list]\n",
    "            ax.plot(item_list, color=color, linestyle='--', linewidth=0.7)\n",
    "\n",
    "        ax.legend(legends, loc='upper left')\n",
    "        ax.grid(True)\n",
    "        ax.set_title(TEST_ID + \": avg_profit and scaled nBettings per profit threshold key. max: {}\".format(gmax))\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch')\n",
    "\n",
    "                # KEEP INSIDE profits_back, casts_back = backtest_with_dataset(back_batches, backtest.profit_keys),\n",
    "    def run_and_append_test(self):\n",
    "        self.profits_round, self.casts_round = backtest_with_dataset(self.data_batches, self.profit_keys)\n",
    "        self.append(self, self.profits_round, self.casts_round)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "checkpointPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_weights_best')\n",
    "borrowedBestCheckpointPath = os.path.join(countryDirPath, '_checkpoints', BASE_TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_history.json')\n",
    "backtestPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_backtest.json')\n",
    "validtestPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_validtest.json')\n",
    "testtestPath = os.path.join(countryDirPath, '_checkpoints', TEST_ID + '_testtest.json')\n",
    "\n",
    "history = history_class(historyPath)\n",
    "backtest = test_class(\"back\", PROFIT_KEYS, back_batches, len(back_ds), len(model_1x2.bookies), backtestPath)\n",
    "validtest = test_class(\"vald\", PROFIT_KEYS, valid_batches, len(valid_ds), len(model_1x2.bookies), validtestPath)\n",
    "testtest = test_class(\"test\", PROFIT_KEYS, test_batches, len(test_ds), len(model_1x2.bookies), testtestPath)\n",
    "\n",
    "def removeFile(path):\n",
    "    files = glob.glob(path + \"*\")   # \"*.*\" may not work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    return\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    removeFile(checkpointPath)\n",
    "    removeFile(checkpointPathBest)\n",
    "    history.reset()\n",
    "    backtest.reset()\n",
    "    validtest.reset()\n",
    "    testtest.reset()\n",
    "\n",
    "    if BASE_TEST_ID != '':\n",
    "        try: \n",
    "            model_1x2.load_weights(borrowedBestCheckpointPath)\n",
    "            print('Model: ' + BASE_TEST_ID + ' loaded.')\n",
    "        except:\n",
    "            print('Failed to load the best model weights.')\n",
    "else:\n",
    "    try: \n",
    "        model_1x2.load_weights(checkpointPath)\n",
    "    except:\n",
    "        print('Previous weights not loaded.')\n",
    "\n",
    "history.load()\n",
    "backtest.load()\n",
    "validtest.load()\n",
    "testtest.load()\n",
    "\n",
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_optimizer():\n",
    "    global_step = tf.Variable(history.len(), dtype=tf.float32, trainable=False)\n",
    "    starter_learning_rate = LEARNING_RATE\n",
    "    # It will reach starter_learning_rate * 0.1 when len(history.history['loss']) reaches 30.\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(starter_learning_rate, global_step, 30, 0.1, staircase=False)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.95, beta_2=0.95, epsilon=1e-9)\n",
    "    return optimizer\n",
    "\n",
    "optimizer = update_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model_1x2(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value = model_1x2.loss(y, outputs)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model_1x2.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model_1x2.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def find_loss_for_step(x, y):\n",
    "    outputs = model_1x2(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value = model_1x2.loss(y, outputs)\n",
    "    # recall_object.update_state(y, val_logits)\n",
    "    # precision_object.update_state(y, val_logits)\n",
    "    return loss_value\n",
    "\n",
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def find_loss_for_dataset(dataset):\n",
    "    n = 0\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = base_label\n",
    "        n += 1\n",
    "        val_loss = val_loss * (n-1) / n + find_loss_for_step(x, y) / n   ###\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_epoch(loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test):\n",
    "    model_1x2.save_weights(checkpointPath)\n",
    "    if validtest.get_best_product(profits_valid, casts_valid) > validtest.get_existing_best_product() \\\n",
    "        or val_loss < history.get_min_val_loss():\n",
    "        model_1x2.save_weights(checkpointPathBest)\n",
    "\n",
    "    history.append(loss, val_loss, learning_rate)\n",
    "    backtest.append(profits_back, casts_back)\n",
    "    validtest.append(profits_valid, casts_valid)\n",
    "    testtest.append(profits_test, casts_test)\n",
    "\n",
    "def show_steps(epoch, step, loss, samples_seen, learning_rate):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hParams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {}, samples_seen: {}, learning_rate: {:.5e}                  \".\n",
    "            format(epoch, step, float(loss), samples_seen, learning_rate), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def print_test(epoch, train_loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test):\n",
    "    print(\"epoch: {}, loss: {}, val_loss: {}, learning_rate: {:.5e}, profit: {}, casts: {}\".format(epoch, float(train_loss), float(val_loss), learning_rate, profits_valid, casts_valid))\n",
    "    pGroups_back = backtest.find_profit_groups(profits_back, casts_back)\n",
    "    backtest.print_profit_groups(pGroups_back, 10)\n",
    "    pGroups_valid = validtest.find_profit_groups(profits_valid, casts_valid)\n",
    "    validtest.print_profit_groups(pGroups_valid, 10)\n",
    "    pGroups_test = testtest.find_profit_groups(profits_test, casts_test)\n",
    "    testtest.print_profit_groups(pGroups_test, 10)\n",
    "\n",
    "    pGroups_back = backtest.find_profit_groups_elements(profits_back, casts_back, sort=False)\n",
    "    backtest.print_profit_groups(pGroups_back, len(backtest.profit_keys)-1)\n",
    "    pGroups_valid = validtest.find_profit_groups_elements(profits_valid, casts_valid, sort=False)\n",
    "    validtest.print_profit_groups(pGroups_valid, len(validtest.profit_keys)-1)\n",
    "    pGroups_test = testtest.find_profit_groups_elements(profits_test, casts_test, sort=False)\n",
    "    testtest.print_profit_groups(pGroups_test, len(testtest.profit_keys)-1)\n",
    "\n",
    "def conclude_train_epoch(epoch, train_loss, learning_rate):\n",
    "    val_loss = find_loss_for_dataset(valid_batches)\n",
    "    profits_back, casts_back = backtest_with_dataset(back_batches, backtest.profit_keys)\n",
    "    profits_valid, casts_valid = backtest_with_dataset(valid_batches, validtest.profit_keys)\n",
    "    profits_test, casts_test = backtest_with_dataset(test_batches, testtest.profit_keys)\n",
    "    save_epoch(train_loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)   \n",
    "    print_test(epoch, train_loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)\n",
    "\n",
    "# The above, in this cell, ugly code should be replaced with the following, later.\n",
    "# def conclude_train_epoch(epoch, train_loss):\n",
    "#     val_loss = find_loss_for_dataset(valid_batches)\n",
    "#     history.append(loss, val_loss)\n",
    "#     model_1x2.save_weights(checkpointPath)\n",
    "\n",
    "#     for test in (backtest, validtest, testtest):\n",
    "#         # KEEP INSIDE profits_back, casts_back = backtest_with_dataset(back_batches, backtest.profit_keys),\n",
    "#         test.run_and_append_test()      # test.profits_round and test.casts_round are computed and appended.\n",
    "\n",
    "#     print(\"epoch: {}, loss: {}, val_loss: {}\".format(epoch, float(train_loss), float(val_loss)))\n",
    "    \n",
    "#     if validtest.get_best_product2(validtest.profits_round, validtest.casts_round) > validtest.get_existing_best_product() \\\n",
    "#         or val_loss < history.get_min_val_loss():\n",
    "#         model_1x2.save_weights(checkpointPathBest)\n",
    "\n",
    "#     for test in (backtest, validtest, testtest):\n",
    "#         pGroups = test.find_profit_groups2()\n",
    "#         test.print_profit_groups(pGroups, 10)\n",
    "\n",
    "#     for test in (backtest, validtest, testtest):\n",
    "#         pGroups_back = test.find_profit_groups_elements2(sort=False)\n",
    "#         test.print_profit_groups(pGroups_back, len(test.profit_keys)-1)\n",
    "\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    learning_rate = optimizer.lr.numpy()\n",
    "    train_loss = find_loss_for_dataset(train_batches)\n",
    "    conclude_train_epoch(0, train_loss, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "history.show(axes[0]); backtest.show(axes[1]); validtest.show(axes[2]); testtest.show(axes[3]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "profit_series_back, cast_series_back = backtest.find_profit_cast_series()\n",
    "profit_series_valid, cast_series_valid = validtest.find_profit_cast_series()\n",
    "profit_series_test, cast_series_test = testtest.find_profit_cast_series()\n",
    "\n",
    "epoch = 0\n",
    "for train_loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test \\\n",
    "    in zip(history.history['loss'], history.history['val_loss'],  history.history['learning_rate'], profit_series_back, cast_series_back, profit_series_valid, cast_series_valid, profit_series_test, cast_series_test):\n",
    "    print_test(epoch, train_loss, val_loss, learning_rate, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    epochs = epochs\n",
    "    global optimizer\n",
    "    for epoch in range(history.len(), epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer = update_optimizer()\n",
    "        learning_rate = optimizer.lr.numpy()\n",
    "        # history.show(axes[0]); backtest.show(axes[1]); plt.show()\n",
    "        n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "        m = 0; train_loss = 0.0\n",
    "        \n",
    "        train_batches = apply_train_pipeline(train_ds)\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label, seq_len_org)) in enumerate(train_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = base_label\n",
    "            batch_loss = train_step(x, y)\n",
    "            n += 1; loss = loss * (n-1)/n + batch_loss/n\n",
    "            m += 1; train_loss = train_loss * (m-1)/m + batch_loss/m\n",
    "\n",
    "            samples_seen += sequence.shape[0]\n",
    "            if step % 50 == 0:\n",
    "                show_steps(epoch, step, loss, samples_seen, learning_rate)\n",
    "                n = 0; loss = 0.0\n",
    "\n",
    "        show_steps(epoch, step, loss, samples_seen, learning_rate)\n",
    "        conclude_train_epoch(epoch, train_loss, learning_rate)\n",
    "        print(\"time taken: {:.1f}m, learning_rate: {:.5e}   \".format((time.time()-start_time)/60, learning_rate))\n",
    "\n",
    "if TRAIN_MODE: train(30)    #--------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_a = 0.7\n",
    "interval_b = 0.8\n",
    "\n",
    "if not TRAIN_MODE:      # just sketch\n",
    "    # df_grown gets to be the existing df_grown, because the train_mode if False.\n",
    "    df_total, df_new = df_grown, df_new = data_helpers.get_grown_extra_from_football_data_co_uk(countryDirPath, Required_Non_Odds_cols, NUMBER_BOOKIES, train_mode = TRAIN_MODE, skip=False)\n",
    "    df_white = df_new\n",
    "    df_black = data_helpers.read_excel(path)\n",
    "\n",
    "    df_total = df_total; df_search = df_new\n",
    "    additional_id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(countryDirPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, df_total, df_search, chooseDivs=chooseDivs)\n",
    "    \n",
    "    ds_path = os.path.join(countryDirPath, '_datasets', id_to_ids_filename + '-dataset-inference')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_total, additional_id_to_ids, tokenizer_team, std_params, df_new=df_new, train_mode=TRAIN_MODE)  #-----------------\n",
    "        ds = normalize_dataset(ds, train_mode=TRAIN_MODE)                                                                           #-----------------\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Wierd, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    ds_inference = apply_test_pipeline(ds)\n",
    "    stake_vectors = inference_with_dataset(ds_inference, interval_a, interval_b) # (batch, nBookies, nQueries)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
