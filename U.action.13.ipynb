{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as ps\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "\n",
    "# from config import config\n",
    "import data_helpers\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = 'UK.B.A.13'\n",
    "TRAIN_PERCENT= 90\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 48\n",
    "LEARNING_RATE = 0.00001\n",
    "LOSS_MULTIPLIER = 2.0\n",
    "TEAM_EMBS = 50\n",
    "LOSS_RAMBDA = 0.5\n",
    "NORM_PP_PATIENCY = 100\n",
    "MAE_NOT_MSE_LOSS = True\n",
    "TRANSFORMER_DROP = 0.2\n",
    "TRANSFORMER_LAYERS = 6\n",
    "TRANSFORMER_HEADS = 6\n",
    "# ADAPTORS_LAYERS = 10\n",
    "RESET_HISTORY = False\n",
    "MIN_PROFIT = -0.10\n",
    "DISTRIBUTION_KEYS = [2.0, 1.0, 0.5, 0.0, -0.5, -1.0]\n",
    "MIN_PROFIT_P_PER_GAME_PER_QGROUP = 0.5\n",
    "id_to_ids_filename = 'England-200-1e-07-7300-75-0.8-False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/England\"\n",
    "df = data_helpers.get_master_df_from_football_data_co_uk(countryDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "['tottenham', 'arsenal', 'liverpool', '[UNK]', 'tottenham', 'chelsea', '[UNK]', 'man_united', '[UNK]', '[UNK]', '[UNK]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 4, 58, 0, 101, 27, 0, 62, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tottenham arsenal liverpool tottenham chelsea man_united'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparams:    \n",
    "    nDivisions = 4 + 1  # E0, E1, E2, E3, and Unknown\n",
    "    division_embs = 4\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 10  # 0 for 0 goals not for Unknown.\n",
    "    goal_embs = 4\n",
    "    nResults = 4    # HWin, Draw, AWin, and Unknown\n",
    "    result_embs = 4\n",
    "    # Mate d_model an even number!!!\n",
    "    d_model = get_std_size()    + division_embs * len(Div_cols) + team_embs * len(Team_cols) \\\n",
    "                                + goal_embs * len(Goal_cols) + result_embs * len(Result_cols)\n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "    # d_model = team_emb_size * 2 + country_emb_size * 3 + odds_size + outcome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'id_to_ids', id_to_ids_filename + '.json')\n",
    "id_to_ids = data_helpers.LoadJsonData(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000, 'E0', datetime.date(2004, 1, 11), 'Man_City', 'Norwich', 1.72, 3.4, 5.0, 1.7, 3.2, 5.0, 1.65, 3.3, 4.4, 1.66, 3.1, 5.0, 1.0, 0.0, 1, 1, 'H', 'D', 19.0, 10.0, 11.0, 5.0, 9.0, 4.0, 10.0, 13.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "_Cols_List_to_Embedd = [Div_cols, Team_cols, Goal_cols, Result_cols]\n",
    "_Cols_List_to_Standardize = [Odds_cols, Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_for_Label = [Full_Goal_cols, Odds_cols]\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "base_bbab = list(df.loc[df['id'] == 100000, BBAB_cols].iloc[0, :])\n",
    "print(base_bbab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B365H': (3.1630270400481795, 1.4687447460469159, 46.0), 'HS': (11.39694154084398, 4.709404811489129, 43.0), 'HST': (4.815343915343916, 2.759941394538306, 24.0), 'HC': (5.34632855852368, 2.842282967456132, 24.0), 'HF': (11.421925409730287, 3.7612036770331043, 77.0), 'HY': (1.5455413601755066, 1.2348960213340971, 11.0), 'HR': (0.08013937282229965, 0.2855927650445304, 3.0)}\n"
     ]
    }
   ],
   "source": [
    "std_path = os.path.join('./data', 'datasets', id_to_ids_filename + \".json\")\n",
    "std_params = get_standardization_params(df)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38745"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = os.path.join('./data', 'datasets', id_to_ids_filename)\n",
    "\n",
    "# ds = generate_dataset_uk(df, id_to_ids, tokenizer_team, std_params)\n",
    "# tf.data.Dataset.save(ds, ds_path)\n",
    "\n",
    "ds = tf.data.Dataset.load(ds_path)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34870 3875 38745 0\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(ds)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "test_ds = ds.skip(train_size)\n",
    "\n",
    "print(len(train_ds), len(test_ds), len(ds), len(ds)-len(train_ds)-len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bbas_tensor = get_dummy_bbas_tensor_uk(df, tokenizer_team, std_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_row(baseId, sequence, base_bb, base_label):\n",
    "    try:\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            block = tf.stack([dummy_bbas_tensor] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, block], axis=0) \n",
    "        # print(\"sequence 1\", sequence.shape)\n",
    "        # sequence[:, 2] = base[2] - sequence[:, 2]   # get delta days.\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, nFeatures)\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]\n",
    "        # print('normalize', sequence.shape, base.shape, mask.shape, mask)\n",
    "        # seq_len_org = tf.Variable(seq_len_org, dtype=tf.int32)    #--------------------------------- comeback\n",
    "        return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "    except:\n",
    "        print('normalize_row exception')\n",
    "        print('norm 1', sequence.shape, base_bb.shape, base_label.shape, mask.shape, nMissings)\n",
    "        print('norm 2', baseId, sequence, base_label, mask, nMissings)\n",
    "        # return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "\n",
    "def prepare_batch(baseId, sequence, base_bb, base_label, mask, seq_len_org):\n",
    "    # target = tf.one_hot(tf.squeeze(tf.cast(base_bbab[:, :, -1], dtype=tf.int32), axis=-1), hyperparams.target_onehot_size)\n",
    "    return (baseId, sequence, base_bb, mask), (base_label, seq_len_org)     # (X, Y)\n",
    "\n",
    "def normalize_dataset(ds):\n",
    "    return (\n",
    "        ds.map(lambda baseId, sequence, base_bb, base_label: tf.py_function(\n",
    "            func=normalize_row,\n",
    "            inp=[baseId, sequence, base_bb, base_label],\n",
    "            Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "        )\n",
    "\n",
    "def make_train_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def make_test_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_train_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(train_ds_path):\n",
    "    train_ds = tf.data.Dataset.load(train_ds_path)\n",
    "else:\n",
    "    train_ds = normalize_dataset(train_ds)\n",
    "    tf.data.Dataset.save(train_ds, train_ds_path)\n",
    "\n",
    "train_batches = make_train_batches(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_test_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(test_ds_path):\n",
    "    test_ds = tf.data.Dataset.load(test_ds_path)\n",
    "else:\n",
    "    test_ds = normalize_dataset(test_ds)\n",
    "    tf.data.Dataset.save(test_ds, test_ds_path)\n",
    "\n",
    "test_batches = make_test_batches(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(memory, depth):\n",
    "    positions = tf.range(memory.shape[-1], dtype=tf.float32)\n",
    "    fractional_pos = memory * positions    # fractional position: (batch, fractional position #)\n",
    "    depth = depth/2\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth\n",
    "    depths = tf.pow(10000.0, depths)    # (depth,)\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] / depths  # (batch, fractional position #, depth)\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = tf.ones((100, 200), dtype=tf.float32) * 0.5\n",
    "pos_encoding = positional_encoding(memory, depth=512)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, hyperparams, isEncoder=True):\n",
    "    super().__init__()\n",
    "    self.isEncoder = isEncoder\n",
    "    self.division_embedding = tf.keras.layers.Embedding(hyperparams.nDivisions, hyperparams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.team_embedding = tf.keras.layers.Embedding(hyperparams.nTeams, hyperparams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.goal_embedding = tf.keras.layers.Embedding(hyperparams.nGoals, hyperparams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn 0-goal\n",
    "    self.result_embedding = tf.keras.layers.Embedding(hyperparams.nResults, hyperparams.result_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "    self.d_model = hyperparams.d_model\n",
    "    # print(self.d_model)\n",
    "    self.position_permuting_dense = tf.keras.layers.Dense(self.d_model)\n",
    "    self.m365_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "\n",
    "    self.idx_Days = BB_cols.index('Date')\n",
    "    assert self.idx_Days == BBAB_cols.index('Date')\n",
    "\n",
    "  def call(self, x):\n",
    "    (sequence, base_bb, mask) = x # sob = sequence or base_bb\n",
    "    sDays = sequence[:, :, self.idx_Days]\n",
    "    bDays = base_bb[:, :, self.idx_Days]\n",
    "    \n",
    "    # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "    # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "    sob = None\n",
    "    if self.isEncoder:\n",
    "      sob = sequence\n",
    "    else:\n",
    "      sob = base_bb\n",
    "\n",
    "    if self.isEncoder:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, goals, results, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Goal_cols), len(Result_cols),  -1], axis=-1)\n",
    "      # print('1', remainder[0, 0])\n",
    "    else:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "      # print('2', remainder[0, 0])  \n",
    "\n",
    "    # print('pe 2.7.1 1', div)\n",
    "    div = self.division_embedding(tf.cast(div, dtype=tf.int32))\n",
    "    div = tf.reshape(div, [div.shape[0], div.shape[1], -1])\n",
    "    # print('pe 2.7.1 1', div)\n",
    "    teams = self.team_embedding(tf.cast(teams, dtype=tf.int32))\n",
    "    teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1])\n",
    "    if self.isEncoder:\n",
    "      goals = self.goal_embedding(tf.cast(goals, dtype=tf.int32))\n",
    "      goals = tf.clip_by_value(goals, 0, hyperparams.nGoals)\n",
    "      goals = tf.reshape(goals, [goals.shape[0], goals.shape[1], -1])\n",
    "      results = self.result_embedding(tf.cast(results, dtype=tf.int32))\n",
    "      results = tf.reshape(results, [results.shape[0], results.shape[1], -1])\n",
    "    \n",
    "    if self.isEncoder:\n",
    "      concat = [div, teams, goals, results, odds, remainder]\n",
    "    else:\n",
    "      concat = [div, teams, odds, remainder]\n",
    "\n",
    "    sob = tf.concat(concat, axis=-1)\n",
    "    sob = self.position_permuting_dense(sob)\n",
    "\n",
    "    days_ago = tf.cast(bDays - sDays, dtype=tf.float32) if self.isEncoder else tf.cast(bDays - bDays, dtype=tf.float32)\n",
    "    \n",
    "    m365 = self.m365_embedding(tf.zeros_like((hyperparams.m365_size,), dtype=tf.float32)) * hyperparams.initial_m365  # expected shape: (1, hyperparams.remain_365_size)\n",
    "    m365 = tf.squeeze(m365, axis=0)\n",
    "    memory_alpha = tf.math.pow(m365, 1.0/365) # (hyperparams.m365_size,)\n",
    "    memory = tf.math.pow(memory_alpha, days_ago[:, :, tf.newaxis])  # decrease as days_ago increase, if memory <= 1.0 as expected.\n",
    "    memory = tf.reduce_mean(memory, axis=-1)\n",
    "\n",
    "    pe = positional_encoding(memory, depth=sob.shape[-1]) # (batch, d_model)\n",
    "    pe = pe / tf.math.sqrt(tf.cast(sob.shape[-1], tf.float32))\n",
    "    sob = sob + pe\n",
    "\n",
    "    if self.isEncoder:\n",
    "      mask = mask\n",
    "    else:\n",
    "      mask = mask[:, 0:sob.shape[1], :]\n",
    "\n",
    "    return (sob, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 200, 152) (48, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "pos = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "\n",
    "cnt = 2\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, mask), (base_label, seq_len_org) = z\n",
    "    cnt -= 1\n",
    "    if cnt == 0: break\n",
    "# print('baseId', baseId)\n",
    "sample_x = (sequence, base_bb, mask)\n",
    "eSob, eMask = pos.call(sample_x)\n",
    "print(eSob.shape, eMask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 200, 152) (48, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "eSob, eMask = PE(sample_x)\n",
    "print(eSob.shape, eMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48, 1, 152) (48, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "dSob, dMask = PE(sample_x)\n",
    "print(dSob.shape, dMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axix is normalized across. No inter-token operatoin.\n",
    "      self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(\n",
    "          query=x,\n",
    "          key=context,\n",
    "          value=context,\n",
    "          attention_mask=mask,\n",
    "          return_attention_scores=True)\n",
    "    \n",
    "      # Cache the attention scores for plotting later.\n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          attention_mask=mask)    # intentional inter-token operation\n",
    "      x = self.add([x, attn_output])  # token-wise\n",
    "      x = self.layernorm(x)         # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          use_causal_mask = True)     # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])  # mask-agnostic\n",
    "      x = self.layernorm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),    # across -1 axis\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])  # mask-agnostic\n",
    "      x = self.layer_norm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hyperparams.d_model,\n",
    "                      num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_embedding(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "\n",
    "      self.causal_self_attention = CausalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "      \n",
    "      self.cross_attention = CrossAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "\n",
    "      # Cache the last attention scores for plotting later\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hyperparams.d_model, num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_embedding(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens)     \n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.decoder = Decoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(hyperparams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, inputs):\n",
    "      # inputs = (sequence, base_bb, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), mask: (batch, max_token, max_token)\n",
    "      x = self.encoder(inputs)  # (batch, max_tokens, d_model)\n",
    "      x = self.decoder(inputs, x)  # (batch, 1, d_model)\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  4490487   \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  7831087   \n",
      "                                                                 \n",
      " dense_29 (Dense)            multiple                  23256     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,344,830\n",
      "Trainable params: 12,344,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(hyperparams)\n",
    "y = sample_transformer(sample_x)\n",
    "\n",
    "sample_transformer.summary()\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hyperparams.d_model + round( (d_output - hyperparams.d_model) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGroup(tf.keras.Model):\n",
    "  softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "  scalar_product = tf.keras.layers.Dot(axes=(-1, -1))\n",
    "\n",
    "  def __init__(self, bookie, nQueries, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.bookie = bookie\n",
    "    self.nQueries = nQueries\n",
    "    self.bookieBase = Adaptor(7, self.nQueries) # 7777777777777777777777777\n",
    "    self.oh_1 = Adaptor(5, self.nQueries)       # 5555555555555555555555555\n",
    "    return\n",
    "\n",
    "  def call(self, input):\n",
    "    # inputs.shape: (batch, d_model)\n",
    "    base = self.bookieBase(input)        # (batch, nQueries)\n",
    "    stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "    oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "    return (oh_1_p, stake_p)  # (batch, 3), (batch, nQueries) \n",
    "\n",
    "  #------------------------------- call used in UK.B.A.01 -------------------------  \n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)        # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "  #   profit_p = tf.math.multiply(oh_1_p, stake_p)   # (batch, nQueries)\n",
    "  #   return (profit_p, stake_p)  # (batch, nQueries), (batch, nQueries) \n",
    "\n",
    "  #------------------------------- call in UK.B.A.02, this version -----------\n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)             # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "  #   profit_p = self.scalar_product([oh_1_p, stake_p]) # (batch, 1)\n",
    "  #   return (profit_p, stake_p)  # (batch, 1), (batch, nQueries) \n",
    "  \n",
    "  #------------------------------- call in UK.B.A.03, a futrue version -----------\n",
    "  # # profit_p is free and independent of stake_p\n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)             # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   profit_p = self.profit(base)              # (batch, 1)\n",
    "  #   return (profit_p, stake_p)  # (batch, 1), (batch, nQueries) \n",
    "\n",
    "  #----- ToDo: replace self.scalar_product layer with a tf scalar product function, for speed.\n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "    # ftGoals:  (batch, 2)\n",
    "    ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "    h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "    h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "    return h\n",
    "\n",
    "  def profit_true(self, ftGoals, odds):\n",
    "    stake_t = self.h_true(ftGoals)  # (batch, nQueries)\n",
    "    oh_1_true = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0  # (batch, nQueries)\n",
    "    profit_t = self.scalar_product([oh_1_true, stake_t])  # (batch, 1)\n",
    "    return profit_t\n",
    "\n",
    "  #-------------------- THis loss is NOT used in action versions.\n",
    "  def loss(self, profit_p, stake_p, ftGoals, odds, rambda):\n",
    "    # profit_p: (batch, 1)\n",
    "    # stake_p:  (batch, nQueries)\n",
    "    # ftGoals:  (batch, 2)\n",
    "    # odds:     (batch, nQueries)\n",
    "    # rambda:   ()\n",
    "    profit_t = self.profit_true(ftGoals, odds)  # (batch, 1)\n",
    "    if MAE_NOT_MSE_LOSS:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.abs(profit_t - profit_p), axis=None) # (), profit dimention.\n",
    "    else:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.pow(profit_t - profit_p, 2.0), axis=None) # (), profit dimention.\n",
    "\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0   # (batch, nQueries)\n",
    "    profit_back = self.scalar_product([oh_1_t, stake_p])    # (batch, 1)\n",
    "    profit_back = tf.reduce_mean(profit_back, axis=None)    # ()\n",
    "    loss = (1.0-rambda) * profit_p_err - profit_back * rambda # ()\n",
    "    return loss # ()\n",
    "  \n",
    "  def oh_1_loss(self, oh_1_p, ftGoals, odds):\n",
    "    # oh_1_p: (bacth, nQueries)\n",
    "    h_true = self.h_true(ftGoals)\n",
    "    oh_1_t = tf.multiply(odds, h_true)  # (batch, nQueries)\n",
    "    if MAE_NOT_MSE_LOSS:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.abs(oh_1_t - oh_1_p), axis=-1) # (batch, ), profit dimention.\n",
    "    else:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.pow(oh_1_t - oh_1_p, 2.0), axis=-1) # (batch, ), profit dimention.\n",
    "    \n",
    "    return profit_p_err # (batch, )\n",
    "\n",
    "  #--------------------------------------- Used as profit_back_with_batch in UK.B.A.01\n",
    "  def profit_eval(self, ftGoals, odds, stake_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    profit_e = QGroup.scalar_product([oh_1_t, stake_p])\n",
    "    return profit_e   # (batch, 1)\n",
    "\n",
    "  #-------------------------------------- The same as 'profit_eval' above --------------\n",
    "  def profit_back_with_batch(self, ftGoals, odds, stake_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    profit_back = self.scalar_product([oh_1_t, stake_p])    # (batch, 1)\n",
    "    return profit_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3.0, 3.0, 4.0], [0.1, 0.7, 0.3]])\n",
    "one_hot_a = tf.squeeze(tf.one_hot(tf.nn.top_k(a).indices, tf.shape(a)[-1]), axis=1)\n",
    "print(one_hot_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGroup1X2(QGroup):\n",
    "  def __init__(self, bookie, dropout_rate=0.1):\n",
    "    super().__init__(bookie=bookie, nQueries=3, dropout_rate=dropout_rate)\n",
    "    self.qGroupName = '1X2'\n",
    "\n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup.\n",
    "    # ftGoals:  (batch, 2)\n",
    "    ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "    h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "    h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BettingEPL(tf.keras.Model):\n",
    "  def __init__(self, hyperparams, loss_rambda=1.0, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.transformer = Transformer(hyperparams, dropout_rate=dropout_rate)\n",
    "    self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "    self.qGroups = [QGroup1X2(bookie) for bookie in self.bookies]\n",
    "    self.rambda = loss_rambda     #----------------------- Sensitive rambda!!!, Automate optimizing it.\n",
    "    # self.shift_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "    # self.shift = None\n",
    "\n",
    "  def call(self, input):\n",
    "      x = self.transformer(input)\n",
    "      outputs = [qGroup(x) for qGroup in self.qGroups]\n",
    "      # self.shift = tf.squeeze(self.shift_embedding(0))  # squeeze((1, 1)) = ()\n",
    "      return outputs  # [ ( shape: (batch, 1), shape: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "  \n",
    "  def loss(self, y, outputs):   \n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( profit_p: (batch, 1), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, sum[qGropu.nQueries for qGroup in self.qGroups])\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)  # [ shape: (batch, qGroup.nQueries) for qGroup in self.qGroups ]\n",
    "      losses = [qGroup.loss(profit_p, stake_p, ftGoals, odds, self.rambda) for (qGroup, odds, (profit_p, stake_p)) in zip(self.qGroups, odds_by_qGroup, outputs)]\n",
    "      # losses: [()] * nQGroup\n",
    "      losses = tf.stack(losses, axis=0) # (nQGroups,)\n",
    "      loss_value = tf.math.mean_reduce(losses, axis=None)\n",
    "      return loss_value\n",
    "  \n",
    "  def profit_back_over_qGroups(self, y, outputs):\n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1)\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)\n",
    "      profit_back = [qGroup.profit_back_with_batch(ftGoals, odds, stake_p) for (qGroup, odds, (_, stake_p)) in zip(self.qGroups, odds_by_qGroup, outputs)]\n",
    "      # profit_back = [(batch, 1) for _ in self.qGroups]\n",
    "      profit_back = tf.concat(profit_back, axis=-1) # (batch, nQGroups)\n",
    "      return profit_back  # A function of stake_p and truth.\n",
    "  \n",
    "  #---------------------------------- The same as in UK.B.A.01, with a bit of code factorization.\n",
    "  def action_loss(self, y, outputs, selectivity):\n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, sum[qGropu.nQueries for qGroup in self.qGroups])\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)  # [ shape: (batch, qGroup.nQueries) for qGroup in self.qGroups ]\n",
    "      oh_1_loss = [qGroup.oh_1_loss(oh_1_p, ftGoals, odds) for (qGroup, odds, (oh_1_p, _)) in zip(self.qGroups, odds_by_qGroup, outputs)] # [(batch,)] * nQGroups\n",
    "      oh_1_loss = tf.stack(oh_1_loss, axis=-1)  # sure (batch, nQGroups)\n",
    "      oh_1_loss = tf.math.reduce_mean(oh_1_loss, axis=None)  # ()\n",
    "      \n",
    "      profit_p = [tf.math.reduce_sum(tf.multiply(oh_1_p, stake_p), axis=-1, keepdims=True) for (oh_1_p, stake_p) in outputs]   # [ shape: (batch, 1) for _ self.qGroups ]\n",
    "      profit_p = tf.concat(profit_p, axis=-1) # (batch, nQGroups)\n",
    "\n",
    "      #======================================================================================================\n",
    "      #   This block simulates the betting action of selecting QGROUP where profit_p is high..\n",
    "      #   Note argmax-based selecting has no gradient.\n",
    "      #======================================================================================================\n",
    "      # least_profit = tf.reduce_min(profit_p, axis=-1)\n",
    "      # delta = tf.reduce_max(profit_p, axis=-1) - least_profit\n",
    "      # normal_profit = tf.transpose(tf.transpose(profit_p) - least_profit)\n",
    "      # normal_profit = tf.transpose(tf.transpose(normal_profit)/(delta + 1e-9)) # no tf.keras.backend.epsilon\n",
    "      # is_zero = ((1.0 - tf.reduce_max(normal_profit, axis=1)))\n",
    "      # normal_profit = tf.transpose(tf.transpose(normal_profit) + is_zero) # (batch, nQGroups). A function of profit_p     \n",
    "      # normal_profit = normal_profit * selectivity + 1.0  # self.shift\n",
    "      \n",
    "      #======================================================================================================\n",
    "      #   This block simulates the betting action of selecting games and qGroups where profit_p is high..\n",
    "      #   Note argmax-based selecting has no gradient.\n",
    "      #======================================================================================================\n",
    "      min = tf.reduce_min(profit_p); max = tf.reduce_max(profit_p)\n",
    "      normal_profit = (profit_p - min) / (max-min + 1e-12) # (batch,)   [0, 1]\n",
    "      normal_profit = tf.math.sigmoid( 10.0 * tf.math.pow(normal_profit, selectivity) - 5.0 ) # (batch,)\n",
    "      normal_profit = normal_profit + 0.5\n",
    "\n",
    "      profit_back = self.profit_back_over_qGroups(y, outputs) # (batch, nQGroups) # A function of stake_p and truth.\n",
    "\n",
    "      mul = tf.multiply(normal_profit, profit_back)   # (batch, nQGroups)\n",
    "      mean_profit_per_game = tf.math.reduce_mean(mul, axis=None)  # ()\n",
    "      \n",
    "      return  (- mean_profit_per_game + oh_1_loss)\n",
    "\n",
    "  #---------------------------------- The same as in UK.B.A.01, with a bit of code factorization.\n",
    "  def back_test(self, y, outputs):\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( shape: (batch, 1), shape: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs) # (batch, nQGroups)\n",
    "\n",
    "    # find the most-profitable QGroup idx\n",
    "    profits_p = [tf.math.reduce_sum(profit_p, axis=-1, keepdims=True) for (profit_p, _) in outputs]   # [ shape: (batch, 1) for _ self.qGroups ]\n",
    "    profits_p = tf.concat(profits_p, axis=-1) # (batch, len(self.qGroups))    \n",
    "    bestQuery = tf.cast(tf.argmax(profits_p, axis=-1), dtype=tf.int32)   # (batch,)\n",
    "    range = tf.range(bestQuery.shape[0], dtype=tf.int32) # (batch,)\n",
    "    best_idx = tf.stack([range, bestQuery], axis=1) # (batch, 2)\n",
    "    \n",
    "    best_profits_eval = tf.gather_nd(profit_back, best_idx)  # (batch, )\n",
    "    profit_eval_mean = tf.math.reduce_mean(best_profits_eval)\n",
    "    return profit_eval_mean\n",
    "  \n",
    "  def back_test_over_chosen_games_and_qGroups(self, y, outputs):\n",
    "    # Choose (game, qGroup), which is greater than 0.05, or MIN_PROFIT_P_PER_GAME_PER_QGROUP\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_p = [tf.math.multiply(p_p, s_p) for (p_p, s_p) in outputs]  # [ (batch, nQueries) ] * nQGroups\n",
    "    profit_p = [tf.math.reduce_sum(p_p, axis=-1, keepdims=True) for p_p in profit_p] # [(batch, 1)] * nQGroups\n",
    "    profit_p = tf.concat(profit_p, axis=-1)   # (batch, nQGroups))\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs)   # (batch, nQGroups)\n",
    "\n",
    "    best_idx = tf.where(profit_p > MIN_PROFIT_P_PER_GAME_PER_QGROUP)  # (nBettings, 2). nBettings unknown yet.\n",
    "\n",
    "    nBettings = 0\n",
    "    profit_back_mean_per_betting = MIN_PROFIT\n",
    "    if best_idx.shape[0] > 0:\n",
    "      best_profits_back = tf.gather_nd(profit_back, best_idx)   # (nBettings, )\n",
    "      nBettings = best_profits_back.shape[0]\n",
    "      profit_back_mean_per_betting = tf.math.reduce_mean(best_profits_back)\n",
    "    return profit_back_mean_per_betting, nBettings\n",
    "  \n",
    "  def back_test_over_chosen_games_and_qGroups_for_distribution(self, y, outputs, keys):\n",
    "    # Choose (game, qGroup), which is greater than 0.05, or MIN_PROFIT_P_PER_GAME_PER_QGROUP\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_p = [tf.math.multiply(p_p, s_p) for (p_p, s_p) in outputs]  # [ (batch, nQueries) ] * nQGroups\n",
    "    profit_p = [tf.math.reduce_sum(p_p, axis=-1, keepdims=True) for p_p in profit_p] # [(batch, 1)] * nQGroups\n",
    "    profit_p = tf.concat(profit_p, axis=-1)   # (batch, nQGroups))\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs)   # (batch, nQGroups)\n",
    "\n",
    "    profit_back_mean_per_betting_list = []\n",
    "    nBettings_list = []\n",
    "\n",
    "    for key in keys:\n",
    "      best_idx = tf.where(profit_p > key)  # (nBettings, 2). nBettings unknown yet.\n",
    "\n",
    "      nBettings = 0\n",
    "      profit_back_mean_per_betting = MIN_PROFIT\n",
    "      if best_idx.shape[0] > 0:\n",
    "        best_profits_back = tf.gather_nd(profit_back, best_idx)   # (nBettings, )\n",
    "        nBettings = best_profits_back.shape[0]\n",
    "        profit_back_mean_per_betting = tf.math.reduce_mean(best_profits_back)\n",
    "      profit_back_mean_per_betting_list.append(float(profit_back_mean_per_betting))\n",
    "      nBettings_list.append(nBettings)\n",
    "    return profit_back_mean_per_betting_list, nBettings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3.0, 3.0, 4.0], [0.1, 0.7, 0.3]])\n",
    "one_hot_a = tf.squeeze(tf.one_hot(tf.nn.top_k(a).indices, tf.shape(a)[-1]), axis=1)\n",
    "print(one_hot_a)\n",
    "# one_hot_a = [[ 0.  0.  1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n",
      "(48, 3) (48, 3)\n",
      "Model: \"betting_epl\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_1 (Transformer)  multiple                 12344830  \n",
      "                                                                 \n",
      " q_group1x2 (QGroup1X2)      multiple                  74282     \n",
      "                                                                 \n",
      " q_group1x2_1 (QGroup1X2)    multiple                  74282     \n",
      "                                                                 \n",
      " q_group1x2_2 (QGroup1X2)    multiple                  74282     \n",
      "                                                                 \n",
      " q_group1x2_3 (QGroup1X2)    multiple                  74282     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,641,958\n",
      "Trainable params: 12,641,958\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPL = BettingEPL(hyperparams, loss_rambda = LOSS_RAMBDA, dropout_rate=TRANSFORMER_DROP)\n",
    "\n",
    "x = (sequence, base_bb, mask)\n",
    "y = EPL(sample_x, training=True)\n",
    "print(len(y))\n",
    "print(len(y[0]))\n",
    "(profit_p, stake_p) = y[0]\n",
    "print(profit_p.shape, stake_p.shape)\n",
    "# print(profit_p, stake_p)   # profit_p tend to have the same sign in the same batch.\n",
    "# shift = tf.squeeze(EPL.layers[-1].get_weights()).numpy()\n",
    "# print('shift', shift)\n",
    "\n",
    "EPL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = CustomSchedule(hyperparams.d_model)\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.95, beta_2=0.95, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss_uk(label, y_pred):\n",
    "  # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch, 1)), y_pred: (batch, 3)\n",
    "  y_true = label[0]   # one_hot: (batch, 3)\n",
    "  seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "  mask = y_true != 0 \n",
    "  loss = loss_object(y_true, y_pred)\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask) # eq. sum_loss / batch\n",
    "  return loss\n",
    "\n",
    "\n",
    "class recall():\n",
    "  def __init__(self, name='recall', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    # print('recall', y_true.shape, y_pred.shape, seq_len_mask.shape)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    # print('recall', true_positives.numpy())\n",
    "    possible_positives = tf.math.reduce_sum(y_true)\n",
    "    recall_keras = true_positives / (possible_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall_keras.numpy() / self.n\n",
    "\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.math.reduce_sum(y_pred)\n",
    "    precision_keras = true_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision_keras.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y, selectivity):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = EPL(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value = EPL.action_loss(y, outputs, selectivity)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, EPL.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, EPL.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y, selectivity):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value = EPL.action_loss(y, outputs, selectivity)\n",
    "    # recall_object.update_state(y, val_logits)\n",
    "    # precision_object.update_state(y, val_logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  # gives a wrong result of tf.where(profit_p > MIN_PROFIT_P_PER_GAME_PER_QGROUP)\n",
    "def back_test_step(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    profit_back_mean_per_betting, nBettings = EPL.back_test_over_chosen_games_and_qGroups(y, outputs)\n",
    "    return profit_back_mean_per_betting, nBettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def back_test_with_dataset(datsset):\n",
    "    profit_back_mean = 0.0\n",
    "    nBettingsTotal = 0\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        profit_back_mean_per_betting, nBettings = back_test_step(x, y)\n",
    "        # print('back_test_with_dataset', profit_back_mean_per_betting, nBettings)\n",
    "        if nBettings > 0:\n",
    "            profit_back_mean = (profit_back_mean * nBettingsTotal + profit_back_mean_per_betting * nBettings) / (nBettingsTotal + nBettings)\n",
    "            nBettingsTotal = nBettingsTotal + nBettings\n",
    "    return profit_back_mean, nBettingsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def test_with_dataset(datsset, selectivity):\n",
    "    n = 0\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        n += 1\n",
    "        val_loss = val_loss * (n-1) / n + test_step(x, y, selectivity) / n   ###\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self):\n",
    "        self.history = {'loss': [], 'val_loss': [], 'back100': [], 'nBettings': []}\n",
    "    def save(self, path):\n",
    "        data_helpers.SaveJsonData(self.history, path)\n",
    "    def load(self, path):\n",
    "        self.history = data_helpers.LoadJsonData(path)\n",
    "        if self.history is None:\n",
    "            self.history = {'loss': [], 'val_loss': [], 'back100': [], 'nBettings': []}\n",
    "    def to_back100(self, back):\n",
    "        return float(back if back >= 0 else back)\n",
    "    def append(self, loss, val_loss, back, nBettings):\n",
    "        self.history['loss'].append(self.round_sig(float(loss), 4))\n",
    "        self.history['val_loss'].append(self.round_sig(float(val_loss), 4))\n",
    "        self.history['back100'].append(self.round_sig(self.to_back100(back), 4))\n",
    "        self.history['nBettings'].append(int(nBettings))\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['back100'])\n",
    "        assert len(self.history['loss']) == len(self.history['nBettings'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_latest_item(self):\n",
    "        return (self.history['loss'][-1], self.history['val_loss'][-1], self.history['back100'][-1], self.history['nBettings'][-1])\n",
    "    def get_max_back(self):\n",
    "        return float('-inf') if self.len() <= 0 else max(self.history['back100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_steps(epoch, step, loss, samples_seen):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hyperparams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {}, samples_seen: {}          \".\n",
    "            format(epoch, step, float(loss), samples_seen), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def show_history(history, baseline=0):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    loss = history.history['loss'][baseline:]\n",
    "    val_loss = history.history['val_loss'][baseline:]\n",
    "    losses = loss + val_loss\n",
    "    back = [b100 * 100 for b100 in history.history['back100']][baseline:]\n",
    "    nBettings = history.history['nBettings'][baseline:]\n",
    "    minBack = min(back) if history.len() > 0 else 0.0; maxBack = max(back) if history.len() > 0 else 1.0\n",
    "    minLosses = min(losses) if history.len() > 0 else 0.0; maxLosses = max(losses) if history.len() > 0 else 1.0\n",
    "    loss = [(elem - minLosses) / (maxLosses-minLosses+1e-9) * (maxBack-minBack) + minBack for elem in loss]\n",
    "    val_loss = [(elem - minLosses) / (maxLosses-minLosses+1e-9) * (maxBack-minBack) + minBack for elem in val_loss]\n",
    "    minBettings = min(nBettings) if history.len() > 0 else 0.0; maxBettings = max(nBettings) if history.len() > 0 else 1.0\n",
    "    nBettings = [(elem - minBettings) / (maxBettings-minBettings+1e-9) * (maxBack-minBack) + minBack for elem in nBettings]\n",
    "    base = 0.0\n",
    "\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.plot(back)\n",
    "    plt.plot(nBettings, color='y', linewidth=0.5)\n",
    "    \n",
    "    bestBack = max(back) if history.len() - baseline > 0 else -1.0\n",
    "    bestBackIdx = back.index(bestBack) if back.count(bestBack) > 0 else 0\n",
    "\n",
    "    all = loss + val_loss + back + nBettings\n",
    "    if len(all) > 0:\n",
    "        ymin = min(all); ymax = max(all); xmin = 0; xmax = history.len() - baseline\n",
    "    else:\n",
    "        ymin = 0.0; ymax = 1.0; xmin = 0.0; xmax = 1.0\n",
    "     \n",
    "    plt.axvline(x=bestBackIdx, ymin=ymin, ymax=ymax, color='r', linewidth=0.3)\n",
    "    plt.axhline(y=bestBack, xmin=xmin, xmax=xmax, color='r', linewidth=0.3)\n",
    "    plt.axhline(y=base, color='b', linestyle='-', linewidth=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.title(TEST_ID + \": Avg profit per betting. max: {}, history len: {}\".format(bestBack, history.len()))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'val_loss', '100 * val_profit', 'nBettings'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join('./data', 'checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join('./data', 'checkpoints', TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join('./data', 'checkpoints', TEST_ID + '_history.json')\n",
    "\n",
    "history = history_class()\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    files = glob.glob(checkpointPath + \"*\")         # \"*.*\" doesn't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    files = glob.glob(historyPath + \"*\")            # \"*.*\" doens't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    EPL.save_weights(checkpointPath)    #\n",
    "    history.save(historyPath)\n",
    "\n",
    "try:\n",
    "    EPL.load_weights(checkpointPath)\n",
    "except:\n",
    "    print('Failed to load model weights.')\n",
    "\n",
    "history.load(historyPath)\n",
    "# if history.len() <= 0:\n",
    "#     print('Creating historic baseline...', end='')\n",
    "#     loss = test_with_dataset(train_batches)\n",
    "#     val_loss = test_with_dataset(test_batches)\n",
    "#     back = back_test_with_dataset(test_batches)\n",
    "#     history.append(loss, val_loss, back)\n",
    "#     history.save(historyPath)\n",
    "#     print('done')\n",
    "\n",
    "def save_checkpoint(loss, val_loss, back, nBettings):\n",
    "    EPL.save_weights(checkpointPath)\n",
    "    max_back = history.get_max_back()\n",
    "    if float(history.to_back100(back)) > max_back:\n",
    "        EPL.save_weights(checkpointPathBest)\n",
    "    history.append(loss, val_loss, back, nBettings)\n",
    "    history.save(historyPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for key, value in history.history.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_history(history, baseline=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule(epoch):\n",
    "    selectivity = 0.05 + (5.) / NORM_PP_PATIENCY * epoch    # start from 0.05, grow linearly over epoch, and hit 5.0 when epoch == NORM_PP_PATIENCY, which should be about 100.\n",
    "    return selectivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 123276, memory365: nan,  time taken: 192s          \n",
      "epoch: 1, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 121145, memory365: nan,  time taken: 179s          \n",
      "epoch: 2, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 121968, memory365: nan,  time taken: 182s          \n",
      "epoch: 3, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 132640, memory365: nan,  time taken: 184s          \n",
      "epoch: 4, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 121976, memory365: nan,  time taken: 188s          \n",
      "epoch: 5, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 109155, memory365: nan,  time taken: 192s          \n",
      "epoch: 6, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 110210, memory365: nan,  time taken: 193s          \n",
      "epoch: 7, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 102852, memory365: nan,  time taken: 195s          \n",
      "epoch: 8, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 121034, memory365: nan,  time taken: 197s          \n",
      "epoch: 9, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 132644, memory365: nan,  time taken: 198s          \n",
      "epoch: 10, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 119510, memory365: nan,  time taken: 201s          \n",
      "epoch: 11, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 122219, memory365: nan,  time taken: 202s          \n",
      "epoch: 12, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 132263, memory365: nan,  time taken: 204s          \n",
      "epoch: 13, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 121581, memory365: nan,  time taken: 206s          \n",
      "epoch: 14, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 112623, memory365: nan,  time taken: 207s          \n",
      "epoch: 15, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 109887, memory365: nan,  time taken: 209s          \n",
      "epoch: 16, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 109279, memory365: nan,  time taken: 212s          \n",
      "epoch: 17, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 131004, memory365: nan,  time taken: 215s          \n",
      "epoch: 18, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 100201, memory365: nan,  time taken: 217s          \n",
      "epoch: 19, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 131718, memory365: nan,  time taken: 220s          \n",
      "epoch: 20, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 133023, memory365: nan,  time taken: 222s          \n",
      "epoch: 21, loss: nan, val_loss: nan, back_test: 0.0, nBettings: 0, baseId_1: 118170, memory365: nan,  time taken: 229s          \n"
     ]
    }
   ],
   "source": [
    "epochs = 500;  prev_loss = float(\"inf\")\n",
    "for epoch in range(history.len(), epochs):\n",
    "    start_time = time.time()\n",
    "    m = 0; epoch_loss = 0.0\n",
    "    n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "    selectivity = schedule(epoch)    # noralized_profit_pred_multiplier is scheduled here.\n",
    "\n",
    "    baseId_1 = None\n",
    "    train_batches = make_train_batches(train_ds)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(train_batches):\n",
    "        if baseId_1 is None: baseId_1 = baseId[0]\n",
    "        # print('train', sequence.shape)\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        batch_loss = train_step(x, y, selectivity)\n",
    "        n += 1; loss = loss * (n-1) / n + batch_loss / n\n",
    "        m += 1; epoch_loss = epoch_loss * (m-1)/m + batch_loss / m\n",
    "\n",
    "        samples_seen += sequence.shape[0]\n",
    "        if step % 50 == 0:\n",
    "            show_steps(epoch, step, loss, samples_seen)\n",
    "            n = 0; loss = 0.0\n",
    "\n",
    "    show_steps(epoch, step, loss, samples_seen)\n",
    "    val_loss = test_with_dataset(test_batches, selectivity)\n",
    "    profit_back_mean, nBettingsTotal = back_test_with_dataset(test_batches)\n",
    "    # back2 = back_test_with_dataset2(test_batches)\n",
    "    save_checkpoint(epoch_loss, val_loss, profit_back_mean, nBettingsTotal)     #------------------------------------------- comeback\n",
    "\n",
    "    eM365W = EPL.layers[0].layers[0].get_weights()[6]; eM365W = list(tf.reshape(eM365W, (-1,)).numpy())\n",
    "    # shift = tf.squeeze(EPL.layers[-1].get_weights()).numpy()\n",
    "    # dM365W =EPL.layers[0].layers[1].get_weights()[4]; dM365W = list(tf.reshape(dM365W, (-1,)).numpy())\n",
    "\n",
    "    print(\"epoch: {}, loss: {}, val_loss: {}, back_test: {}, nBettings: {}, baseId_1: {}, memory365: {:.4f},  time taken: {:.0f}s          \"\n",
    "          .format(epoch, float(epoch_loss), float(val_loss), float(profit_back_mean), nBettingsTotal, baseId_1, eM365W[0] * hyperparams.initial_m365, (time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
