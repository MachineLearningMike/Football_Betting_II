{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as ps\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "\n",
    "# from config import config\n",
    "import data_helpers\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = 'U.H.07'\n",
    "TRAIN_PERCENT= 90\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 48\n",
    "TEAM_EMBS = 50\n",
    "LOSS_RAMBDA = 1.0\n",
    "LEARNING_RATE = 0.00001\n",
    "MAE_NOT_MCE_P_LOSS = True\n",
    "TRANSFORMER_DROP = 0.2  #-------------------\n",
    "TRANSFORMER_LAYERS = 6\n",
    "TRANSFORMER_HEADS = 6\n",
    "RESET_HISTORY = False\n",
    "id_to_ids_filename = 'England-200-1e-07-7300-75-0.8-False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/England\"\n",
    "df = data_helpers.get_master_df_from_football_data_co_uk(countryDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'id_to_ids', id_to_ids_filename + '.json')\n",
    "id_to_ids = data_helpers.LoadJsonData(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "_Cols_List_to_Embedd = [Div_cols, Team_cols, Goal_cols, Result_cols]\n",
    "_Cols_List_to_Standardize = [Odds_cols, Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_for_Label = [Full_Goal_cols, Odds_cols]\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "base_bbab = list(df.loc[df['id'] == 100000, BBAB_cols].iloc[0, :])\n",
    "print(base_bbab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = df[Red_cols]\n",
    "array = np.array(array)\n",
    "print(array.mean(), array.std())\n",
    "array = (array - array.mean()) / array.std()\n",
    "print(array)\n",
    "print(array.mean(), array.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_path = os.path.join('./data', 'datasets', id_to_ids_filename + \".json\")\n",
    "std_params = get_standardization_params(df)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_path = os.path.join('./data', 'datasets', id_to_ids_filename)\n",
    "\n",
    "ds = generate_dataset_uk(df, id_to_ids, tokenizer_team, std_params)\n",
    "tf.data.Dataset.save(ds, ds_path)\n",
    "\n",
    "ds = tf.data.Dataset.load(ds_path)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = len(ds)\n",
    "print(dataset_size)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "print(train_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "test_ds = ds.skip(train_size)\n",
    "\n",
    "print(len(train_ds), len(test_ds), len(ds), len(ds)-len(train_ds)-len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bbas_tensor = get_dummy_bbas_tensor_uk(df, tokenizer_team, std_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_row(baseId, sequence, base_bb, base_label):\n",
    "    try:\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org    # MAX_TOKENS - tf.shape(sequence)[0].numpy()\n",
    "        if nMissings > 0:\n",
    "            block = tf.stack([dummy_bbas_tensor] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, block], axis=0) \n",
    "        # print(\"sequence 1\", sequence.shape)\n",
    "        # sequence[:, 2] = base[2] - sequence[:, 2]   # get delta days.\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, nFeatures)\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]\n",
    "        # print('normalize', sequence.shape, base.shape, mask.shape, mask)\n",
    "        # seq_len_org = tf.Variable(seq_len_org, dtype=tf.int32)    #--------------------------------- comeback\n",
    "        return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "    except:\n",
    "        print('normalize_row exception')\n",
    "        print('norm 1', sequence.shape, base_bb.shape, base_label.shape, mask.shape, nMissings)\n",
    "        print('norm 2', baseId, sequence, base_label, mask, nMissings)\n",
    "        # return (baseId, sequence, base_bb, base_label, mask, seq_len_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 600\n",
    "for (baseId, sequence, base_bb, base_label) in train_ds:\n",
    "    z = normalize_row(baseId, sequence, base_bb, base_label)\n",
    "    cnt -= 1\n",
    "    if cnt == 0:\n",
    "        print(baseId)\n",
    "        print(sequence[0])\n",
    "        break\n",
    "# [div 4, teams 8, odds 12, goals 16, results 8, Shoot 2, ShootT 2;    Corner 2, Faul 2, Yellow 2, Red 2]\n",
    "# 0 Yellows -> -1.25, 0 Reds -> -.2806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparams:    \n",
    "    nDivisions = 4 + 1  # E0, E1, E2, E3, and Unknown\n",
    "    division_embs = 4\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    happen_size = 3 + 1 # 1 for place-holder, for even d_model.\n",
    "    nResults = 4    # HWin, Draw, AWin, and Unknown\n",
    "    result_embs = 4\n",
    "    # odds removed\n",
    "    d_model = get_std_size()    + division_embs * len(Div_cols) + team_embs * len(Team_cols) \\\n",
    "                                + happen_size - len(Odds_cols)\n",
    "    batch_size = BATCH_SIZE\n",
    "    max_tokens = MAX_TOKENS\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    days_spanning_years = 30\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hyperparams.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(baseId, sequence, base_bb, base_label, mask, seq_len_org):\n",
    "    # target = tf.one_hot(tf.squeeze(tf.cast(base_bbab[:, :, -1], dtype=tf.int32), axis=-1), hyperparams.target_onehot_size)\n",
    "    \n",
    "    return (baseId, sequence, base_bb, mask), (base_label, seq_len_org)     # (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(ds):\n",
    "    return (\n",
    "        ds.map(lambda baseId, sequence, base_bb, base_label: tf.py_function(\n",
    "            func=normalize_row,\n",
    "            inp=[baseId, sequence, base_bb, base_label],\n",
    "            Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def make_test_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "# Normalize_row\n",
    "# In: baseId, sequence, base_bb, base_label\n",
    "# Out: baseId, sequence, base_bb, base_label, mask, seq_len_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_train_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(train_ds_path):\n",
    "    train_ds = tf.data.Dataset.load(train_ds_path)\n",
    "else:\n",
    "    train_ds = normalize_dataset(train_ds)\n",
    "    tf.data.Dataset.save(train_ds, train_ds_path)\n",
    "\n",
    "train_batches = make_train_batches(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_test_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(test_ds_path):\n",
    "    test_ds = tf.data.Dataset.load(test_ds_path)\n",
    "else:\n",
    "    test_ds = normalize_dataset(test_ds)\n",
    "    tf.data.Dataset.save(test_ds, test_ds_path)\n",
    "\n",
    "test_batches = make_test_batches(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in train_batches:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(memory, depth):\n",
    "    positions = tf.range(memory.shape[-1], dtype=tf.float32)\n",
    "    fractional_pos = memory * positions    # fractional position: (batch, fractional position #)\n",
    "    depth = depth/2\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth\n",
    "    depths = tf.pow(10000.0, depths)    # (depth,)\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] / depths  # (batch, fractional position #, depth)\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = tf.ones((100, 200), dtype=tf.float32) * 0.5\n",
    "pos_encoding = positional_encoding(memory, depth=512)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, hyperparams, isEncoder=True):\n",
    "    super().__init__()\n",
    "    self.isEncoder = isEncoder\n",
    "    self.division_embedding = tf.keras.layers.Embedding(hyperparams.nDivisions, hyperparams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.team_embedding = tf.keras.layers.Embedding(hyperparams.nTeams, hyperparams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    # self.goal_embedding = tf.keras.layers.Embedding(hyperparams.nGoals, hyperparams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn 0-goal\n",
    "    # self.result_embedding = tf.keras.layers.Embedding(hyperparams.nResults, hyperparams.result_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "    self.d_model = hyperparams.d_model\n",
    "    # print(self.d_model)\n",
    "    self.position_permuting_dense = tf.keras.layers.Dense(self.d_model)\n",
    "    self.m365_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "\n",
    "    self.idx_Days = BB_cols.index('Date')\n",
    "    assert self.idx_Days == BBAB_cols.index('Date')\n",
    "\n",
    "  def call(self, x):\n",
    "    # print('x', x[0].shape, x[1].shape)\n",
    "    (sequence, base_bb, mask) = x # sob = sequence or base_bb\n",
    "    sDays = sequence[:, :, self.idx_Days]\n",
    "    bDays = base_bb[:, :, self.idx_Days]\n",
    "    # print('days shapes', days.shape)\n",
    "    \n",
    "    # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "    # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "    sob = None\n",
    "    if self.isEncoder:\n",
    "      sob = sequence\n",
    "    else:\n",
    "      sob = base_bb\n",
    "\n",
    "    if self.isEncoder:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, goals, results, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Goal_cols), len(Result_cols),  -1], axis=-1)\n",
    "      # print('1', remainder[0, 0])\n",
    "    else:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "      # print('2', remainder[0, 0])  \n",
    "\n",
    "    div = self.division_embedding(tf.cast(div, dtype=tf.int32))\n",
    "    div = tf.reshape(div, [div.shape[0], div.shape[1], -1])\n",
    "    teams = self.team_embedding(tf.cast(teams, dtype=tf.int32))\n",
    "    teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1])\n",
    "    if self.isEncoder:\n",
    "      htGoals, ftGoals = tf.split(goals, [2, 2], axis=-1)\n",
    "      # print('goals', ftGoals.shape)\n",
    "      ftGoals = tf.cast(ftGoals, dtype=tf.float32)\n",
    "      placeHolder = tf.cast(tf.zeros_like(ftGoals[:, :, 1]), dtype=tf.bool)\n",
    "      happen = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]), placeHolder)\n",
    "      # happen: (batch, seq, 3)      \n",
    "      happen = tf.cast(tf.transpose(happen), dtype=tf.float32)  # (batch, nQueries=3)\n",
    "      happen = tf.einsum('jik', happen)\n",
    "      # print('happen', happen.shape)\n",
    "      # goals = self.goal_embedding(tf.cast(goals, dtype=tf.int32))\n",
    "      # goals = tf.clip_by_value(goals, 0, hyperparams.nGoals)\n",
    "      # goals = tf.reshape(goals, [goals.shape[0], goals.shape[1], -1])\n",
    "      # results = self.result_embedding(tf.cast(results, dtype=tf.int32))\n",
    "      # results = tf.reshape(results, [results.shape[0], results.shape[1], -1])\n",
    "      # print('results', results.shape)\n",
    "    \n",
    "    if self.isEncoder:\n",
    "      concat = [div, teams, happen, remainder]  # odds removed for this test.\n",
    "      # print('10', div.shape, teams.shape, odds.shape, goals.shape, results.shape, remainder.shape)\n",
    "      # print('11', remainder[0,0])\n",
    "    else:\n",
    "      concat = [div, teams, remainder]  # odds removed.\n",
    "      # print('22', remainder[0,0])\n",
    "\n",
    "    sob = tf.concat(concat, axis=-1)\n",
    "    # print('sob', sob.shape)\n",
    "    sob = self.position_permuting_dense(sob)\n",
    "    # print('33', sob[0][0])\n",
    "\n",
    "    days_ago = tf.cast(bDays - sDays, dtype=tf.float32) if self.isEncoder else tf.cast(bDays - bDays, dtype=tf.float32)\n",
    "    \n",
    "    m365 = self.m365_embedding(tf.zeros_like((hyperparams.m365_size,), dtype=tf.float32)) * hyperparams.initial_m365  # expected shape: (1, hyperparams.remain_365_size)\n",
    "    m365 = tf.squeeze(m365, axis=0)\n",
    "    # Let m365 drift freely by commenting this line out, though I am sure it will be within (0, 1)\n",
    "    # m365 = tf.clip_by_value(m365, 0.01, 1.0)\n",
    "    memory_alpha = tf.math.pow(m365, 1.0/365) # (hyperparams.m365_size,)\n",
    "    memory = tf.math.pow(memory_alpha, days_ago[:, :, tf.newaxis])  # decrease as days_ago increase, if memory <= 1.0 as expected.\n",
    "    memory = tf.reduce_mean(memory, axis=-1)\n",
    "    # print('pe1', memory.shape)\n",
    "\n",
    "    pe = positional_encoding(memory, depth=sob.shape[-1]) # (batch, d_model)\n",
    "    # print('pe2', pe.shape)\n",
    "    # print('x, pe, d_model', x.shape, pe.shape, self.d_model)\n",
    "    # This factor sets the relative scale of the embedding and positonal_encoding.\n",
    "    pe = pe / tf.math.sqrt(tf.cast(sob.shape[-1], tf.float32))\n",
    "    # print('pe3', pe.shape)\n",
    "    sob = sob + pe  # comeback\n",
    "\n",
    "    if self.isEncoder:\n",
    "      mask = mask\n",
    "    else:\n",
    "      mask = mask[:, 0:sob.shape[1], :]\n",
    "\n",
    "    return (sob, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "\n",
    "cnt = 2\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, mask), (base_label, seq_len_org) = z\n",
    "    cnt -= 1\n",
    "    if cnt == 0: break\n",
    "# print('baseId', baseId)\n",
    "sample_x = (sequence, base_bb, mask)\n",
    "eSob, eMask = pos.call(sample_x)\n",
    "print(eSob.shape, eMask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axix is normalized across. No inter-token operatoin.\n",
    "      self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(\n",
    "          query=x,\n",
    "          key=context,\n",
    "          value=context,\n",
    "          attention_mask=mask,\n",
    "          return_attention_scores=True)\n",
    "    \n",
    "      # Cache the attention scores for plotting later.\n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          attention_mask=mask)    # intentional inter-token operation\n",
    "      x = self.add([x, attn_output])  # token-wise\n",
    "      x = self.layernorm(x)         # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          use_causal_mask = True)     # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])  # mask-agnostic\n",
    "      x = self.layernorm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),    # across -1 axis\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])  # mask-agnostic\n",
    "      x = self.layer_norm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hyperparams.d_model,\n",
    "                      num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_embedding(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "\n",
    "      self.causal_self_attention = CausalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "      \n",
    "      self.cross_attention = CrossAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "\n",
    "      # Cache the last attention scores for plotting later\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hyperparams.d_model, num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_embedding(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens)     \n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.decoder = Decoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(hyperparams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, inputs):\n",
    "      # inputs = (sequence, base_bb, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), mask: (batch, max_token, max_token)\n",
    "      x = self.encoder(inputs)  # (batch, max_tokens, d_model)\n",
    "      x = self.decoder(inputs, x)  # (batch, 1, d_model)\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_output, d_middle, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.seq = tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(d_middle, activation='relu'),    # across -1 axis\n",
    "      tf.keras.layers.Dense(d_output),    # across -1 axis\n",
    "      tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "    ])\n",
    "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    x = self.layer_norm(x)  # Do we need this?\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneExTwo(tf.keras.Model):\n",
    "  def __init__(self, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.nQueries = 3\n",
    "    self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "    self.adaptor = Adaptor(self.nQueries, hyperparams.d_model)\n",
    "    self.mce = tf.keras.losses.CategoricalCrossentropy(\n",
    "      from_logits=False, axis=-1, reduction='sum_over_batch_size'\n",
    "    )\n",
    "\n",
    "    return\n",
    "  \n",
    "  def call(self, input):\n",
    "    # expected inputs.shape: (batch, hyperparas.d_model)\n",
    "    happen_logits = self.adaptor(input)     # (batch, nQueries)\n",
    "    happen_p = self.softmax(happen_logits)\n",
    "    return happen_p # (batch, nQueries)\n",
    "\n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "    # ftGoals:  (batch, 2)\n",
    "    ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "    h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "    h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "    return h\n",
    "  \n",
    "  def loss(self, ftGoals, happen_p, rambda):\n",
    "    happen_t = self.h_true(ftGoals)\n",
    "    loss = self.mce(happen_t, happen_p)   # reduce_sum\n",
    "    return loss\n",
    "  \n",
    "  def profit_back(self, ftGoals, odds, happen_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    stake_p = happen_p  # (batch, nQueries)\n",
    "    # # Choose not to try one-hot, meaning we bet on all queries with different amount.\n",
    "    stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(stake_p).indices, tf.shape(stake_p)[-1]), axis=1) # (batch, nQueries)\n",
    "    profit_back = tf.math.multiply(oh_1_t, stake_p)   # (batch, nQueries)\n",
    "    profit_back = tf.math.reduce_sum(profit_back, axis=-1)  # (batch,)  sum across queries  \n",
    "    profit_back = tf.math.reduce_mean(profit_back, axis=None)  # () mean across games.\n",
    "    return profit_back  # ()\n",
    "  \n",
    "  def profit_back_for_best_games(self, ftGoals, odds, happen_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    stake_p = happen_p  # (batch, nQueries)\n",
    "    # # Choose not to try one-hot, meaning we bet on all queries with different amount.\n",
    "    stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(stake_p).indices, tf.shape(stake_p)[-1]), axis=1) # (batch, nQueries)\n",
    "    profit_back = tf.math.multiply(oh_1_t, stake_p)   # (batch, nQueries)\n",
    "    profit_back = tf.math.reduce_sum(profit_back, axis=-1)  # (batch,)  sum across queries  \n",
    "    profit_back = tf.math.reduce_mean(profit_back, axis=None)  # () mean across games.\n",
    "    return profit_back  # ()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[3.0, 3.0, 4.0], [0.1, 0.7, 0.3]])\n",
    "one_hot_a = tf.squeeze(tf.one_hot(tf.nn.top_k(a).indices, tf.shape(a)[-1]), axis=1)\n",
    "print(one_hot_a)\n",
    "# one_hot_a = [[ 0.  0.  1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BettingEPL(tf.keras.Model):\n",
    "  def __init__(self, hyperparams, loss_rambda=1.0, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.transformer = Transformer(hyperparams, dropout_rate=dropout_rate)\n",
    "    self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "    self.qGroup = OneExTwo()\n",
    "    self.rambda = loss_rambda     #----------------------- Sensitive rambda!!!, Automate optimizing it.\n",
    "\n",
    "  def call(self, input):\n",
    "    x = self.transformer(input)\n",
    "    happen_p = self.qGroup(x)\n",
    "    return happen_p  # happen_p: (batch, nQueries)\n",
    "  \n",
    "  def loss(self, y, happen_p):\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( happen_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, sum[qGropu.nQueries for qGroup in self.qGroups])\n",
    "    loss = self.qGroup.loss(ftGoals, happen_p, self.rambda)\n",
    "    return loss\n",
    "  \n",
    "  def back_test(self, y, happen_p):\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( shape: (batch, nQueries), shape: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    ftGoals, odds = tf.split(y, [2, -1], axis=-1)\n",
    "    odds_by_qGroup = tf.split(odds, [self.qGroup.nQueries for qQroup in self.bookies], axis=-1)\n",
    "    scalar_profit_sum_query_mean_batch = [self.qGroup.profit_back(ftGoals, odds, happen_p) \n",
    "                   for odds in odds_by_qGroup] # [ () for _ in self.qGroups]\n",
    "    profit_back = tf.stack(scalar_profit_sum_query_mean_batch, axis=0) # (nQGroups,)\n",
    "    profit_mean_game_mean_qGroup = tf.math.reduce_mean(profit_back)  # mean across qGroups\n",
    "\n",
    "    return profit_mean_game_mean_qGroup # profit_mean_game_mean_qGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPL = BettingEPL(hyperparams, loss_rambda = LOSS_RAMBDA, dropout_rate=TRANSFORMER_DROP)\n",
    "\n",
    "# for z in train_batches:\n",
    "#     (baseId, sequence, base_bb, mask), (base_label, seq_len_org) = z\n",
    "#     x = (sequence, base_bb, mask)\n",
    "#     print(sequence.shape)\n",
    "#     break\n",
    "# x = (sequence, base_bb, mask)\n",
    "# y = EPL(x, training=False)\n",
    "# print(len(y))\n",
    "# print(len(y[0]))\n",
    "# happen_p = y[0]\n",
    "# print(happen_p.shape)\n",
    "# # print(profit_p, stake_p)   # profit_p tend to have the same sign in the same batch.\n",
    "\n",
    "# EPL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(EPL.layers[0].layers[0].get_weights()[6])\n",
    "# print(EPL.layers[0].layers[1].get_weights()[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = CustomSchedule(hyperparams.d_model)\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.95, beta_2=0.95, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss_uk(label, y_pred):\n",
    "  # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch, 1)), y_pred: (batch, 3)\n",
    "  y_true = label[0]   # one_hot: (batch, 3)\n",
    "  seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "  mask = y_true != 0 \n",
    "  loss = loss_object(y_true, y_pred)\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask) # eq. sum_loss / batch\n",
    "  return loss\n",
    "\n",
    "\n",
    "class recall():\n",
    "  def __init__(self, name='recall', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    # print('recall', y_true.shape, y_pred.shape, seq_len_mask.shape)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    # print('recall', true_positives.numpy())\n",
    "    possible_positives = tf.math.reduce_sum(y_true)\n",
    "    recall_keras = true_positives / (possible_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall_keras.numpy() / self.n\n",
    "\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.math.reduce_sum(y_pred)\n",
    "    precision_keras = true_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision_keras.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = EPL(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value = EPL.loss(y, outputs)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, EPL.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, EPL.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value = EPL.loss(y, outputs)\n",
    "    # recall_object.update_state(y, val_logits)\n",
    "    # precision_object.update_state(y, val_logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def back_test_step(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    profit_back = EPL.back_test(y, outputs)\n",
    "    return profit_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def back_test_step2(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    profit_back = EPL.back_test2(y, outputs)\n",
    "    return profit_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def back_test_with_dataset(datsset):\n",
    "    n = 0\n",
    "    profit_back_total = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        profit_back = back_test_step(x, y)\n",
    "        n += 1\n",
    "        profit_back_total = profit_back_total * (n-1) / n + profit_back / n\n",
    "    return profit_back_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def back_test_with_dataset2(datsset):\n",
    "    n = 0\n",
    "    profit_back_total = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        profit_back = back_test_step2(x, y)\n",
    "        n += 1\n",
    "        profit_back_total = profit_back_total * (n-1) / n + profit_back / n\n",
    "    return profit_back_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def test_with_dataset(datsset):\n",
    "    n = 0\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        n += 1\n",
    "        val_loss = val_loss * (n-1) / n + test_step(x, y) / n\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return round(x, sig-int(math.floor(math.log10(abs(x))))-1)\n",
    "    def __init__(self):\n",
    "        self.history = {'loss': [], 'val_loss': [], 'back100': []}\n",
    "    def save(self, path):\n",
    "        data_helpers.SaveJsonData(self.history, path)\n",
    "    def load(self, path):\n",
    "        self.history = data_helpers.LoadJsonData(path)\n",
    "        if self.history is None:\n",
    "            self.history = {'loss': [], 'val_loss': [], 'back100': []}\n",
    "    def to_back100(self, back):\n",
    "        # return float(back * 100 if back >= 0 else back)\n",
    "        return float(back if back >= 0 else back)\n",
    "    def append(self, loss, val_loss, back):\n",
    "        self.history['loss'].append(self.round_sig(float(loss), 4))\n",
    "        self.history['val_loss'].append(self.round_sig(float(val_loss), 4))\n",
    "        self.history['back100'].append(self.round_sig(self.to_back100(back), 4))\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['back100'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_latest_item(self):\n",
    "        if self.len() > 0:\n",
    "            return (self.history['loss'][-1], self.history['val_loss'][-1], self.history['back100'][-1])\n",
    "        else:\n",
    "            return (float('inf'), float('inf'), -1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_steps(epoch, step, loss, samples_seen):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hyperparams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {}, samples_seen: {}          \".\n",
    "            format(epoch, step, float(loss), samples_seen), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def show_history(history, baseline=0):\n",
    "    plt.plot(history.history['loss'][baseline:])\n",
    "    plt.plot(history.history['val_loss'][baseline:])\n",
    "    if history.len() > 0:\n",
    "        base1 = min(history.history['loss'])\n",
    "        base2 = min(history.history['val_loss'])\n",
    "        base = min(base1, base2)\n",
    "    else:\n",
    "        base = 0.0\n",
    "    plt.plot([base + b100 for b100 in history.history['back100']][baseline:])\n",
    "    plt.axhline(y=base, color='r', linestyle='-', linewidth=0.8)\n",
    "    maxBackTest = max(history.history['back100'][baseline:]) if history.len() > baseline else -1.0\n",
    "    plt.title(TEST_ID + \": max back100: {}, history len: {}\".format(maxBackTest, history.len()))\n",
    "    plt.grid(True)\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'val_loss', 'val_profit'], loc='lower left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join('./data', 'checkpoints', TEST_ID + '_weights')\n",
    "historyPath = os.path.join('./data', 'checkpoints', TEST_ID + '_history.json')\n",
    "\n",
    "history = history_class()\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    files = glob.glob(checkpointPath + \"*\")         # \"*.*\" doesn't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    files = glob.glob(historyPath + \"*\")            # \"*.*\" doens't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    EPL.save_weights(checkpointPath)\n",
    "    history.save(historyPath)\n",
    "\n",
    "try:\n",
    "    EPL.load_weights(checkpointPath)\n",
    "except:\n",
    "    print('Failed to load model weights.')\n",
    "\n",
    "history.load(historyPath)\n",
    "# if history.len() <= 0:\n",
    "#     print('Creating historic baseline...', end='')\n",
    "#     loss = test_with_dataset(train_batches)\n",
    "#     val_loss = test_with_dataset(test_batches)\n",
    "#     back = back_test_with_dataset(test_batches)\n",
    "#     history.append(loss, val_loss, back)\n",
    "#     history.save(historyPath)\n",
    "#     print('done')\n",
    "\n",
    "def save_checkpoint(loss, val_loss, back):\n",
    "    (pre_loss, pre_val_loss, pre_back100) = history.get_latest_item()\n",
    "    if float(history.to_back100(back)) > pre_back100:\n",
    "        EPL.save_weights(checkpointPath)\n",
    "    history.append(loss, val_loss, back)\n",
    "    history.save(historyPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in history.history.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_history(history, baseline=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500;  prev_loss = float(\"inf\")\n",
    "for epoch in range(history.len(), epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    m = 0; epoch_loss = 0.0\n",
    "    n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "\n",
    "    baseId_1 = None\n",
    "    train_batches = make_train_batches(train_ds)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(train_batches):\n",
    "        if baseId_1 is None: baseId_1 = baseId[0]\n",
    "        # print('train', sequence.shape)\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        batch_loss = train_step(x, y)\n",
    "        n += 1; loss = loss * (n-1) / n + batch_loss / n\n",
    "        \n",
    "        m += 1; epoch_loss = epoch_loss * (m-1)/m + batch_loss / m\n",
    "\n",
    "        samples_seen += sequence.shape[0]\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            show_steps(epoch, step, loss, samples_seen)\n",
    "            n = 0; loss = 0.0\n",
    "\n",
    "    show_steps(epoch, step, loss, samples_seen)\n",
    "    val_loss = test_with_dataset(test_batches)\n",
    "    back = back_test_with_dataset(test_batches)\n",
    "    # back2 = back_test_with_dataset2(test_batches)\n",
    "    save_checkpoint(epoch_loss, val_loss, back)     #------------------------------------------- comeback\n",
    "\n",
    "    eM365W = EPL.layers[0].layers[0].get_weights()[6]; eM365W = list(tf.reshape(eM365W, (-1,)).numpy())\n",
    "    # dM365W =EPL.layers[0].layers[1].get_weights()[4]; dM365W = list(tf.reshape(dM365W, (-1,)).numpy())\n",
    "\n",
    "    print(\"epoch: {}, loss: {}, val_loss: {}, back_test: {}, baseId_1: {}, memory365: {:.4f}, time taken: {:.0f}s          \"\n",
    "          .format(epoch, float(epoch_loss), float(val_loss), float(back), baseId_1, eM365W[0] * hyperparams.initial_m365, (time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
