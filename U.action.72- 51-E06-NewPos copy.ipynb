{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as ps\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = 'UK.B.A.72'\n",
    "BASE_TEST_ID = ''  # should be either '' or an existing TEST_ID.\n",
    "DATA_BITS = 32              #------------------ 16 doesn't work.\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 6          # 0 -> 6\n",
    "TRAIN_PERCENT= 94\n",
    "VALID_PERCENT = 3\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT\n",
    "BUFFER_SIZE = 35000\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 3e-7 # 1e-6 : [:44] # 1e-6 if BASE_TEST_ID == ''. \n",
    "TEAM_EMBS = 50\n",
    "DROPOUT = 0.0               # 0.1 -> 0.0\n",
    "TRANSFORMER_LAYERS = 6\n",
    "TRANSFORMER_HEADS = 60\n",
    "ADAPTORS_LAYERS = 10\n",
    "RESET_HISTORY = False\n",
    "MIN_PROFIT = -1.0\n",
    "PROFIT_KEYS = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "id_to_ids_filename = 'England-300-1e-07-7300-75-0.9-False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config types: <class 'numpy.int32'> <class 'numpy.float32'> <dtype: 'int32'> <dtype: 'float32'>\n"
     ]
    }
   ],
   "source": [
    "# if DATA_BITS == 32:\n",
    "#     np.int32 = np.int32; np.float32 = np.float32; tf.int32 = tf.int32; tf.float32 = tf.float32\n",
    "# elif DATA_BITS == 16:\n",
    "#     np.int32 = np.int16; np.float32 = np.float16; tf.int32 = tf.int16; tf.float32 = tf.float16\n",
    "\n",
    "# from config import config\n",
    "# config['np.int32'] = np.int32; config['np_flaot'] = np.float32; config['tf.int32'] = tf.int32; config['tf.float32'] = tf.float32\n",
    "\n",
    "import data_helpers\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 32\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/England\"\n",
    "df = data_helpers.get_master_df_from_football_data_co_uk(countryDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "['tottenham', 'arsenal', 'liverpool', '[UNK]', 'tottenham', 'chelsea', '[UNK]', 'man_united', '[UNK]', '[UNK]', '[UNK]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[102, 4, 59, 0, 102, 28, 0, 63, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tottenham arsenal liverpool tottenham chelsea man_united'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Max length of ids: 300')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjoAAAGxCAYAAABr1xxGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA0wUlEQVR4nO3de3RU5aH+8WcSkiHEMOYiGaYgRUEUE3A12BAQg1wCNAFb9RSN5kClUOVmlqCV2iPoaSEHLdUeFKxFxVMk7ToQjwtoSiyYNiWBEMlpAsVjK3cTUEgmIUKu7+8PF/vnEG4BJPLm+1lrr+Xs/czsd7/u5TzumZ1xGWOMAAAALBTU3gMAAAD4qlB0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXSAK+jNN9+Uy+WSy+XS+++/32q7MUZ9+vSRy+XS8OHDr/j4Jk+erG9+85tXfL9n8/bbb+vFF19stX7v3r1yuVx64YUXvvIx7NixQ8nJyfJ4PHK5XGcczykul0sLFiw472ueOg/27t172cZ5uh/+8IeKi4vTtddeq7CwMN1000164okn9Nlnn7XKHj9+XJmZmfL5fOrcubNuu+02ZWdnn/F1P/jgA40aNUrXXHONrr32Wt1zzz36+OOPv7LjAC5Vp/YeANARRUREaMWKFa3KTH5+vv75z38qIiKifQb2NfP222+rvLxcmZmZ7TaGhx9+WHV1dcrOzlZkZOQ5i2BhYaF69Ohx5QZ3DnV1dZo2bZr69Omjzp07a/v27fr5z3+uDRs2aMeOHQoNDXWy99xzj4qLi5WVlaWbbrpJb7/9th544AG1tLQoPT3dye3evVvDhw/Xbbfdpt///vc6efKknnnmGQ0bNkylpaW67rrr2uNQgXOi6ADtYOLEiVq1apVefvllde3a1Vm/YsUKJSUlqaamph1Hhy8rLy/X1KlTNW7cuPNmBw8efAVGdGFWr14d8HjEiBGKiIjQ9OnTVVBQoBEjRkiSNmzYoLy8PKfcSNJdd92lffv26YknntDEiRMVHBwsSXrmmWfkdru1bt0657xNSEhQ37599cILL+g//uM/ruARAheGj66AdnDqDeXLb0Z+v19r1qzRww8/fMbnPPvss0pMTFRUVJS6du2qb33rW1qxYoW+/Lu8BQUFCgkJ0dy5cwOee+qjkhUrVrR5rMYYvfLKK7rtttsUFhamyMhI3Xfffa0+rhg+fLji4uJUXFysYcOGqUuXLrrhhhuUlZWllpaWgOzOnTuVkpKiLl266LrrrtOMGTO0fv36gI/0hg8frvXr12vfvn3Ox30ul6vV+JYsWaLevXvrmmuuUVJSkoqKii7ouMrLy3X33XcrMjLS+bhm5cqVzvZTc9bU1KRly5addf9fdqaProqKijR06FB17txZPp9P8+bNU2NjY6vnbtq0ScOHD1d0dLTCwsJ0/fXX695779Xnn39+QcdzIU5dcenU6f//P25OTo6uueYa/cu//EtA9gc/+IE++eQTbd26VZLU1NSkdevW6d577w0o57169dJdd92lnJycyzZO4HKi6ADtoGvXrrrvvvv0+uuvO+tWr16toKAgTZw48YzP2bt3r370ox/p97//vdauXat77rlHs2bN0r//+787mTvuuEM/+9nP9Itf/ELvvvuupC9KxYwZM/TQQw9pypQpbR7rj370I2VmZmrUqFF655139Morr2jnzp0aMmSIDh8+HJCtrKzUgw8+qIceekjvvvuuxo0bp3nz5um3v/2tk6moqFBycrI+/PBDLVu2TG+99ZZqa2s1c+bMgNd65ZVXNHToUHm9XhUWFjrLl7388svKy8vTiy++qFWrVqmurk7f+c535Pf7z3lMH374oYYMGaKdO3fqV7/6ldauXav+/ftr8uTJWrx4sSQpNTXV2d999913xv2fz65duzRy5EhVV1frzTff1PLly7Vjxw797Gc/C8jt3btXqampCg0N1euvv67c3FxlZWUpPDxcDQ0NTm7y5Mlt/m5PU1OT6urq9Ne//lX/9m//pjvuuENDhw51tpeXl+uWW24JKD+SNGDAAGe7JP3zn//UiRMnnPWnZ//xj3/o5MmTFzwu4IoxAK6YN954w0gyxcXFZvPmzUaSKS8vN8YYc/vtt5vJkycbY4y59dZbTXJy8llfp7m52TQ2NprnnnvOREdHm5aWFmdbS0uL+c53vmOuvfZaU15ebvr3729uvvlmc/z48fOOb9KkSaZXr17O48LCQiPJ/OIXvwjIHThwwISFhZknn3zSWZecnGwkma1btwZk+/fvb8aMGeM8fuKJJ4zL5TI7d+4MyI0ZM8ZIMps3b3bWpaamBoznlD179hhJJj4+3jQ1NTnrt23bZiSZ1atXn/M477//fuN2u83+/fsD1o8bN8506dLFVFdXO+skmRkzZpzz9b6cnT9/vvN44sSJJiwszFRWVjrrmpqazM0332wkmT179hhjjPnv//5vI8mUlpae8/UffvhhExwcbPbu3XtB4zn17+/U8p3vfMfU1NQEZPr27Rvw7+eUTz75xEgyCxcuNMYY89e//vWsc7tw4UIjyXzyyScXNC7gSuKKDtBOkpOTdeONN+r1119XWVmZiouLz/qxlfTFRxujRo2Sx+NRcHCwQkJC9Mwzz+jo0aM6cuSIk3O5XHrrrbcUERGhQYMGac+ePfr973+v8PDwNo9x3bp1crlceuihh9TU1OQsXq9XAwcObHXnmNfr1be//e2AdQMGDNC+ffucx/n5+YqLi1P//v0Dcqc+zmuL1NRU5/sjp/YlKWB/Z7Jp0yaNHDlSPXv2DFg/efJkff75522+cnM2mzdv1siRIxUbG+usCw4ObnXV7rbbblNoaKimTZumlStXnvUuphUrVqipqUm9evW6oP3Hx8eruLhY+fn5eumll7Rjxw6NHj261cdh5/pI7vRtbckCXwcUHaCduFwu/eAHP9Bvf/tbLV++XDfddJOGDRt2xuy2bduUkpIiSXrttdf017/+VcXFxXr66aclSSdOnAjIR0dHa8KECTp58qTGjh2r+Pj4ixrj4cOHZYxRbGysQkJCApaioqJWtypHR0e3eg232x0wvqNHjwa88Z9ypnXnc/r+3G63pNbzcbqjR4+qe/furdb7fD5n++Vw9OhReb3eVutPX3fjjTfqvffeU7du3TRjxgzdeOONuvHGG/XSSy9d0v7Dw8M1aNAg3XnnnZo9e7ZycnK0detWvfrqq04mOjr6jMd77NgxSVJUVJSTO3VMZ8q6XC5de+21lzRe4KvAXVdAO5o8ebKeeeYZLV++XD//+c/PmsvOzlZISIjWrVunzp07O+vfeeedM+bz8vK0bNkyffvb31ZOTo7WrFmje++9t83ji4mJkcvl0l/+8henRHzZmdadT3R0dKvv9khffL/nSomOjlZFRUWr9Z988omkL477cu3nTMd1pnXDhg3TsGHD1NzcrO3bt+s///M/lZmZqdjYWN1///2XZTyDBg1SUFCQ/u///s9ZFx8fr9WrV6upqSngezplZWWSpLi4OElflLGwsDBn/ZeVlZU5t7EDXzdc0QHa0Te+8Q098cQTGj9+vCZNmnTWnMvlUqdOnQI+pjlx4oT+67/+q1W2oqJCDz30kJKTk7VlyxZNmDBBU6ZM0Z49e9o8vrS0NBljdOjQIQ0aNKjVcjFXipKTk1VeXq5du3YFrD/TH6g7/WrQ5TJy5Eht2rTJKTanvPXWW+rSpctlu038rrvu0p/+9KeAYtfc3Kzf/e53Z31OcHCwEhMT9fLLL0v64g/0XS75+flqaWlRnz59nHXf+973dPz4ca1ZsyYgu3LlSvl8PiUmJkr64k6t8ePHa+3ataqtrXVy+/fv1+bNm3XPPfdctnEClxNXdIB2lpWVdd5MamqqlixZovT0dE2bNk1Hjx7VCy+80OqKSnNzsx544AG5XC69/fbbCg4O1ptvvqnbbrtNEydOVEFBQcAfijufoUOHatq0afrBD36g7du3684771R4eLgqKipUUFCg+Ph4Pfroo2063szMTL3++usaN26cnnvuOcXGxurtt9/W7t27JUlBQf///7/i4+O1du1aLVu2TAkJCQoKCtKgQYPatL8zmT9/vtatW6e77rpLzzzzjKKiorRq1SqtX79eixcvlsfjueR9SNJPf/pTvfvuuxoxYoSeeeYZdenSRS+//LLq6uoCcsuXL9emTZuUmpqq66+/XidPnnTuyBs1apSTmzJlilauXKl//vOf5/yezrp16/Taa69pwoQJ6tWrlxobG7V9+3a9+OKL6tOnj374wx862XHjxmn06NF69NFHVVNToz59+mj16tXKzc3Vb3/724By/eyzz+r2229XWlqannrqKecPBsbExGjOnDmXZc6Ay669vw0NdCRfvuvqXM5019Xrr79u+vXrZ9xut7nhhhvMokWLzIoVKwLu3nn66adNUFCQ+dOf/hTw3C1btphOnTqZxx577Jz7Pf2uqy/vOzEx0YSHh5uwsDBz4403mn/9138127dvdzLJycnm1ltvvaDXLC8vN6NGjTKdO3c2UVFRZsqUKWblypVGkvnf//1fJ3fs2DFz3333mWuvvda4XC5z6j9Zp+66ev7551vtT6fd+XQ2ZWVlZvz48cbj8ZjQ0FAzcOBA88Ybb5zx9S72ritjvrhbafDgwcbtdhuv12ueeOIJ8+tf/zrg31thYaH53ve+Z3r16mXcbreJjo42ycnJ5t133w14rUmTJgU872z+/ve/m/vuu8/06tXLdO7c2XTu3NncfPPN5oknnjBHjx5tla+trTWzZ882Xq/XhIaGmgEDBpz1zrXt27ebkSNHmi5dupiuXbua7373u+Yf//jHBc0P0B5cxnzpr40BQDuZNm2aVq9eraNHj7bpqhMAnAsfXQG44p577jn5fD7dcMMNOn78uNatW6ff/OY3+ulPf0rJAXBZUXQAXHEhISF6/vnndfDgQTU1Nalv375asmSJHnvssfYeGgDL8NEVAACwFreXAwAAa1F0AACAtSg6AADAWh36y8gtLS365JNPFBERwY/RAQBwlTDGqLa2Vj6fL+CPjJ5Jhy46n3zySatfLwYAAFeHAwcOqEePHufMdOiiExERIemLieratWs7jwYAAFyImpoa9ezZ03kfP5cOXXROfVzVtWtXig4AAFeZC/naCV9GBgAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwVpuKzoIFC+RyuQIWr9frbDfGaMGCBfL5fAoLC9Pw4cO1c+fOgNeor6/XrFmzFBMTo/DwcE2YMEEHDx4MyFRVVSkjI0Mej0cej0cZGRmqrq4OyOzfv1/jx49XeHi4YmJiNHv2bDU0NLTx8AEAgM3afEXn1ltvVUVFhbOUlZU52xYvXqwlS5Zo6dKlKi4ultfr1ejRo1VbW+tkMjMzlZOTo+zsbBUUFOj48eNKS0tTc3Ozk0lPT1dpaalyc3OVm5ur0tJSZWRkONubm5uVmpqquro6FRQUKDs7W2vWrNGcOXMudh4AAICNTBvMnz/fDBw48IzbWlpajNfrNVlZWc66kydPGo/HY5YvX26MMaa6utqEhISY7OxsJ3Po0CETFBRkcnNzjTHG7Nq1y0gyRUVFTqawsNBIMrt37zbGGLNhwwYTFBRkDh065GRWr15t3G638fv9F3w8fr/fSGrTcwAAQPtqy/t3m6/ofPTRR/L5fOrdu7fuv/9+ffzxx5KkPXv2qLKyUikpKU7W7XYrOTlZW7ZskSSVlJSosbExIOPz+RQXF+dkCgsL5fF4lJiY6GQGDx4sj8cTkImLi5PP53MyY8aMUX19vUpKSs469vr6etXU1AQsAADAXm0qOomJiXrrrbf0xz/+Ua+99poqKys1ZMgQHT16VJWVlZKk2NjYgOfExsY62yorKxUaGqrIyMhzZrp169Zq3926dQvInL6fyMhIhYaGOpkzWbRokfO9H4/Hww96AgBguTYVnXHjxunee+9VfHy8Ro0apfXr10uSVq5c6WRO/90JY8x5f4vi9MyZ8heTOd28efPk9/ud5cCBA+ccFwAAuLpd0u3l4eHhio+P10cffeTcfXX6FZUjR444V1+8Xq8aGhpUVVV1zszhw4db7evTTz8NyJy+n6qqKjU2Nra60vNlbrfb+QFPfsgTAAD7XVLRqa+v19///nd1795dvXv3ltfrVV5enrO9oaFB+fn5GjJkiCQpISFBISEhAZmKigqVl5c7maSkJPn9fm3bts3JbN26VX6/PyBTXl6uiooKJ7Nx40a53W4lJCRcyiEBAACbtOVbznPmzDHvv/+++fjjj01RUZFJS0szERERZu/evcYYY7KysozH4zFr1641ZWVl5oEHHjDdu3c3NTU1zms88sgjpkePHua9994zH3zwgRkxYoQZOHCgaWpqcjJjx441AwYMMIWFhaawsNDEx8ebtLQ0Z3tTU5OJi4szI0eONB988IF57733TI8ePczMmTPbcjjcdQUAuKr1+vG69h5Cu2jL+3entpSigwcP6oEHHtBnn32m6667ToMHD1ZRUZF69eolSXryySd14sQJTZ8+XVVVVUpMTNTGjRsVERHhvMYvf/lLderUSd///vd14sQJjRw5Um+++aaCg4OdzKpVqzR79mzn7qwJEyZo6dKlzvbg4GCtX79e06dP19ChQxUWFqb09HS98MILl1D5AACAbVzGGNPeg2gvNTU18ng88vv9fF8HAHDV+eZT67U3K7W9h3HFteX9m9+6AgAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBal1R0Fi1aJJfLpczMTGedMUYLFiyQz+dTWFiYhg8frp07dwY8r76+XrNmzVJMTIzCw8M1YcIEHTx4MCBTVVWljIwMeTweeTweZWRkqLq6OiCzf/9+jR8/XuHh4YqJidHs2bPV0NBwKYcEAAAsctFFp7i4WL/+9a81YMCAgPWLFy/WkiVLtHTpUhUXF8vr9Wr06NGqra11MpmZmcrJyVF2drYKCgp0/PhxpaWlqbm52cmkp6ertLRUubm5ys3NVWlpqTIyMpztzc3NSk1NVV1dnQoKCpSdna01a9Zozpw5F3tIAADANuYi1NbWmr59+5q8vDyTnJxsHnvsMWOMMS0tLcbr9ZqsrCwne/LkSePxeMzy5cuNMcZUV1ebkJAQk52d7WQOHTpkgoKCTG5urjHGmF27dhlJpqioyMkUFhYaSWb37t3GGGM2bNhggoKCzKFDh5zM6tWrjdvtNn6//4KOw+/3G0kXnAcA4Ouk14/XtfcQ2kVb3r8v6orOjBkzlJqaqlGjRgWs37NnjyorK5WSkuKsc7vdSk5O1pYtWyRJJSUlamxsDMj4fD7FxcU5mcLCQnk8HiUmJjqZwYMHy+PxBGTi4uLk8/mczJgxY1RfX6+SkpIzjru+vl41NTUBCwAAsFentj4hOztbJSUl2r59e6ttlZWVkqTY2NiA9bGxsdq3b5+TCQ0NVWRkZKvMqedXVlaqW7durV6/W7duAZnT9xMZGanQ0FAnc7pFixbp2WefvZDDBAAAFmjTFZ0DBw7oscce06pVq9S5c+ez5lwuV8BjY0yrdac7PXOm/MVkvmzevHny+/3OcuDAgXOOCQAAXN3aVHRKSkp05MgRJSQkqFOnTurUqZPy8/P1q1/9Sp06dXKusJx+ReXIkSPONq/Xq4aGBlVVVZ0zc/jw4Vb7//TTTwMyp++nqqpKjY2Nra70nOJ2u9W1a9eABQAA2KtNRWfkyJEqKytTaWmpswwaNEgPPvigSktLdcMNN8jr9SovL895TkNDg/Lz8zVkyBBJUkJCgkJCQgIyFRUVKi8vdzJJSUny+/3atm2bk9m6dav8fn9Apry8XBUVFU5m48aNcrvdSkhIuIipAAAAtmnTd3QiIiIUFxcXsC48PFzR0dHO+szMTC1cuFB9+/ZV3759tXDhQnXp0kXp6emSJI/HoylTpmjOnDmKjo5WVFSU5s6dq/j4eOfLzbfccovGjh2rqVOn6tVXX5UkTZs2TWlpaerXr58kKSUlRf3791dGRoaef/55HTt2THPnztXUqVO5UgMAACRdxJeRz+fJJ5/UiRMnNH36dFVVVSkxMVEbN25URESEk/nlL3+pTp066fvf/75OnDihkSNH6s0331RwcLCTWbVqlWbPnu3cnTVhwgQtXbrU2R4cHKz169dr+vTpGjp0qMLCwpSenq4XXnjhch8SAAC4SrmMMaa9B9Feampq5PF45Pf7uQoEALjqfPOp9dqbldrew7ji2vL+zW9dAQAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGCtNhWdZcuWacCAAeratau6du2qpKQk/eEPf3C2G2O0YMEC+Xw+hYWFafjw4dq5c2fAa9TX12vWrFmKiYlReHi4JkyYoIMHDwZkqqqqlJGRIY/HI4/Ho4yMDFVXVwdk9u/fr/Hjxys8PFwxMTGaPXu2Ghoa2nj4AADAZm0qOj169FBWVpa2b9+u7du3a8SIEbr77rudMrN48WItWbJES5cuVXFxsbxer0aPHq3a2lrnNTIzM5WTk6Ps7GwVFBTo+PHjSktLU3Nzs5NJT09XaWmpcnNzlZubq9LSUmVkZDjbm5ublZqaqrq6OhUUFCg7O1tr1qzRnDlzLnU+AACATcwlioyMNL/5zW9MS0uL8Xq9Jisry9l28uRJ4/F4zPLly40xxlRXV5uQkBCTnZ3tZA4dOmSCgoJMbm6uMcaYXbt2GUmmqKjIyRQWFhpJZvfu3cYYYzZs2GCCgoLMoUOHnMzq1auN2+02fr//gsfu9/uNpDY9BwCAr4teP17X3kNoF215/77o7+g0NzcrOztbdXV1SkpK0p49e1RZWamUlBQn43a7lZycrC1btkiSSkpK1NjYGJDx+XyKi4tzMoWFhfJ4PEpMTHQygwcPlsfjCcjExcXJ5/M5mTFjxqi+vl4lJSVnHXN9fb1qamoCFgAAYK82F52ysjJdc801crvdeuSRR5STk6P+/fursrJSkhQbGxuQj42NdbZVVlYqNDRUkZGR58x069at1X67desWkDl9P5GRkQoNDXUyZ7Jo0SLnez8ej0c9e/Zs49EDAICrSZuLTr9+/VRaWqqioiI9+uijmjRpknbt2uVsd7lcAXljTKt1pzs9c6b8xWRON2/ePPn9fmc5cODAOccFAACubm0uOqGhoerTp48GDRqkRYsWaeDAgXrppZfk9XolqdUVlSNHjjhXX7xerxoaGlRVVXXOzOHDh1vt99NPPw3InL6fqqoqNTY2trrS82Vut9u5Y+zUAgAA7HXJf0fHGKP6+nr17t1bXq9XeXl5zraGhgbl5+dryJAhkqSEhASFhIQEZCoqKlReXu5kkpKS5Pf7tW3bNiezdetW+f3+gEx5ebkqKiqczMaNG+V2u5WQkHCphwQAACzRqS3hn/zkJxo3bpx69uyp2tpaZWdn6/3331dubq5cLpcyMzO1cOFC9e3bV3379tXChQvVpUsXpaenS5I8Ho+mTJmiOXPmKDo6WlFRUZo7d67i4+M1atQoSdItt9yisWPHaurUqXr11VclSdOmTVNaWpr69esnSUpJSVH//v2VkZGh559/XseOHdPcuXM1depUrtIAAABHm4rO4cOHlZGRoYqKCnk8Hg0YMEC5ubkaPXq0JOnJJ5/UiRMnNH36dFVVVSkxMVEbN25URESE8xq//OUv1alTJ33/+9/XiRMnNHLkSL355psKDg52MqtWrdLs2bOdu7MmTJigpUuXOtuDg4O1fv16TZ8+XUOHDlVYWJjS09P1wgsvXNJkAAAAu7iMMaa9B9Feampq5PF45Pf7uRIEALjqfPOp9dqbldrew7ji2vL+zW9dAQAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALBWm4rOokWLdPvttysiIkLdunXTd7/7XX344YcBGWOMFixYIJ/Pp7CwMA0fPlw7d+4MyNTX12vWrFmKiYlReHi4JkyYoIMHDwZkqqqqlJGRIY/HI4/Ho4yMDFVXVwdk9u/fr/Hjxys8PFwxMTGaPXu2Ghoa2nJIAADAYm0qOvn5+ZoxY4aKioqUl5enpqYmpaSkqK6uzsksXrxYS5Ys0dKlS1VcXCyv16vRo0ertrbWyWRmZionJ0fZ2dkqKCjQ8ePHlZaWpubmZieTnp6u0tJS5ebmKjc3V6WlpcrIyHC2Nzc3KzU1VXV1dSooKFB2drbWrFmjOXPmXMp8AAAAm5hLcOTIESPJ5OfnG2OMaWlpMV6v12RlZTmZkydPGo/HY5YvX26MMaa6utqEhISY7OxsJ3Po0CETFBRkcnNzjTHG7Nq1y0gyRUVFTqawsNBIMrt37zbGGLNhwwYTFBRkDh065GRWr15t3G638fv9FzR+v99vJF1wHgCAr5NeP17X3kNoF215/76k7+j4/X5JUlRUlCRpz549qqysVEpKipNxu91KTk7Wli1bJEklJSVqbGwMyPh8PsXFxTmZwsJCeTweJSYmOpnBgwfL4/EEZOLi4uTz+ZzMmDFjVF9fr5KSkjOOt76+XjU1NQELAACw10UXHWOMHn/8cd1xxx2Ki4uTJFVWVkqSYmNjA7KxsbHOtsrKSoWGhioyMvKcmW7durXaZ7du3QIyp+8nMjJSoaGhTuZ0ixYtcr7z4/F41LNnz7YeNgAAuIpcdNGZOXOm/va3v2n16tWttrlcroDHxphW6053euZM+YvJfNm8efPk9/ud5cCBA+ccEwAAuLpdVNGZNWuW3n33XW3evFk9evRw1nu9XklqdUXlyJEjztUXr9erhoYGVVVVnTNz+PDhVvv99NNPAzKn76eqqkqNjY2trvSc4na71bVr14AFAADYq01FxxijmTNnau3atdq0aZN69+4dsL13797yer3Ky8tz1jU0NCg/P19DhgyRJCUkJCgkJCQgU1FRofLycieTlJQkv9+vbdu2OZmtW7fK7/cHZMrLy1VRUeFkNm7cKLfbrYSEhLYcFgAAsFSntoRnzJiht99+W//zP/+jiIgI54qKx+NRWFiYXC6XMjMztXDhQvXt21d9+/bVwoUL1aVLF6WnpzvZKVOmaM6cOYqOjlZUVJTmzp2r+Ph4jRo1SpJ0yy23aOzYsZo6dapeffVVSdK0adOUlpamfv36SZJSUlLUv39/ZWRk6Pnnn9exY8c0d+5cTZ06lSs1AADgC225nUvSGZc33njDybS0tJj58+cbr9dr3G63ufPOO01ZWVnA65w4ccLMnDnTREVFmbCwMJOWlmb2798fkDl69Kh58MEHTUREhImIiDAPPvigqaqqCsjs27fPpKammrCwMBMVFWVmzpxpTp48ecHHw+3lAICrGbeXn//922WMMe1Xs9pXTU2NPB6P/H4/V4EAAFedbz61XnuzUtt7GFdcW96/+a0rAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKzV5qLz5z//WePHj5fP55PL5dI777wTsN0YowULFsjn8yksLEzDhw/Xzp07AzL19fWaNWuWYmJiFB4ergkTJujgwYMBmaqqKmVkZMjj8cjj8SgjI0PV1dUBmf3792v8+PEKDw9XTEyMZs+erYaGhrYeEgAAsFSbi05dXZ0GDhyopUuXnnH74sWLtWTJEi1dulTFxcXyer0aPXq0amtrnUxmZqZycnKUnZ2tgoICHT9+XGlpaWpubnYy6enpKi0tVW5urnJzc1VaWqqMjAxne3Nzs1JTU1VXV6eCggJlZ2drzZo1mjNnTlsPCQAA2MpcAkkmJyfHedzS0mK8Xq/Jyspy1p08edJ4PB6zfPlyY4wx1dXVJiQkxGRnZzuZQ4cOmaCgIJObm2uMMWbXrl1GkikqKnIyhYWFRpLZvXu3McaYDRs2mKCgIHPo0CEns3r1auN2u43f77+g8fv9fiPpgvMAAHyd9PrxuvYeQrtoy/v3Zf2Ozp49e1RZWamUlBRnndvtVnJysrZs2SJJKikpUWNjY0DG5/MpLi7OyRQWFsrj8SgxMdHJDB48WB6PJyATFxcnn8/nZMaMGaP6+nqVlJSccXz19fWqqakJWAAAgL0ua9GprKyUJMXGxgasj42NdbZVVlYqNDRUkZGR58x069at1et369YtIHP6fiIjIxUaGupkTrdo0SLnOz8ej0c9e/a8iKMEAABXi6/kriuXyxXw2BjTat3pTs+cKX8xmS+bN2+e/H6/sxw4cOCcYwIAAFe3y1p0vF6vJLW6onLkyBHn6ovX61VDQ4OqqqrOmTl8+HCr1//0008DMqfvp6qqSo2Nja2u9JzidrvVtWvXgAUAANjrshad3r17y+v1Ki8vz1nX0NCg/Px8DRkyRJKUkJCgkJCQgExFRYXKy8udTFJSkvx+v7Zt2+Zktm7dKr/fH5ApLy9XRUWFk9m4caPcbrcSEhIu52EBAICrVKe2PuH48eP6xz/+4Tzes2ePSktLFRUVpeuvv16ZmZlauHCh+vbtq759+2rhwoXq0qWL0tPTJUkej0dTpkzRnDlzFB0draioKM2dO1fx8fEaNWqUJOmWW27R2LFjNXXqVL366quSpGnTpiktLU39+vWTJKWkpKh///7KyMjQ888/r2PHjmnu3LmaOnUqV2oAAMAX2npL1+bNm42kVsukSZOMMV/cYj5//nzj9XqN2+02d955pykrKwt4jRMnTpiZM2eaqKgoExYWZtLS0sz+/fsDMkePHjUPPvigiYiIMBEREebBBx80VVVVAZl9+/aZ1NRUExYWZqKioszMmTPNyZMnL/hYuL0cAHA14/by879/u4wxph17VruqqamRx+OR3+/nKhAA4KrzzafWa29WansP44pry/s3v3UFAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa1F0AACAtSg6AADAWhQdAABgLYoOAACwFkUHAABYi6IDAACsRdEBAADWougAAABrUXQAAIC1KDoAAMBaFB0AAGAtig4AALAWRQcAAFiLogMAAKxF0QEAANai6AAAAGtRdAAAgLWu+qLzyiuvqHfv3urcubMSEhL0l7/8pb2HBAAAviau6qLzu9/9TpmZmXr66ae1Y8cODRs2TOPGjdP+/fvbe2gAAOBr4KouOkuWLNGUKVP0wx/+ULfccotefPFF9ezZU8uWLWvvoQEAgK+BTu09gIvV0NCgkpISPfXUUwHrU1JStGXLljM+p76+XvX19c5jv98vSaqpqfnqBgoAwFekpf7zDvkeduqYjTHnzV61Reezzz5Tc3OzYmNjA9bHxsaqsrLyjM9ZtGiRnn322Vbre/bs+ZWMEQCAr5rnxfYeQfupra2Vx+M5Z+aqLTqnuFyugMfGmFbrTpk3b54ef/xx53FLS4uOHTum6Ojosz7nYtXU1Khnz546cOCAunbtellf+2rEfARiPlpjTgIxH4GYj9Y68pwYY1RbWyufz3fe7FVbdGJiYhQcHNzq6s2RI0daXeU5xe12y+12B6y79tprv6ohSpK6du3a4U7Ac2E+AjEfrTEngZiPQMxHax11Ts53JeeUq/bLyKGhoUpISFBeXl7A+ry8PA0ZMqSdRgUAAL5OrtorOpL0+OOPKyMjQ4MGDVJSUpJ+/etfa//+/XrkkUfae2gAAOBr4KouOhMnTtTRo0f13HPPqaKiQnFxcdqwYYN69erV3kOT2+3W/PnzW31U1lExH4GYj9aYk0DMRyDmozXm5MK4zIXcmwUAAHAVumq/owMAAHA+FB0AAGAtig4AALAWRQcAAFiLogMAAKxF0fkKvPLKK+rdu7c6d+6shIQE/eUvf2nvIV0RCxYskMvlCli8Xq+z3RijBQsWyOfzKSwsTMOHD9fOnTvbccSX35///GeNHz9ePp9PLpdL77zzTsD2C5mD+vp6zZo1SzExMQoPD9eECRN08ODBK3gUl8/55mPy5MmtzpnBgwcHZGyaj0WLFun2229XRESEunXrpu9+97v68MMPAzId6Ry5kPnoSOfIsmXLNGDAAOcvHSclJekPf/iDs70jnRuXE0XnMvvd736nzMxMPf3009qxY4eGDRumcePGaf/+/e09tCvi1ltvVUVFhbOUlZU52xYvXqwlS5Zo6dKlKi4ultfr1ejRo1VbW9uOI7686urqNHDgQC1duvSM2y9kDjIzM5WTk6Ps7GwVFBTo+PHjSktLU3Nz85U6jMvmfPMhSWPHjg04ZzZs2BCw3ab5yM/P14wZM1RUVKS8vDw1NTUpJSVFdXV1TqYjnSMXMh9SxzlHevTooaysLG3fvl3bt2/XiBEjdPfddztlpiOdG5eVwWX17W9/2zzyyCMB626++Wbz1FNPtdOIrpz58+ebgQMHnnFbS0uL8Xq9Jisry1l38uRJ4/F4zPLly6/QCK8sSSYnJ8d5fCFzUF1dbUJCQkx2draTOXTokAkKCjK5ublXbOxfhdPnwxhjJk2aZO6+++6zPsfm+TDGmCNHjhhJJj8/3xjDOXL6fBjDORIZGWl+85vfdPhz41JwRecyamhoUElJiVJSUgLWp6SkaMuWLe00qivro48+ks/nU+/evXX//ffr448/liTt2bNHlZWVAXPjdruVnJzcYebmQuagpKREjY2NARmfz6e4uDhr5+n9999Xt27ddNNNN2nq1Kk6cuSIs832+fD7/ZKkqKgoSZwjp8/HKR3xHGlublZ2drbq6uqUlJTU4c+NS0HRuYw+++wzNTc3t/r19NjY2Fa/sm6jxMREvfXWW/rjH/+o1157TZWVlRoyZIiOHj3qHH9HnRtJFzQHlZWVCg0NVWRk5FkzNhk3bpxWrVqlTZs26Re/+IWKi4s1YsQI1dfXS7J7Powxevzxx3XHHXcoLi5OUsc+R840H1LHO0fKysp0zTXXyO1265FHHlFOTo769+/foc+NS3VV/9bV15XL5Qp4bIxptc5G48aNc/45Pj5eSUlJuvHGG7Vy5Urny4MddW6+7GLmwNZ5mjhxovPPcXFxGjRokHr16qX169frnnvuOevzbJiPmTNn6m9/+5sKCgpabeuI58jZ5qOjnSP9+vVTaWmpqqurtWbNGk2aNEn5+fnO9o54blwqruhcRjExMQoODm7VnI8cOdKqhXcE4eHhio+P10cffeTcfdWR5+ZC5sDr9aqhoUFVVVVnzdise/fu6tWrlz766CNJ9s7HrFmz9O6772rz5s3q0aOHs76jniNnm48zsf0cCQ0NVZ8+fTRo0CAtWrRIAwcO1EsvvdRhz43LgaJzGYWGhiohIUF5eXkB6/Py8jRkyJB2GlX7qa+v19///nd1795dvXv3ltfrDZibhoYG5efnd5i5uZA5SEhIUEhISECmoqJC5eXlHWKejh49qgMHDqh79+6S7JsPY4xmzpyptWvXatOmTerdu3fA9o52jpxvPs7E9nPkdMYY1dfXd7hz47Jqhy9AWy07O9uEhISYFStWmF27dpnMzEwTHh5u9u7d295D+8rNmTPHvP/+++bjjz82RUVFJi0tzURERDjHnpWVZTwej1m7dq0pKyszDzzwgOnevbupqalp55FfPrW1tWbHjh1mx44dRpJZsmSJ2bFjh9m3b58x5sLm4JFHHjE9evQw7733nvnggw/MiBEjzMCBA01TU1N7HdZFO9d81NbWmjlz5pgtW7aYPXv2mM2bN5ukpCTzjW98w9r5ePTRR43H4zHvv/++qaiocJbPP//cyXSkc+R889HRzpF58+aZP//5z2bPnj3mb3/7m/nJT35igoKCzMaNG40xHevcuJwoOl+Bl19+2fTq1cuEhoaab33rWwG3Stps4sSJpnv37iYkJMT4fD5zzz33mJ07dzrbW1pazPz5843X6zVut9vceeedpqysrB1HfPlt3rzZSGq1TJo0yRhzYXNw4sQJM3PmTBMVFWXCwsJMWlqa2b9/fzsczaU713x8/vnnJiUlxVx33XUmJCTEXH/99WbSpEmtjtWm+TjTXEgyb7zxhpPpSOfI+eajo50jDz/8sPPecd1115mRI0c6JceYjnVuXE4uY4y5ctePAAAArhy+owMAAKxF0QEAANai6AAAAGtRdAAAgLUoOgAAwFoUHQAAYC2KDgAAsBZFBwAAWIuiAwAArEXRAQAA1qLoAAAAa/0/6l18HHDDGFQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#================= build_id_to_ids.ipynb\n",
    "\n",
    "QUALITY = 75\n",
    "\n",
    "targetLength = 300          # A target game will be explained by maximum 'targetLength' count of past games.\n",
    "minCurrent = 1e-7\n",
    "sinceDaysAgo = 365 * 20     # A target game will be explained by past games that took place since 'sinceDaysAgo' days ago. \n",
    "qualityPct = QUALITY        #\n",
    "conductance365 = 0.9        # 0.9 comes from test.\n",
    "chooseDivs=False\n",
    "\n",
    "country = countryDirPath.split('/')[-1]\n",
    "filename = country + '-' + str(targetLength) + '-' + str(minCurrent) + '-' + str(sinceDaysAgo) + '-' + str(qualityPct) + '-' + str(conductance365) + '-' + str(chooseDivs) + '.json'\n",
    "filepath = os.path.join(os.getcwd(), 'data', 'id_to_ids', filename)\n",
    "\n",
    "# df = df.loc[df[\"id\"] < 1000007]\n",
    "id_to_ids = data_helpers.LoadJsonData(filepath)\n",
    "\n",
    "if (id_to_ids == None):\n",
    "    id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, df, chooseDivs=chooseDivs)\n",
    "    data_helpers.SaveJsonData(id_to_ids, filepath)\n",
    "    # id_to_ids = data_helpers.LoadJsonData(filepath)\n",
    "\n",
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparams:    \n",
    "    nDivisions = 4 + 1  # E0, E1, E2, E3, and Unknown\n",
    "    division_embs = 4\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 10  # 0 for 0 goals not for Unknown.\n",
    "    goal_embs = 4\n",
    "    nResults = 4    # HWin, Draw, AWin, and Unknown\n",
    "    result_embs = 4\n",
    "    # Mate d_model an even number!!!\n",
    "    d_model = get_std_size()    + division_embs * len(Div_cols) + team_embs * len(Team_cols) \\\n",
    "                                + goal_embs * len(Goal_cols) + result_embs * len(Result_cols)\n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "    # d_model = team_emb_size * 2 + country_emb_size * 3 + odds_size + outcome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'id_to_ids', id_to_ids_filename + '.json')\n",
    "id_to_ids = data_helpers.LoadJsonData(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000000, 'E0', datetime.date(2003, 1, 11), 'Everton', 'Chelsea', 4.5, 3.4, 1.72, 4.2, 3.25, 1.72, 0.0, 0.0, 0, 1, 'D', 'A', 12.0, 17.0, 4.0, 7.0, 3.0, 1.0, 5.0, 11.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "# Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "_Cols_List_to_Embedd = [Div_cols, Team_cols, Goal_cols, Result_cols]\n",
    "_Cols_List_to_Standardize = [Odds_cols, Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_for_Label = [Full_Goal_cols, Odds_cols]\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "base_bbab = list(df.loc[df['id'] == 1000000, BBAB_cols].iloc[0, :])\n",
    "print(base_bbab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B365H': (3.3260954323823335, 1.8186724826678475, 46.0), 'HS': (11.595230899419729, 4.844247410877609, 43.0), 'HST': (4.883200354609929, 2.824192285578062, 24.0), 'HC': (5.343195921985815, 2.8733602402788536, 24.0), 'HF': (11.411075515796261, 3.753049296959251, 77.0), 'HY': (1.5786992263056092, 1.2479238143212679, 11.0), 'HR': (0.07773210831721471, 0.2809324906010517, 3.0)}\n"
     ]
    }
   ],
   "source": [
    "std_path = os.path.join('./data', 'datasets', id_to_ids_filename + \".json\")\n",
    "std_params = get_standardization_params(df)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 300, baseId: 1000300\r"
     ]
    }
   ],
   "source": [
    "ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_' + str(DATA_BITS))\n",
    "\n",
    "if os.path.exists(ds_path):\n",
    "    ds = tf.data.Dataset.load(ds_path)\n",
    "else:\n",
    "    ds = generate_dataset_uk(df, id_to_ids, tokenizer_team, std_params)\n",
    "    tf.data.Dataset.save(ds, ds_path)\n",
    "    # ds = tf.data.Dataset.load(ds_path)\n",
    "\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(ds)\n",
    "\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "take_size = total_size - starting_size - ending_size\n",
    "remaining_ds = ds.skip(starting_size)\n",
    "dataset = remaining_ds.take(take_size)\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size - valid_size\n",
    "\n",
    "_train_ds = dataset.take(train_size)                # [: train_size]\n",
    "remaining_ds = dataset.skip(train_size - valid_size)     # [train_size - valid_size: ]\n",
    "\n",
    "_back_ds = remaining_ds.take(valid_size)            # [train_size - valid_size, train_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)        # [train_size: ]\n",
    "\n",
    "_valid_ds = remaining_ds.take(valid_size)            # [train_size, train_size + valid_size]\n",
    "_test_ds = remaining_ds.skip(valid_size)             # [train_size + valid_size :]\n",
    "\n",
    "assert len(_test_ds) == test_size\n",
    "\n",
    "assert dataset_size == len(_train_ds) + len(_valid_ds) + len(_test_ds)\n",
    "\n",
    "print(total_size, len(dataset), len(_train_ds), len(_back_ds), len(_valid_ds), len(_test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bbas_tensor = get_dummy_bbas_tensor_uk(df, tokenizer_team, std_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_row(baseId, sequence, base_bb, base_label):\n",
    "    try:\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            block = tf.stack([dummy_bbas_tensor] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, block], axis=0) \n",
    "        # print(\"sequence 1\", sequence.shape)\n",
    "        # sequence[:, 2] = base[2] - sequence[:, 2]   # get delta days.\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, nFeatures)\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]\n",
    "        # print('normalize', sequence.shape, base.shape, mask.shape, mask)\n",
    "        # seq_len_org = tf.Variable(seq_len_org, dtype=tf.int32)\n",
    "        return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "    except:\n",
    "        print('normalize_row exception')\n",
    "        print('norm 1', sequence.shape, base_bb.shape, base_label.shape, mask.shape, nMissings)\n",
    "        print('norm 2', baseId, sequence, base_label, mask, nMissings)\n",
    "        # return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "\n",
    "def prepare_batch(baseId, sequence, base_bb, base_label, mask, seq_len_org):\n",
    "    # target = tf.one_hot(tf.squeeze(tf.cast(base_bbab[:, :, -1], dtype=tf.int32), axis=-1), hyperparams.target_onehot_size)\n",
    "    return (baseId, sequence, base_bb, mask), (base_label, seq_len_org)     # (X, Y)\n",
    "\n",
    "def normalize_dataset(ds):\n",
    "    return (\n",
    "        ds.map(lambda baseId, sequence, base_bb, base_label: tf.py_function(\n",
    "            func=normalize_row,\n",
    "            inp=[baseId, sequence, base_bb, base_label],\n",
    "            Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "        )\n",
    "\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def make_train_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(train_batch_size)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def make_test_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename +  '_' + str(DATA_BITS) + '_' \\\n",
    "        + str(starting_size) + '_' + str(len(_train_ds)) )\n",
    "\n",
    "if os.path.exists(train_ds_path):\n",
    "    train_ds = tf.data.Dataset.load(train_ds_path)\n",
    "else:\n",
    "    train_ds = normalize_dataset(_train_ds)\n",
    "    tf.data.Dataset.save(train_ds, train_ds_path)\n",
    "\n",
    "print(len(train_ds))\n",
    "\n",
    "train_batches = make_train_batches(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 3\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, mask), (base_label, seq_len_org) = z\n",
    "    cnt -= 1\n",
    "    if cnt == 0: break\n",
    "print(baseId.shape, sequence.shape, base_bb.shape, mask.shape, base_label.shape, seq_len_org.shape)\n",
    "sample_x = (sequence, base_bb, mask)\n",
    "sample_y = (base_label, seq_len_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename +  '_' + str(DATA_BITS) + '_' \\\n",
    "        + str(starting_size) + '_' + str(len(train_ds)) + '_' + str(len(_back_ds)) )\n",
    "if os.path.exists(back_ds_path):\n",
    "    back_ds = tf.data.Dataset.load(back_ds_path)\n",
    "else:\n",
    "    back_ds = normalize_dataset(_back_ds)\n",
    "    tf.data.Dataset.save(back_ds, back_ds_path)\n",
    "\n",
    "print(len(back_ds))\n",
    "\n",
    "back_batches = make_test_batches(back_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename +  '_' + str(DATA_BITS) + '_' \\\n",
    "        + str(starting_size) + '_' + str(len(train_ds)) + '_' + str(len(back_ds)) + '_' + str(len(_valid_ds)) )\n",
    "if os.path.exists(valid_ds_path):\n",
    "    valid_ds = tf.data.Dataset.load(valid_ds_path)\n",
    "else:\n",
    "    valid_ds = normalize_dataset(_valid_ds)\n",
    "    tf.data.Dataset.save(valid_ds, valid_ds_path)\n",
    "\n",
    "print(len(valid_ds))\n",
    "\n",
    "valid_batches = make_test_batches(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename +  '_' + str(DATA_BITS) + '_' \\\n",
    "        + str(starting_size) + '_' + str(len(train_ds)) + '_' + str(len(back_ds)) + '_' + str(len(valid_ds)) + '_' + str(len(_test_ds)))\n",
    "if os.path.exists(test_ds_path):\n",
    "    test_ds = tf.data.Dataset.load(test_ds_path)\n",
    "else:\n",
    "    test_ds = normalize_dataset(_test_ds)\n",
    "    tf.data.Dataset.save(test_ds, test_ds_path)\n",
    "\n",
    "print(len(test_ds))\n",
    "\n",
    "test_batches = make_test_batches(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(memory, depth):\n",
    "    positions = tf.ones(memory.shape[-1], dtype=tf.float32) * memory.shape[-1]      # NPOS, New Position Encoding\n",
    "\n",
    "    fractional_pos = memory * positions    # fractional position: (batch, fractional position #)\n",
    "    # fractional_pos = memory\n",
    "    \n",
    "    depth = depth/2\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth\n",
    "    supper_fractional_pos = 1.0 * positions.shape[-1]\n",
    "    target_supper_angle_rads = 0.8 # rads\n",
    "    depths = tf.pow(supper_fractional_pos / target_supper_angle_rads, depths)    # (depth,)\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] / depths  # (batch, fractional position #, depth)\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = tf.ones((100, 200), dtype=tf.float32) * tf.exp(-tf.range(200, dtype=tf.float32)/100)\n",
    "pos_encoding = positional_encoding(memory, depth=512)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "# Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'WHH', 'WHD', 'WHA']    # Two bookies, BW and IW, were disappeared in fall 2024\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, hyperparams, isEncoder=True):\n",
    "    super().__init__()\n",
    "    self.isEncoder = isEncoder\n",
    "    self.division_embedding = tf.keras.layers.Embedding(hyperparams.nDivisions, hyperparams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.team_embedding = tf.keras.layers.Embedding(hyperparams.nTeams, hyperparams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.goal_embedding = tf.keras.layers.Embedding(hyperparams.nGoals, hyperparams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn 0-goal\n",
    "    self.result_embedding = tf.keras.layers.Embedding(hyperparams.nResults, hyperparams.result_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "    self.d_model = hyperparams.d_model\n",
    "    # print(self.d_model)\n",
    "    self.position_permuting_dense = tf.keras.layers.Dense(self.d_model)\n",
    "    self.m365_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "\n",
    "    self.idx_Days = BB_cols.index('Date')\n",
    "    assert self.idx_Days == BBAB_cols.index('Date')\n",
    "\n",
    "  def call(self, x):\n",
    "    (sequence, base_bb, mask) = x # sob = sequence or base_bb\n",
    "    sDays = sequence[:, :, self.idx_Days]\n",
    "    bDays = base_bb[:, :, self.idx_Days]\n",
    "    \n",
    "    # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "    # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "    sob = None\n",
    "    if self.isEncoder:\n",
    "      sob = sequence\n",
    "    else:\n",
    "      sob = base_bb\n",
    "\n",
    "    if self.isEncoder:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, goals, results, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Goal_cols), len(Result_cols),  -1], axis=-1)\n",
    "      # print('1', remainder[0, 0])\n",
    "    else:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "      # print('2', remainder[0, 0])  \n",
    "\n",
    "    # print('pe 2.7.1 1', div)\n",
    "    div = self.division_embedding(tf.cast(div, dtype=tf.int32))\n",
    "    div = tf.reshape(div, [div.shape[0], div.shape[1], -1])\n",
    "    # print('pe 2.7.1 1', div)\n",
    "    teams = self.team_embedding(tf.cast(teams, dtype=tf.int32))\n",
    "    teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1])\n",
    "    if self.isEncoder:\n",
    "      goals = self.goal_embedding(tf.cast(goals, dtype=tf.int32))\n",
    "      goals = tf.clip_by_value(goals, 0, hyperparams.nGoals)\n",
    "      goals = tf.reshape(goals, [goals.shape[0], goals.shape[1], -1])\n",
    "      results = self.result_embedding(tf.cast(results, dtype=tf.int32))\n",
    "      results = tf.reshape(results, [results.shape[0], results.shape[1], -1])\n",
    "\n",
    "    # prob = 1.0 / odds   # U.HO.05\n",
    "    \n",
    "    if self.isEncoder:\n",
    "      concat = [div, teams, goals, results, odds, remainder]\n",
    "    else:\n",
    "      concat = [div, teams, odds, remainder]\n",
    "\n",
    "    sob = tf.concat(concat, axis=-1)\n",
    "    sob = self.position_permuting_dense(sob)\n",
    "\n",
    "    days_ago = tf.cast(bDays - sDays, dtype=tf.float32) if self.isEncoder else tf.cast(bDays - bDays, dtype=tf.float32)\n",
    "    \n",
    "    m365 = self.m365_embedding(tf.zeros_like((hyperparams.m365_size,), dtype=tf.float32)) * hyperparams.initial_m365  # expected shape: (1, hyperparams.remain_365_size)\n",
    "    m365 = tf.squeeze(m365, axis=0)\n",
    "    memory_alpha = tf.math.pow(m365, 1.0/365) # (hyperparams.m365_size,)\n",
    "    memory = tf.math.pow(memory_alpha, tf.cast(days_ago[:, :, tf.newaxis], tf.float32))  # decrease as days_ago increase, if memory <= 1.0 as expected.\n",
    "    memory = tf.reduce_mean(memory, axis=-1)\n",
    "\n",
    "    pe = positional_encoding(memory, depth=sob.shape[-1]) # (batch, d_model)\n",
    "    pe = pe / tf.math.sqrt(tf.cast(sob.shape[-1], tf.float32))\n",
    "    sob = sob + pe\n",
    "\n",
    "    if self.isEncoder:\n",
    "      mask = mask\n",
    "    else:\n",
    "      mask = mask[:, 0:sob.shape[1], :]\n",
    "\n",
    "    return (sob, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "eSob, eMask = PE(sample_x)\n",
    "print(eSob.shape, eMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "dSob, dMask = PE(sample_x)\n",
    "print(dSob.shape, dMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axix is normalized across. No inter-token operatoin.\n",
    "      self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(\n",
    "          query=x,\n",
    "          key=context,\n",
    "          value=context,\n",
    "          attention_mask=mask,\n",
    "          return_attention_scores=True)\n",
    "    \n",
    "      # Cache the attention scores for plotting later.\n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          attention_mask=mask)    # intentional inter-token operation\n",
    "      x = self.add([x, attn_output])  # token-wise\n",
    "      x = self.layernorm(x)         # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          use_causal_mask = True)     # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])  # mask-agnostic\n",
    "      x = self.layernorm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),    # across -1 axis\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])  # mask-agnostic\n",
    "      x = self.layer_norm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hyperparams.d_model,\n",
    "                      num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_embedding(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.dropout(x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "      return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "\n",
    "      self.causal_self_attention = CausalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "      \n",
    "      self.cross_attention = CrossAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "\n",
    "      # Cache the last attention scores for plotting later\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hyperparams.d_model, num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_embedding(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens)     \n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.decoder = Decoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(hyperparams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, inputs):\n",
    "      # inputs = (sequence, base_bb, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), mask: (batch, max_token, max_token)\n",
    "      x = self.encoder(inputs)  # (batch, max_tokens, d_model)\n",
    "      x = self.decoder(inputs, x)  # (batch, 1, d_model)\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_transformer = Transformer(hyperparams)\n",
    "y = sample_transformer(sample_x)\n",
    "\n",
    "sample_transformer.summary()\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hyperparams.d_model * 2] * nLayers\n",
    "    dims = dims + [hyperparams.d_model * 2 + round( (d_output - hyperparams.d_model * 2) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, kernel_initializer=tf.keras.initializers.LecunUniform(), activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x\n",
    "\n",
    "class BaseAdaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hyperparams.d_model] * nLayers + [hyperparams.d_model + round( (d_output - hyperparams.d_model) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x\n",
    "\n",
    "class OhAdaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [d_output] * nLayers\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BettingEPL(tf.keras.Model):\n",
    "  softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "  def __init__(self, hyperparams, nQueries, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.nQueries = nQueries\n",
    "      self.transformer = Transformer(hyperparams, dropout_rate=dropout_rate)\n",
    "      #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "      self.bookies = ['B365', 'William'] # Two bookies, Betfair and Interwetten, were disappeared in fall 2024.\n",
    "      self.baseAdaptors = [Adaptor(ADAPTORS_LAYERS, self.nQueries) for _ in self.bookies]\n",
    "      return\n",
    "\n",
    "  def call(self, input):\n",
    "      x = self.transformer(input)\n",
    "      stake_p = [adaptor(x) for adaptor in self.baseAdaptors]  # [(batch, nQueries)] * nBookies\n",
    "      stake_p = tf.stack(stake_p, axis=0)   # (nBookies, batch, nQueries)\n",
    "      stake_p = BettingEPL.softmax(stake_p)  # (nBookies, batch, nQueries)\n",
    "      outputs = (stake_p) # (nBookies, batch, nQueries)\n",
    "      return outputs  # happen_p: (nBookies, batch, nQueries), stake_p: (nBookies, batch, nQueries)\n",
    "  \n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "      # ftGoals:  (batch, 2)\n",
    "      ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "      h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "      h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "      return h\n",
    "\n",
    "  def loss(self, y, outputs):   \n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: stake_p: (nBookies, batch, nQueries)\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "      odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "      odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "      happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "      oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "      (stake_p) = outputs   # (nBookies, batch, nQueries), (nBookies, batch, nQueries)\n",
    "     \n",
    "      # -----------------------------------------------------------------------------------------\n",
    "      # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "      #------------------------------------------------------------------------------------------\n",
    "      \n",
    "      profit_backtest = tf.reduce_mean(tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1), axis=None)  # () \n",
    "      loss = - profit_backtest  # U.action.42\n",
    "  \n",
    "      return loss # (), negative average profit on a game on a bookie\n",
    "  \n",
    "  def back_test(self, y, outputs, profit_keys):\n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: stake_p: (nBookies, batch, nQueries)\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "      odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "      odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "      happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "      oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "      (stake_p) = outputs   # (nBookies, batch, nQueries), (nBookies, batch, nQueries)\n",
    "\n",
    "      # -----------------------------------------------------------------------------------------\n",
    "      # Note: oh_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "      #------------------------------------------------------------------------------------------\n",
    "      profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "      profit_backtest = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "      profit_list = []; cast_list = []\n",
    "      profit = MIN_PROFIT\n",
    "      for profit_key in profit_keys:\n",
    "        best_profit_backtest = profit_backtest[profit_p >= profit_key]\n",
    "        cast = best_profit_backtest.shape[-1]\n",
    "        if cast > 0: profit = tf.math.reduce_mean(best_profit_backtest)\n",
    "        profit_list.append(float(profit))\n",
    "        cast_list.append(cast)\n",
    "      return profit_list, cast_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPL = BettingEPL(hyperparams, 3, dropout_rate=DROPOUT)\n",
    "\n",
    "happen_prob = EPL(sample_x, training=True)\n",
    "loss = EPL.loss(sample_y[0], happen_prob)\n",
    "\n",
    "profits, casts = EPL.back_test(sample_y[0], happen_prob, PROFIT_KEYS)\n",
    "# print(profit_list)\n",
    "# print(nBettings_list)\n",
    "\n",
    "EPL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(EPL, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = CustomSchedule(hyperparams.d_model)\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.95, beta_2=0.95, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss_uk(label, y_pred):\n",
    "  # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch, 1)), y_pred: (batch, 3)\n",
    "  y_true = label[0]   # one_hot: (batch, 3)\n",
    "  seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "  mask = y_true != 0 \n",
    "  loss = loss_object(y_true, y_pred)\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask) # eq. sum_loss / batch\n",
    "  return loss\n",
    "\n",
    "\n",
    "class recall():\n",
    "  def __init__(self, name='recall', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    # print('recall', y_true.shape, y_pred.shape, seq_len_mask.shape)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    # print('recall', true_positives.numpy())\n",
    "    possible_positives = tf.math.reduce_sum(y_true)\n",
    "    recall_keras = true_positives / (possible_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall_keras.numpy() / self.n\n",
    "\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.math.reduce_sum(y_pred)\n",
    "    precision_keras = true_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision_keras.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = EPL(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_value = EPL.loss(y, outputs)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, EPL.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, EPL.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "@tf.function\n",
    "def test_step(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value = EPL.loss(y, outputs)\n",
    "    # recall_object.update_state(y, val_logits)\n",
    "    # precision_object.update_state(y, val_logits)\n",
    "    return loss_value\n",
    "\n",
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def test_with_dataset(dataset):\n",
    "    n = 0\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        n += 1\n",
    "        val_loss = val_loss * (n-1) / n + test_step(x, y) / n   ###\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def back_test_step(x, y, profit_keys):\n",
    "    outputs = EPL(x, training=False)  #\n",
    "    profits, casts = EPL.back_test(y, outputs, profit_keys)\n",
    "    # print('key', profit_back_mean_per_betting, nBettings)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def back_test_with_dataset(dataset, profit_keys):\n",
    "    profits = [MIN_PROFIT] * len(profit_keys)\n",
    "    casts = [0] * len(profit_keys)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        profit_list, cast_list = back_test_step(x, y, profit_keys)\n",
    "\n",
    "        for p, c, id in zip(profit_list, cast_list, range(len(profit_keys))):\n",
    "            if c > 0:\n",
    "                profits[id] = (profits[id] * casts[id] + p * c) / (casts[id] + c)\n",
    "                casts[id] = casts[id] + c\n",
    "    # print('key', profit_back_mean, nBettingsTotal)\n",
    "    return profits, casts\n",
    "\n",
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'loss': [], 'val_loss': []}\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = data_helpers.LoadJsonData(self.filepath)\n",
    "        if history is not None:\n",
    "            self.history = history\n",
    "    def append(self, loss, val_loss):\n",
    "        self.history['loss'].append(self.round_sig(float(loss), 4))\n",
    "        self.history['val_loss'].append(self.round_sig(float(val_loss), 4))\n",
    "        self.save()\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_latest_item(self):\n",
    "        return (self.history['loss'][-1], self.history['val_loss'][-1])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "\n",
    "    def show(self, ax):\n",
    "        ax.set_title(TEST_ID + \": loss history\")\n",
    "        ax.plot(self.history['loss'])\n",
    "        ax.plot(self.history['val_loss'])\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch')\n",
    "        ax.legend(['train_loss', 'val_loss'], loc='upper left')\n",
    "\n",
    "class test_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            # return x\n",
    "            return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, profit_keys, valid_size, nBookies, filepath):\n",
    "        self.profit_keys = profit_keys\n",
    "        self.nTests = valid_size * nBookies\n",
    "        self.filepath = filepath\n",
    "        self.profits = {str(key): [] for key in self.profit_keys}\n",
    "        self.casts = {str(key): [] for key in self.profit_keys}\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData([self.profits, self.casts], self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        test = data_helpers.LoadJsonData(self.filepath)\n",
    "        if test is not None:\n",
    "            [self.profits, self.casts] = test\n",
    "    def getLen(self, dict):\n",
    "        length = None\n",
    "        try:\n",
    "            for key, value in dict.items():\n",
    "                if length is None:\n",
    "                    length = len(value)\n",
    "                else:\n",
    "                    assert len(value) == length\n",
    "            return length\n",
    "        except:\n",
    "            raise Exception(\"Un-uniform length in  distribution\")      \n",
    "\n",
    "    def len(self):\n",
    "        assert len(self.profits) == len(self.profit_keys)\n",
    "        assert len(self.casts) == len(self.profit_keys)\n",
    "        length = self.getLen(self.profits)\n",
    "        assert self.getLen(self.casts) == length\n",
    "        return length\n",
    "\n",
    "    def append(self, profits, casts):\n",
    "        length = self.len()     # for all the asserts.\n",
    "        assert len(profits) == len(self.profit_keys)\n",
    "        assert len(casts) == len(self.profit_keys)\n",
    "        for item_list, item in zip(self.profits.values(), profits):\n",
    "            item_list.append(item)\n",
    "\n",
    "        assert len(casts) == len(self.profit_keys)\n",
    "        for item_list, item in zip(self.casts.values(), casts):\n",
    "            item_list.append(item)\n",
    "\n",
    "        self.save()\n",
    "\n",
    "    def get_best_product(self, profits, casts):\n",
    "        best_product = -float('inf') # MIN_PROFIT * 1e6\n",
    "        for (p, n) in zip(profits, casts):\n",
    "            if p * n > best_product:\n",
    "                best_product = p * n\n",
    "        return best_product\n",
    "    \n",
    "    def get_existing_best_product(self):\n",
    "        all_profits = []\n",
    "        for item_list in self.profits.values():\n",
    "            all_profits += item_list\n",
    "        all_casts = []\n",
    "        for item_list in self.casts.values():\n",
    "            all_casts += item_list\n",
    "        return self.get_best_product(all_profits, all_casts)\n",
    "    \n",
    "    def find_profit_cast_series(self):\n",
    "        nSeries = len(tuple(self.profits.values())[0])\n",
    "        for v in self.profits.values():\n",
    "            assert len(v) == nSeries\n",
    "        for v in self.casts.values():\n",
    "            assert len(v) == nSeries\n",
    "        \n",
    "        profit_series =  [[v[serial] for v in self.profits.values()] for serial in range(nSeries)] # [ [ profit for _ in profit_keys ] ] * nSeries\n",
    "        cast_series =  [[v[serial] for v in self.casts.values()] for serial in range(nSeries)] # [ [ cast for _ in profit_keys ] ] * nSeries\n",
    "        return profit_series, cast_series\n",
    "       \n",
    "    def find_total_profit_groups(self):       \n",
    "        profit_series, cast_series = self.find_profit_cast_series()\n",
    "        total_profit_groups = []\n",
    "        for profits, casts in zip(profit_series, cast_series):\n",
    "            profit_groups = self.find_profit_groups(profits, casts, sort=False)\n",
    "            total_profit_groups.append(profit_groups)   # [ [ (product, profit, cast, id_string) for nGroups ] ] * nSeries\n",
    "        return total_profit_groups\n",
    "    \n",
    "    def track_profit_groups(self, total_profit_groups):\n",
    "        # total_profit_groups: [ [ (product, profit, cast, id_string) for nGroups ] ] * nSeries\n",
    "        profit_groups_track = None\n",
    "        nSeries = len(total_profit_groups)\n",
    "        if nSeries > 0:\n",
    "            nGroups = len(total_profit_groups[0])\n",
    "            for profit_groups in total_profit_groups:\n",
    "                assert len(profit_groups) == nGroups\n",
    "            profit_groups_track = [[total_profit_groups[series][profit_group] for series in range(nSeries)] for profit_group in range(nGroups)]\n",
    "        return profit_groups_track  # [ [ (product, profit, cast, id) for _ in range(nSeries)] for _ in range(nGroups) ]\n",
    "        # profit_groups_track = { profit_groups_track[group][0][3] : [(profit, cast) for _, profit, cast, _ in profit_groups_track[group]] for group in range(nGroups) }\n",
    "        # return profit_groups_track  # { id : [ (profit, cast) for _ in range(nSeries)] for _ in range(nGroups) }\n",
    "    \n",
    "    def find_profit_groups(self, profits, casts, sort=True):\n",
    "        result = []\n",
    "        for n1 in range(len(self.profit_keys)):\n",
    "            for n2 in range(n1, len(self.profit_keys)): \n",
    "                # n2 >= n1. profit_keys[n2] >= profit_keys[n1], casts[n2] <= casts[n1]\n",
    "                if n1 == n2:\n",
    "                    result.append((profits[n1] * casts[n1], profits[n1], casts[n1], str(self.profit_keys[n1])+\"-\"))\n",
    "                else:\n",
    "                    cast3 = casts[n1] - casts[n2]\n",
    "                    if cast3 > 0:\n",
    "                        profit3 = (profits[n1] * casts[n1] - profits[n2] * casts[n2]) / cast3\n",
    "                    else:\n",
    "                        profit3 = MIN_PROFIT\n",
    "                    result.append( (profit3 * cast3, profit3, cast3, str(self.profit_keys[n1])+\"-\"+str(self.profit_keys[n2])))\n",
    "        if sort: result.sort(reverse=True)\n",
    "        return result\n",
    "    \n",
    "    def find_profit_groups_elements(self, profits, casts, sort=True):\n",
    "        result = []\n",
    "        for n1 in range(len(self.profit_keys)-1):\n",
    "            n2 = n1 + 1\n",
    "            cast3 = casts[n1] - casts[n2]\n",
    "            if cast3 > 0:\n",
    "                profit3 = (profits[n1] * casts[n1] - profits[n2] * casts[n2]) / cast3\n",
    "            else:\n",
    "                profit3 = MIN_PROFIT\n",
    "            result.append( (profit3 * cast3, profit3, cast3, str(self.profit_keys[n1])+\"-\"+str(self.profit_keys[n2])))\n",
    "        if sort: result.sort(reverse=True)\n",
    "        return result\n",
    "    \n",
    "    def print_profit_groups(self, groups, count):\n",
    "        # groups: [ (product, profit, cast, interval) ] * n\n",
    "        for (product, profit, cast, interval) in groups:\n",
    "            print(\"[{:.5f}, {:.4f}, {}, {}]\".format(product, profit, cast, interval), end=', ')\n",
    "            count -= 1\n",
    "            if count <= 0:\n",
    "                print(); break\n",
    "            \n",
    "    # def show_profit_distribution(self):\n",
    "\n",
    "\n",
    "    def show_profit_groups(self, minProduct=0.0):\n",
    "        total_profit_groups = self.find_total_profit_groups()   # [ [ (product, profit, cast, group_id) for nGroups ] ] * nSeries\n",
    "        if len(total_profit_groups) < 1: return\n",
    "\n",
    "        profit_groups_track = self.track_profit_groups(total_profit_groups) # [ [ (product, profit, cast, group_id) for _ in range(nSeries)] for _ in range(nGroups) ]\n",
    "\n",
    "        nGroups = len(total_profit_groups[0])\n",
    "        for profit_groups in total_profit_groups:\n",
    "            assert len(profit_groups) == nGroups\n",
    "        profit_groups_track = { profit_groups_track[group][0][3] : [(profit, cast) for _, profit, cast, _ in profit_groups_track[group]] for group in range(nGroups) }\n",
    "        # { group_id : [ (profit, cast) for _ in range(nSeries)] for _ in range(nGroups) }\n",
    "        \n",
    "        minCasts = self.nTests; maxCasts = 0\n",
    "        minProfit = 50.0; maxProfit = MIN_PROFIT\n",
    "        for key, value in profit_groups_track.items():\n",
    "            casts = [cast for _, cast in value]\n",
    "            profits = [profit for profit, _ in value]\n",
    "            # if profits[-1] * casts[-1] > minProduct:\n",
    "            if key.endswith('-'):\n",
    "                if minCasts > min(casts): minCasts = min(casts)\n",
    "                if maxCasts < max(casts): maxCasts = max(casts)\n",
    "                if minProfit > min(profits): minProfit = min(profits)\n",
    "                if maxProfit < max(profits): maxProfit = max(profits)\n",
    "\n",
    "        step = 5; x = np.arange(minCasts, maxCasts + step, step).reshape(-1, 1)\n",
    "        step = 0.0005; y = np.arange(minProfit, maxProfit + step, step).reshape(-1, 1)\n",
    "        X, Y = np.meshgrid(x, y)    # (n, m)\n",
    "        XY = np.stack((X, Y), axis=-1)  # (n, m, 2)\n",
    "        Z = XY[:, :, 0] * XY[:, :, 1]   # (n, m)\n",
    "\n",
    "        if Z.shape[0] >= 2 and Z.shape[1] >= 2:\n",
    "            sLevels = (0) #, 1, 2, 3, 4, 5) if GUI.loss == 'mean_squared_error' else (0,)\n",
    "            sColors = ['r'] # GUI.colors[: len(sLevels)]\n",
    "            nContours = 80\n",
    "            plt.figure(figsize=(12,8))\n",
    "            CS0 = plt.contourf(X, Y, Z, nContours, cmap=plt.cm.bone, origin='lower')\n",
    "            CS = plt.contour(X, Y, Z, CS0.levels, colors=('k'), origin='lower', linewidths=.2)\n",
    "            plt.contour(X, Y, Z, sLevels, colors=sColors, origin='lower', linewidths=.5)    \n",
    "            plt.clabel(CS, fmt='%1.1f', colors='c', fontsize=8, inline=True)\n",
    "\n",
    "        for key, value in profit_groups_track.items():\n",
    "            casts = [cast for _, cast in value]\n",
    "            profits = [profit for profit, _ in value]\n",
    "            # if profits[-1] * casts[-1] > minProduct:\n",
    "            if key.endswith('-'):\n",
    "                plt.plot(casts, profits, label=key, marker='o', lw=0.5)\n",
    "                plt.plot(casts[-1], profits[-1], marker='o', color='k')\n",
    "                plt.plot(casts[-1], profits[-1], marker='x', color='w')\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def show(self, ax):\n",
    "        colors = ['black', 'firebrick', 'darkgreen', 'c', 'blue', 'blueviolet', 'magenta', 'maroon', \"yellowgreen\", 'cadetblue', 'purple', 'c', 'blue']\n",
    "\n",
    "        gmin = MIN_PROFIT - 1.0; gmax = MIN_PROFIT\n",
    "        all_profits = []\n",
    "        for item_list in self.profits.values():\n",
    "            all_profits += item_list\n",
    "        if len(all_profits) > 0:\n",
    "            gmin = min(all_profits); gmax = max(all_profits)\n",
    "\n",
    "        _min = 0.0; _max = self.nTests        \n",
    "        # _min = 0.0; _max = 1.0\n",
    "        # all_nBettings = []\n",
    "        # for item_list in self.nBettings.values():\n",
    "        #     all_nBettings += item_list\n",
    "        # if len(all_nBettings) > 0:\n",
    "        #     _min = min(all_nBettings); _max = max(all_nBettings)\n",
    "        \n",
    "        legends = []\n",
    "        for item_list, color, key in zip(self.profits.values(), colors[:len(self.profit_keys)], self.profit_keys):\n",
    "            # print(item_list, color, key)\n",
    "            ax.plot(item_list, color=color, linewidth=0.7)\n",
    "            legends.append(\"> \" + str(key))\n",
    "        # print(legends)\n",
    "\n",
    "        for item_list, color in zip(self.casts.values(), colors[:len(self.profit_keys)]):\n",
    "            item_list = [ (item-_min)/(_max-_min+1e-9) * (gmax-gmin) + gmin for item in item_list]\n",
    "            ax.plot(item_list, color=color, linestyle='--', linewidth=0.7)\n",
    "\n",
    "        ax.legend(legends, loc='upper left')\n",
    "        ax.grid(True)\n",
    "        ax.set_title(TEST_ID + \": avg_profit and scaled nBettings per profit threshold key. max: {}\".format(gmax))\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch')\n",
    "\n",
    "checkpointPath = os.path.join('./data', 'checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join('./data', 'checkpoints', TEST_ID + '_weights_best')\n",
    "borrowedBestCheckpointPath = os.path.join('./data', 'checkpoints', BASE_TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join('./data', 'checkpoints', TEST_ID + '_history.json')\n",
    "backtestPath = os.path.join('./data', 'checkpoints', TEST_ID + '_backtest.json')\n",
    "validtestPath = os.path.join('./data', 'checkpoints', TEST_ID + '_validtest.json')\n",
    "testtestPath = os.path.join('./data', 'checkpoints', TEST_ID + '_testtest.json')\n",
    "\n",
    "history = history_class(historyPath)\n",
    "backtest = test_class(PROFIT_KEYS, len(back_ds), len(EPL.bookies), backtestPath)\n",
    "validtest = test_class(PROFIT_KEYS, len(valid_ds), len(EPL.bookies), validtestPath)\n",
    "testtest = test_class(PROFIT_KEYS, len(test_ds), len(EPL.bookies), testtestPath)\n",
    "\n",
    "def removeFile(path):\n",
    "    files = glob.glob(path + \"*\")   # \"*.*\" may not work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    return\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    removeFile(checkpointPath)\n",
    "    removeFile(checkpointPathBest)\n",
    "    history.reset()\n",
    "    backtest.reset()\n",
    "    validtest.reset()\n",
    "    testtest.reset()\n",
    "\n",
    "    if BASE_TEST_ID != '':\n",
    "        try: \n",
    "            EPL.load_weights(borrowedBestCheckpointPath)\n",
    "            print('Model: ' + BASE_TEST_ID + ' loaded.')\n",
    "        except:\n",
    "            print('Failed to load the best model weights.')\n",
    "\n",
    "#---------------------------------------- load \n",
    "try: \n",
    "    EPL.load_weights(checkpointPath)\n",
    "except:\n",
    "    print('Previous weights not loaded.')\n",
    "\n",
    "history.load()\n",
    "backtest.load()\n",
    "validtest.load()\n",
    "testtest.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test):\n",
    "    EPL.save_weights(checkpointPath)\n",
    "    if validtest.get_best_product(profits_valid, casts_valid) > validtest.get_existing_best_product() \\\n",
    "        or val_loss < history.get_min_val_loss():\n",
    "        EPL.save_weights(checkpointPathBest)\n",
    "\n",
    "    history.append(loss, val_loss)\n",
    "    backtest.append(profits_back, casts_back)\n",
    "    validtest.append(profits_valid, casts_valid)\n",
    "    testtest.append(profits_test, casts_test)\n",
    "\n",
    "def show_steps(epoch, step, loss, samples_seen):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hyperparams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {}, samples_seen: {}                  \".\n",
    "            format(epoch, step, float(loss), samples_seen), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def print_test(epoch, train_loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test):\n",
    "    print(\"epoch: {}, loss: {}, val_loss: {}, profit: {}, casts: {}\".format(epoch, float(train_loss), float(val_loss), profits_valid, casts_valid))\n",
    "    pGroups_back = backtest.find_profit_groups(profits_back, casts_back)\n",
    "    backtest.print_profit_groups(pGroups_back, 10)\n",
    "    pGroups_valid = validtest.find_profit_groups(profits_valid, casts_valid)\n",
    "    validtest.print_profit_groups(pGroups_valid, 10)\n",
    "    pGroups_test = testtest.find_profit_groups(profits_test, casts_test)\n",
    "    testtest.print_profit_groups(pGroups_test, 10)\n",
    "\n",
    "    pGroups_back = backtest.find_profit_groups_elements(profits_back, casts_back, sort=False)\n",
    "    backtest.print_profit_groups(pGroups_back, len(backtest.profit_keys)-1)\n",
    "    pGroups_valid = validtest.find_profit_groups_elements(profits_valid, casts_valid, sort=False)\n",
    "    validtest.print_profit_groups(pGroups_valid, len(validtest.profit_keys)-1)\n",
    "    pGroups_test = testtest.find_profit_groups_elements(profits_test, casts_test, sort=False)\n",
    "    testtest.print_profit_groups(pGroups_test, len(testtest.profit_keys)-1)\n",
    "\n",
    "def conclude_train_epoch(epoch, train_loss):\n",
    "    val_loss = test_with_dataset(valid_batches)\n",
    "    profits_back, casts_back = back_test_with_dataset(back_batches, backtest.profit_keys)\n",
    "    profits_valid, casts_valid = back_test_with_dataset(valid_batches, validtest.profit_keys)\n",
    "    profits_test, casts_test = back_test_with_dataset(test_batches, testtest.profit_keys)\n",
    "    save_checkpoint(train_loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)   \n",
    "    print_test(epoch, train_loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    train_loss = test_with_dataset(train_batches)\n",
    "    conclude_train_epoch(0, train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testtest.show_profit_groups(minProduct=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "history.show(axes[0]); backtest.show(axes[1]); validtest.show(axes[2]); testtest.show(axes[3]); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "\n",
    "profit_series_back, cast_series_back = backtest.find_profit_cast_series()\n",
    "profit_series_valid, cast_series_valid = validtest.find_profit_cast_series()\n",
    "profit_series_test, cast_series_test = testtest.find_profit_cast_series()\n",
    "\n",
    "epoch = 0\n",
    "for train_loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test \\\n",
    "    in zip(history.history['loss'], history.history['val_loss'], profit_series_back, cast_series_back, profit_series_valid, cast_series_valid, profit_series_test, cast_series_test):\n",
    "    print_test(epoch, train_loss, val_loss, profits_back, casts_back, profits_valid, casts_valid, profits_test, casts_test)\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "for epoch in range(history.len(), epochs):\n",
    "    start_time = time.time()\n",
    "    history.show(axes[0]); backtest.show(axes[1]); plt.show()\n",
    "    n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "    m = 0; train_loss = 0.0\n",
    "    \n",
    "    train_batches = make_train_batches(train_ds)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(train_batches):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        batch_loss = train_step(x, y)\n",
    "        n += 1; loss = loss * (n-1)/n + batch_loss/n\n",
    "        m += 1; train_loss = train_loss * (m-1)/m + batch_loss/m\n",
    "\n",
    "        samples_seen += sequence.shape[0]\n",
    "        if step % 50 == 0:\n",
    "            show_steps(epoch, step, loss, samples_seen)\n",
    "            n = 0; loss = 0.0\n",
    "\n",
    "    show_steps(epoch, step, loss, samples_seen)\n",
    "    conclude_train_epoch(epoch, train_loss)\n",
    "\n",
    "    eM365W = EPL.layers[0].layers[0].get_weights()[6]; eM365W = list(tf.reshape(eM365W, (-1,)).numpy())\n",
    "    print(\"memory365: {:.4f},  time taken: {:.1f}m                    \".format(eM365W[0] * hyperparams.initial_m365, (time.time()-start_time)/60))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
