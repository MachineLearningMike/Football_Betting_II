{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "# import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt\n",
    "# import pandas as ps\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import math\n",
    "\n",
    "# from config import config\n",
    "import data_helpers\n",
    "from data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_ID = 'UK.B.A.12'\n",
    "TRAIN_PERCENT= 90\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 4\n",
    "LEARNING_RATE = 0.00001\n",
    "LOSS_MULTIPLIER = 2.0\n",
    "TEAM_EMBS = 50\n",
    "LOSS_RAMBDA = 0.5\n",
    "NORM_PP_PATIENCY = 100\n",
    "MAE_NOT_MSE_LOSS = True\n",
    "TRANSFORMER_DROP = 0.2\n",
    "TRANSFORMER_LAYERS = 6\n",
    "TRANSFORMER_HEADS = 6\n",
    "# ADAPTORS_LAYERS = 10\n",
    "RESET_HISTORY = False\n",
    "MIN_PROFIT = -0.10\n",
    "DISTRIBUTION_KEYS = [2.0, 1.0, 0.5, 0.0, -0.5, -1.0]\n",
    "MIN_PROFIT_P_PER_GAME_PER_QGROUP = 0.9\n",
    "id_to_ids_filename = 'England-200-1e-07-7300-75-0.8-False'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryDirPath = \"./data/football-data-co-uk/England\"\n",
    "df = data_helpers.get_master_df_from_football_data_co_uk(countryDirPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n",
      "['tottenham', 'arsenal', 'liverpool', '[UNK]', 'tottenham', 'chelsea', '[UNK]', 'man_united', '[UNK]', '[UNK]', '[UNK]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[101, 4, 58, 0, 101, 27, 0, 62, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tottenham arsenal liverpool tottenham chelsea man_united'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hyperparams:    \n",
    "    nDivisions = 4 + 1  # E0, E1, E2, E3, and Unknown\n",
    "    division_embs = 4\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 10  # 0 for 0 goals not for Unknown.\n",
    "    goal_embs = 4\n",
    "    nResults = 4    # HWin, Draw, AWin, and Unknown\n",
    "    result_embs = 4\n",
    "    # Mate d_model an even number!!!\n",
    "    d_model = get_std_size()    + division_embs * len(Div_cols) + team_embs * len(Team_cols) \\\n",
    "                                + goal_embs * len(Goal_cols) + result_embs * len(Result_cols)\n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "    # d_model = team_emb_size * 2 + country_emb_size * 3 + odds_size + outcome_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(os.getcwd(), 'data', 'id_to_ids', id_to_ids_filename + '.json')\n",
    "id_to_ids = data_helpers.LoadJsonData(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(ids) for (tag, label, ids) in id_to_ids.values()]\n",
    "maxLen = max(lengths)\n",
    "plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "plt.ylim(plt.ylim())\n",
    "maxLen = max(lengths)\n",
    "# plt.plot([maxLen, maxLen], plt.ylim())\n",
    "plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "MAX_TOKENS = maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100000, 'E0', datetime.date(2004, 1, 11), 'Man_City', 'Norwich', 1.72, 3.4, 5.0, 1.7, 3.2, 5.0, 1.65, 3.3, 4.4, 1.66, 3.1, 5.0, 1.0, 0.0, 1, 1, 'H', 'D', 19.0, 10.0, 11.0, 5.0, 9.0, 4.0, 10.0, 13.0, 1.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "_Cols_List_to_Embedd = [Div_cols, Team_cols, Goal_cols, Result_cols]\n",
    "_Cols_List_to_Standardize = [Odds_cols, Shoot_cols, ShootT_cols, Corner_cols, Faul_cols, Yellow_cols, Red_cols]\n",
    "_Cols_List_for_Label = [Full_Goal_cols, Odds_cols]\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n",
    "\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "base_bbab = list(df.loc[df['id'] == 100000, BBAB_cols].iloc[0, :])\n",
    "print(base_bbab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B365H': (3.1630270400481795, 1.4687447460469159, 46.0), 'HS': (11.39694154084398, 4.709404811489129, 43.0), 'HST': (4.815343915343916, 2.759941394538306, 24.0), 'HC': (5.34632855852368, 2.842282967456132, 24.0), 'HF': (11.421925409730287, 3.7612036770331043, 77.0), 'HY': (1.5455413601755066, 1.2348960213340971, 11.0), 'HR': (0.08013937282229965, 0.2855927650445304, 3.0)}\n"
     ]
    }
   ],
   "source": [
    "std_path = os.path.join('./data', 'datasets', id_to_ids_filename + \".json\")\n",
    "std_params = get_standardization_params(df)\n",
    "print(std_params)\n",
    "data_helpers.SaveJsonData(std_params, std_path)\n",
    "std_params = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38745"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_path = os.path.join('./data', 'datasets', id_to_ids_filename)\n",
    "\n",
    "# ds = generate_dataset_uk(df, id_to_ids, tokenizer_team, std_params)\n",
    "# tf.data.Dataset.save(ds, ds_path)\n",
    "\n",
    "ds = tf.data.Dataset.load(ds_path)\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34870 3875 38745 0\n"
     ]
    }
   ],
   "source": [
    "dataset_size = len(ds)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_ds = ds.take(train_size)\n",
    "test_ds = ds.skip(train_size)\n",
    "\n",
    "print(len(train_ds), len(test_ds), len(ds), len(ds)-len(train_ds)-len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_bbas_tensor = get_dummy_bbas_tensor_uk(df, tokenizer_team, std_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_row(baseId, sequence, base_bb, base_label):\n",
    "    try:\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            block = tf.stack([dummy_bbas_tensor] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, block], axis=0) \n",
    "        # print(\"sequence 1\", sequence.shape)\n",
    "        # sequence[:, 2] = base[2] - sequence[:, 2]   # get delta days.\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, nFeatures)\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]\n",
    "        # print('normalize', sequence.shape, base.shape, mask.shape, mask)\n",
    "        # seq_len_org = tf.Variable(seq_len_org, dtype=tf.int32)    #--------------------------------- comeback\n",
    "        return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "    except:\n",
    "        print('normalize_row exception')\n",
    "        print('norm 1', sequence.shape, base_bb.shape, base_label.shape, mask.shape, nMissings)\n",
    "        print('norm 2', baseId, sequence, base_label, mask, nMissings)\n",
    "        # return (baseId, sequence, base_bb, base_label, mask, seq_len_org)\n",
    "\n",
    "def prepare_batch(baseId, sequence, base_bb, base_label, mask, seq_len_org):\n",
    "    # target = tf.one_hot(tf.squeeze(tf.cast(base_bbab[:, :, -1], dtype=tf.int32), axis=-1), hyperparams.target_onehot_size)\n",
    "    return (baseId, sequence, base_bb, mask), (base_label, seq_len_org)     # (X, Y)\n",
    "\n",
    "def normalize_dataset(ds):\n",
    "    return (\n",
    "        ds.map(lambda baseId, sequence, base_bb, base_label: tf.py_function(\n",
    "            func=normalize_row,\n",
    "            inp=[baseId, sequence, base_bb, base_label],\n",
    "            Tout=[tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32])) #, tf.data.AUTOTUNE == Instability!!!\n",
    "        )\n",
    "\n",
    "def make_train_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def make_test_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(prepare_batch, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_train_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(train_ds_path):\n",
    "    train_ds = tf.data.Dataset.load(train_ds_path)\n",
    "else:\n",
    "    train_ds = normalize_dataset(train_ds)\n",
    "    tf.data.Dataset.save(train_ds, train_ds_path)\n",
    "\n",
    "train_batches = make_train_batches(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds_path = os.path.join('./data', 'datasets', id_to_ids_filename + '_test_' + str(TRAIN_PERCENT))\n",
    "if os.path.exists(test_ds_path):\n",
    "    test_ds = tf.data.Dataset.load(test_ds_path)\n",
    "else:\n",
    "    test_ds = normalize_dataset(test_ds)\n",
    "    tf.data.Dataset.save(test_ds, test_ds_path)\n",
    "\n",
    "test_batches = make_test_batches(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(memory, depth):\n",
    "    positions = tf.range(memory.shape[-1], dtype=tf.float32)\n",
    "    fractional_pos = memory * positions    # fractional position: (batch, fractional position #)\n",
    "    depth = depth/2\n",
    "    depths = tf.range(depth, dtype=tf.float32) / depth\n",
    "    depths = tf.pow(10000.0, depths)    # (depth,)\n",
    "    angle_rads = fractional_pos[:, :, tf.newaxis] / depths  # (batch, fractional position #, depth)\n",
    "    # pos_encoding = rearrange([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], 'w b p d -> w h (w t)')\n",
    "    pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)\n",
    "    return pos_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = tf.ones((100, 200), dtype=tf.float32) * 0.5\n",
    "pos_encoding = positional_encoding(memory, depth=512)\n",
    "# print('pos_encoding', pos_encoding.shape)\n",
    "pos_encoding = pos_encoding[0, :, :]\n",
    "# print(pos_encoding.shape)\n",
    "# Plot the dimensions.\n",
    "plt.pcolormesh(pos_encoding.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ['B365H', 'B365D', 'B365A', 'BWH', 'BWD', 'BWA', 'IWH', 'IWD', 'IWA', 'WHH', 'WHD', 'WHA']\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Result_cols = ['HTR', 'FTR']    # A function of Goal_cols, but contribute to better representation.\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "Red_cols = ['HR', 'AR']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "  def __init__(self, hyperparams, isEncoder=True):\n",
    "    super().__init__()\n",
    "    self.isEncoder = isEncoder\n",
    "    self.division_embedding = tf.keras.layers.Embedding(hyperparams.nDivisions, hyperparams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.team_embedding = tf.keras.layers.Embedding(hyperparams.nTeams, hyperparams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "    self.goal_embedding = tf.keras.layers.Embedding(hyperparams.nGoals, hyperparams.goal_embs, dtype=tf.float32, mask_zero=False) # Learn 0-goal\n",
    "    self.result_embedding = tf.keras.layers.Embedding(hyperparams.nResults, hyperparams.result_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "    self.d_model = hyperparams.d_model\n",
    "    # print(self.d_model)\n",
    "    self.position_permuting_dense = tf.keras.layers.Dense(self.d_model)\n",
    "    self.m365_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "\n",
    "    self.idx_Days = BB_cols.index('Date')\n",
    "    assert self.idx_Days == BBAB_cols.index('Date')\n",
    "\n",
    "  def call(self, x):\n",
    "    # print('pe 1', x)\n",
    "    (sequence, base_bb, mask) = x # sob = sequence or base_bb\n",
    "    sDays = sequence[:, :, self.idx_Days]\n",
    "    bDays = base_bb[:, :, self.idx_Days]\n",
    "    \n",
    "    # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "    # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "    sob = None\n",
    "    if self.isEncoder:\n",
    "      sob = sequence\n",
    "    else:\n",
    "      sob = base_bb\n",
    "    # print('pe 2', sob)\n",
    "\n",
    "    if self.isEncoder:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, goals, results, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Goal_cols), len(Result_cols),  -1], axis=-1)\n",
    "      # print('1', remainder[0, 0])\n",
    "    else:\n",
    "      # Extract odds to remove them\n",
    "      id, div, days, teams, odds, remainder \\\n",
    "      = tf.split(sob, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "      # print('2', remainder[0, 0])  \n",
    "\n",
    "    print('pe 2.7.1 1', div)\n",
    "    div = self.division_embedding(tf.cast(div, dtype=tf.int32))\n",
    "    div = tf.reshape(div, [div.shape[0], div.shape[1], -1])\n",
    "    print('pe 2.7.1 2', div)\n",
    "    teams = self.team_embedding(tf.cast(teams, dtype=tf.int32))\n",
    "    teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1])\n",
    "    if self.isEncoder:\n",
    "      goals = self.goal_embedding(tf.cast(goals, dtype=tf.int32))\n",
    "      goals = tf.clip_by_value(goals, 0, hyperparams.nGoals)\n",
    "      goals = tf.reshape(goals, [goals.shape[0], goals.shape[1], -1])\n",
    "      results = self.result_embedding(tf.cast(results, dtype=tf.int32))\n",
    "      results = tf.reshape(results, [results.shape[0], results.shape[1], -1])    \n",
    "      # print('pe 2.8', div, teams, goals, results, odds, remainder)\n",
    "      \n",
    "    if self.isEncoder:\n",
    "      concat = [div, teams, goals, results, odds, remainder]\n",
    "    else:\n",
    "      concat = [div, teams, odds, remainder]\n",
    "\n",
    "    sob = tf.concat(concat, axis=-1)\n",
    "    sob = self.position_permuting_dense(sob)\n",
    "\n",
    "    days_ago = tf.cast(bDays - sDays, dtype=tf.float32) if self.isEncoder else tf.cast(bDays - bDays, dtype=tf.float32)\n",
    "    \n",
    "    m365 = self.m365_embedding(tf.zeros_like((hyperparams.m365_size,), dtype=tf.float32)) * hyperparams.initial_m365  # expected shape: (1, hyperparams.remain_365_size)\n",
    "    m365 = tf.squeeze(m365, axis=0)\n",
    "    memory_alpha = tf.math.pow(m365, 1.0/365) # (hyperparams.m365_size,)\n",
    "    memory = tf.math.pow(memory_alpha, days_ago[:, :, tf.newaxis])  # decrease as days_ago increase, if memory <= 1.0 as expected.\n",
    "    memory = tf.reduce_mean(memory, axis=-1)\n",
    "\n",
    "    pe = positional_encoding(memory, depth=sob.shape[-1]) # (batch, d_model)\n",
    "    pe = pe / tf.math.sqrt(tf.cast(sob.shape[-1], tf.float32))\n",
    "    sob = sob + pe\n",
    "\n",
    "    print('pe 4', sob)\n",
    "\n",
    "    if self.isEncoder:\n",
    "      mask = mask\n",
    "    else:\n",
    "      mask = mask[:, 0:sob.shape[1], :]\n",
    "\n",
    "    return (sob, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]]\n",
      "\n",
      " [[1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  ...\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]]\n",
      "\n",
      " [[ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [-0.03459274 -0.01653482 -0.03282318 -0.01451234]\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  ...\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]]\n",
      "\n",
      " [[ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  [-0.02123066 -0.04310587 -0.00337918  0.01811862]\n",
      "  [-0.02123066 -0.04310587 -0.00337918  0.01811862]\n",
      "  ...\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]\n",
      "  [ 0.03913656 -0.01701573  0.01289271 -0.03024174]]\n",
      "\n",
      " [[ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [-0.02123066 -0.04310587 -0.00337918  0.01811862]\n",
      "  ...\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [ 0.03344646 -0.02148699 -0.00353544  0.0300147 ]\n",
      "  [-0.03459274 -0.01653482 -0.03282318 -0.01451234]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[-0.07853314 -0.6087461  -0.19263305 ...  0.5172075  -0.02629056\n",
      "    0.20137215]\n",
      "  [-0.203362   -0.0280304   0.07296687 ...  0.08810566  0.25701517\n",
      "    0.2672611 ]\n",
      "  [-0.1535339   0.2093333   0.31114712 ...  0.13532811  0.14130118\n",
      "    0.03030709]\n",
      "  ...\n",
      "  [-0.4794122  -0.26939398 -0.0879734  ... -0.08706371  0.2318748\n",
      "    0.2683349 ]\n",
      "  [-0.36532977  0.23870797  0.29714155 ...  0.19794098 -0.09435267\n",
      "    0.46410227]\n",
      "  [ 0.26620677  0.5391999   0.3469692  ...  0.10658519  0.25727338\n",
      "    0.33856913]]\n",
      "\n",
      " [[-0.3097897  -0.19802083  0.40229604 ...  0.12089184  0.22744012\n",
      "    0.14526033]\n",
      "  [ 0.563128   -0.36038676  0.24957623 ...  0.33370644 -0.22367746\n",
      "    0.19655591]\n",
      "  [ 0.41830075  0.6815189   0.3543856  ...  0.13764289  0.08719492\n",
      "    0.07867224]\n",
      "  ...\n",
      "  [-0.11297102  0.1426714   0.13724054 ...  0.1894074   0.26536995\n",
      "    0.03143341]\n",
      "  [-0.25698495 -0.24225488 -0.01766615 ... -0.21835731  0.37614322\n",
      "    0.18002525]\n",
      "  [-0.4691445  -0.16455159  0.4177537  ...  0.20571017  0.13765508\n",
      "    0.38703242]]\n",
      "\n",
      " [[ 0.14627467 -0.6464349   0.54469264 ...  0.08237167  0.34066468\n",
      "   -0.06861628]\n",
      "  [-0.46021578  0.22668731  0.22727671 ...  0.40394795 -0.48593932\n",
      "    0.86539525]\n",
      "  [-0.88733655 -0.72887844 -0.03068168 ... -0.4215017   0.33293113\n",
      "    0.26158205]\n",
      "  ...\n",
      "  [-0.5506898  -0.1556478   0.296791   ... -0.0489242   0.27495247\n",
      "    0.3248419 ]\n",
      "  [-0.39548376 -0.07682889  0.03355013 ...  0.2183125   0.41151315\n",
      "    0.01742684]\n",
      "  [-0.300677    0.01785273  0.06989504 ...  0.09184718  0.39067808\n",
      "   -0.16263735]]\n",
      "\n",
      " [[-0.24033067 -0.31668985  0.07111865 ... -0.19348818  0.25376755\n",
      "    0.21245909]\n",
      "  [ 0.08006191 -0.23551947 -0.05374534 ...  0.36995852 -0.18567806\n",
      "   -0.2604278 ]\n",
      "  [ 0.04645222  0.15307748  0.09881898 ...  0.04254414  0.32618937\n",
      "    0.26321363]\n",
      "  ...\n",
      "  [-0.5105852   0.12969118  0.2933273  ...  0.19793652 -0.09435616\n",
      "    0.46409953]\n",
      "  [-0.02662528  0.28495196  0.14241797 ...  0.12719108  0.3769023\n",
      "    0.17871718]\n",
      "  [-0.38539585  0.13881373  0.62965155 ...  0.17368405  0.16191079\n",
      "    0.20318092]]], shape=(4, 200, 152), dtype=float32)\n",
      "(4, 200, 152) (4, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "pos = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "\n",
    "cnt = 2\n",
    "for z in train_batches:\n",
    "    (baseId, sequence, base_bb, mask), (base_label, seq_len_org) = z\n",
    "    cnt -= 1\n",
    "    if cnt == 0: break\n",
    "# print('baseId', baseId)\n",
    "sample_x = (sequence, base_bb, mask)\n",
    "eSob, eMask = pos.call(sample_x)\n",
    "print(eSob.shape, eMask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]]\n",
      "\n",
      " [[1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  ...\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]]\n",
      "\n",
      " [[-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.02573355  0.02306061 -0.00515777 -0.0063461 ]\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  ...\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]]\n",
      "\n",
      " [[ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  [-0.04298767  0.04895666 -0.00707898 -0.00426584]\n",
      "  [-0.04298767  0.04895666 -0.00707898 -0.00426584]\n",
      "  ...\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]\n",
      "  [ 0.01161345  0.01376218  0.03536971 -0.04714228]]\n",
      "\n",
      " [[-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04298767  0.04895666 -0.00707898 -0.00426584]\n",
      "  ...\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.04097806  0.04344261 -0.03476082 -0.02476925]\n",
      "  [-0.02573355  0.02306061 -0.00515777 -0.0063461 ]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[-0.11522605 -0.22138542 -0.61806625 ...  0.35530162 -0.04749005\n",
      "    0.6617651 ]\n",
      "  [ 0.10908492  0.10972522  0.11862217 ... -0.02009235  0.2189587\n",
      "    0.04640009]\n",
      "  [ 0.33327875  0.4977515  -0.22131768 ... -0.22688061 -0.19139716\n",
      "   -0.1083462 ]\n",
      "  ...\n",
      "  [ 0.23349465 -0.30007517 -0.27681926 ...  0.13011934 -0.37992027\n",
      "   -0.05223496]\n",
      "  [ 0.29224366 -0.049245   -0.16140145 ... -0.42584234 -0.06177851\n",
      "    0.2962529 ]\n",
      "  [-0.06735144 -0.35726443 -0.12275177 ... -0.13633806  0.22129735\n",
      "    0.19834541]]\n",
      "\n",
      " [[ 0.2538227   0.16786884  0.01699217 ... -0.29804334  0.32384145\n",
      "    0.22116345]\n",
      "  [ 0.2915609  -0.07376733 -0.4222618  ... -0.29135978 -0.449067\n",
      "    0.49807277]\n",
      "  [ 0.13933513  0.15481625  0.12499963 ... -0.3830244  -0.11292044\n",
      "    0.15404823]\n",
      "  ...\n",
      "  [ 0.02606613  0.10483613 -0.35173443 ...  0.03252963 -0.0557261\n",
      "   -0.00698941]\n",
      "  [ 0.21667765 -0.034518    0.27981555 ...  0.08890034  0.09363095\n",
      "    0.03937888]\n",
      "  [-0.01492812 -0.11329548 -0.20848775 ... -0.17930168  0.09804542\n",
      "    0.10652007]]\n",
      "\n",
      " [[ 0.32433054  0.12509885  0.0195526  ... -0.7286351   0.02819159\n",
      "    0.09549343]\n",
      "  [ 0.26214013  0.44481197  0.29982954 ...  0.4226223   0.19702521\n",
      "    0.49108064]\n",
      "  [ 0.78349096  0.42435277  0.02402486 ...  0.03248449 -0.25187543\n",
      "   -0.06742676]\n",
      "  ...\n",
      "  [-0.04795364 -0.10648038  0.01726598 ... -0.03330971  0.34953567\n",
      "    0.22585678]\n",
      "  [-0.22006735 -0.2512741  -0.08132309 ...  0.44032577  0.582596\n",
      "    0.02265299]\n",
      "  [ 0.09236731  0.16737111 -0.38498673 ... -0.00759177 -0.02163748\n",
      "   -0.1345096 ]]\n",
      "\n",
      " [[ 0.3954096  -0.11351285 -0.27165836 ... -0.2632462  -0.47423446\n",
      "   -0.06401906]\n",
      "  [ 0.2566502   0.6305985  -0.16759625 ...  0.25963843 -0.36450383\n",
      "    0.15920627]\n",
      "  [ 0.11856398 -0.22999999 -0.0783639  ...  0.12464198  0.06282089\n",
      "    0.11761817]\n",
      "  ...\n",
      "  [ 0.14698827 -0.1582618  -0.1652157  ... -0.42584682 -0.061782\n",
      "    0.29625016]\n",
      "  [-0.0991806  -0.36237127 -0.3522177  ...  0.04720211  0.02089491\n",
      "    0.16621527]\n",
      "  [-0.2839096   0.03195199 -0.40525115 ...  0.13446301  0.21875243\n",
      "   -0.13523978]]], shape=(4, 200, 152), dtype=float32)\n",
      "(4, 200, 152) (4, 200, 200)\n"
     ]
    }
   ],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=True)\n",
    "eSob, eMask = PE(sample_x)\n",
    "print(eSob.shape, eMask.shape )\n",
    "# print(eSob, eMask)\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[ 0.04187502 -0.01505711 -0.04645841 -0.03023331]]\n",
      "\n",
      " [[ 0.01205026  0.04118559 -0.04382021 -0.01632625]]\n",
      "\n",
      " [[ 0.01205026  0.04118559 -0.04382021 -0.01632625]]\n",
      "\n",
      " [[ 0.04187502 -0.01505711 -0.04645841 -0.03023331]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[-1.70423150e-01 -9.28651392e-02 -1.83716968e-01  2.20134735e-01\n",
      "    2.39576876e-01  2.59256631e-01 -2.62633234e-01 -9.80153680e-02\n",
      "   -7.82292336e-02  1.82329461e-01 -1.69055596e-01 -3.95437069e-02\n",
      "   -8.85909423e-03 -7.37540573e-02 -6.54950514e-02 -2.72160619e-01\n",
      "   -6.66015372e-02  1.39113873e-01 -1.13103271e-01 -6.57496154e-02\n",
      "    7.69052804e-02  1.34132668e-01  1.19498275e-01  1.48088530e-01\n",
      "   -1.01894408e-01 -3.04594636e-02 -6.70219213e-02 -1.36059612e-01\n",
      "    6.97656870e-02  1.43787786e-01 -1.91867292e-01  9.02842134e-02\n",
      "    1.35264948e-01 -2.46907353e-01 -1.38916820e-03 -1.42982155e-01\n",
      "   -1.46819353e-01 -1.53017670e-01  1.57550842e-01 -2.10454971e-01\n",
      "   -3.36731263e-02  2.85888255e-01  6.85836226e-02 -2.22699016e-01\n",
      "    4.23148036e-01  2.40461543e-01 -2.83062220e-01 -2.38554418e-01\n",
      "   -2.37275630e-01  1.79608196e-01 -2.83507854e-01 -2.65817285e-01\n",
      "    6.23899102e-02 -1.68007344e-01 -3.21854323e-01 -1.11576002e-02\n",
      "   -2.26070415e-02  7.02805817e-02 -2.66848374e-02  1.99362755e-01\n",
      "   -4.29538965e-01 -4.85573225e-02  1.41201496e-01 -4.99658734e-02\n",
      "   -2.64252096e-01 -8.27446058e-02  5.24521112e-01 -1.84404347e-02\n",
      "   -5.27325459e-02  1.20835587e-01  1.04303285e-01  1.64143771e-01\n",
      "   -1.57323942e-01  1.84692264e-01  3.23944777e-01 -1.23854995e-01\n",
      "    2.94407934e-01  1.08790956e-01  1.51463360e-01  1.60173997e-02\n",
      "    3.06400895e-01  3.71686630e-02  2.49232799e-02 -3.09596807e-02\n",
      "   -2.02798188e-01 -4.01271015e-01  9.77980345e-02 -5.60636744e-02\n",
      "    2.34810263e-02 -1.96812570e-01 -3.07337940e-03  2.07872421e-01\n",
      "    2.50933319e-01 -9.99117568e-02  3.69320720e-01 -5.78422621e-02\n",
      "   -3.15710872e-01  3.58514860e-02  1.31774545e-01  4.69522849e-02\n",
      "    2.82895625e-01  1.13008827e-01 -5.12023494e-02  3.07071283e-02\n",
      "    3.60402986e-02  1.54624522e-01  5.18309586e-02  3.72185074e-02\n",
      "    4.56052274e-02  3.38952094e-01  1.15201943e-01  8.30889270e-02\n",
      "    1.29010901e-02  2.61144847e-01  7.97662884e-03 -1.81987077e-01\n",
      "   -1.39794350e-02 -6.17324784e-02 -1.09332822e-01  2.65136391e-01\n",
      "   -7.35779926e-02 -6.98402524e-04  8.94254819e-02  2.27470368e-01\n",
      "    5.41843250e-02  8.19950849e-02  8.22662264e-02  2.57420897e-01\n",
      "   -1.38011336e-01  1.85765177e-01 -1.57837272e-02 -1.16462782e-02\n",
      "    1.24684341e-01  1.69768482e-01  5.64820990e-02  1.83682397e-01\n",
      "   -7.25634024e-02  3.53602052e-01  1.50748760e-01  7.98985064e-02\n",
      "    1.33287430e-01  1.70831308e-02  1.52422771e-01  2.33756065e-01\n",
      "    1.78497225e-01 -2.72700340e-02 -2.17382401e-01  4.91036475e-02\n",
      "   -9.84760746e-02  6.43950850e-02  3.28407586e-02 -4.53352109e-02]]\n",
      "\n",
      " [[-1.54219508e-01  9.49505530e-03 -1.48182958e-01  2.29198202e-01\n",
      "    1.99915051e-01  1.93973333e-01 -1.73515707e-01 -1.53339133e-01\n",
      "   -1.68751776e-02  1.46803334e-01 -1.04333922e-01 -1.06453098e-01\n",
      "   -2.03984715e-02  7.88798276e-03 -1.69908479e-01 -2.72787094e-01\n",
      "   -2.30946615e-01  8.47763792e-02 -1.00185782e-01 -9.67769697e-02\n",
      "    2.02343334e-03  8.65435526e-02  5.21820858e-02  5.07064909e-02\n",
      "   -1.23312600e-01 -8.95067155e-02 -1.64296225e-01 -1.14733025e-01\n",
      "    3.81412059e-02  5.42612672e-02 -1.81470439e-01  1.14693016e-01\n",
      "    2.05812126e-01 -1.40972361e-01  1.82052888e-02 -1.49919808e-01\n",
      "   -6.12953939e-02 -1.80085376e-01  1.78013995e-01 -1.71393335e-01\n",
      "   -1.17987916e-01  2.28545681e-01  7.25373775e-02 -1.88451558e-01\n",
      "    3.80352199e-01  2.95062125e-01 -3.58242840e-01 -2.46060282e-01\n",
      "   -3.02917242e-01  1.79354250e-01 -3.31360698e-01 -2.28651717e-01\n",
      "    3.58533487e-02 -1.71532348e-01 -3.24233323e-01  1.23100188e-02\n",
      "    2.58101113e-02  1.67053729e-01 -3.73633876e-02  1.99785113e-01\n",
      "   -4.12433773e-01 -8.83156881e-02  1.40750259e-01 -3.25082801e-02\n",
      "   -2.10325390e-01 -5.15206531e-02  4.20370847e-01 -9.56846252e-02\n",
      "   -4.81118746e-02  1.98621452e-01  8.20705444e-02  1.93503290e-01\n",
      "   -8.43094960e-02  2.42992759e-01  2.93132097e-01 -3.62967774e-02\n",
      "    2.95178771e-01  2.01832145e-01  1.50681287e-01  4.33301404e-02\n",
      "    3.82514030e-01  4.20835502e-02  7.40713924e-02 -8.03238228e-02\n",
      "   -2.25511968e-01 -3.03081036e-01  7.44084641e-02 -5.24486080e-02\n",
      "   -1.44694671e-02 -6.75386116e-02  6.60730153e-03  2.42764860e-01\n",
      "    2.68575460e-01 -4.00097594e-02  3.00897807e-01  4.91801128e-02\n",
      "   -2.35211819e-01  4.29429039e-02  1.90267861e-01  1.13588057e-01\n",
      "    2.66816080e-01  1.08959936e-01 -3.68661433e-02  5.93150705e-02\n",
      "    1.30286813e-01  7.77816772e-02  9.12962407e-02  1.14780426e-01\n",
      "   -9.13554206e-02  2.57848054e-01  7.33851269e-02  1.54376552e-01\n",
      "   -5.77374920e-02  2.03711212e-01 -7.68420473e-02 -1.40371293e-01\n",
      "   -6.62534907e-02 -5.86053208e-02 -4.93746772e-02  2.11097240e-01\n",
      "   -8.38033482e-02 -3.20418626e-02  2.42685303e-02  1.98534325e-01\n",
      "   -2.75618806e-02  7.36585110e-02  5.15083298e-02  2.63322622e-01\n",
      "   -9.12305787e-02  2.30911195e-01 -7.64391795e-02  1.85802877e-02\n",
      "    1.25246048e-01  1.29849344e-01  5.21122962e-02  2.22060084e-01\n",
      "   -3.03284973e-02  3.81423384e-01  9.39686671e-02  7.56203830e-02\n",
      "    1.46338791e-01  8.39697272e-02  1.92286521e-01  1.69369712e-01\n",
      "    2.24011511e-01 -5.25394753e-02 -1.21637769e-01  1.03471369e-01\n",
      "   -1.02604963e-01  2.74105072e-02  3.35168131e-02 -7.44245946e-03]]\n",
      "\n",
      " [[-1.31194904e-01 -7.05669075e-02 -1.18508205e-01  3.04363966e-01\n",
      "    2.22757295e-01  1.61885917e-01 -2.25151300e-01 -1.68286443e-01\n",
      "   -3.16820145e-02  1.43883854e-01 -2.90540308e-02 -9.61489230e-02\n",
      "   -2.01873519e-02  5.10699078e-02 -1.01523638e-01 -2.82634556e-01\n",
      "   -1.74434513e-01  1.22583821e-01 -1.31861508e-01 -1.12293288e-02\n",
      "    5.83653599e-02  1.01985842e-01  8.16338137e-03  5.91960177e-02\n",
      "   -4.75291684e-02 -7.94472247e-02 -1.42040059e-01 -6.78862557e-02\n",
      "    5.70801795e-02  5.25329895e-02 -1.96035147e-01  1.33575380e-01\n",
      "    1.26846746e-01 -1.60401747e-01  2.03978643e-03 -1.83661163e-01\n",
      "   -1.09282553e-01 -1.94950104e-01  1.61118269e-01 -1.99559450e-01\n",
      "   -7.47386143e-02  1.76673785e-01  1.24079861e-01 -1.70721471e-01\n",
      "    3.88722062e-01  2.50142723e-01 -2.73660392e-01 -2.54887104e-01\n",
      "   -3.32514197e-01  1.18847907e-01 -2.11074516e-01 -2.84478039e-01\n",
      "    1.06027141e-01 -9.98316184e-02 -3.05415511e-01 -3.24824303e-02\n",
      "   -9.44510475e-03  1.92229867e-01  5.87030826e-03  1.73863053e-01\n",
      "   -3.71887565e-01 -1.51083469e-01  1.57691985e-01 -7.29024038e-02\n",
      "   -2.20546588e-01 -9.04867500e-02  4.20041114e-01 -3.72339487e-02\n",
      "    3.30064557e-02  1.40703857e-01  9.65117142e-02  1.49584487e-01\n",
      "   -1.37986124e-01  2.33790427e-01  2.87734151e-01 -7.93339834e-02\n",
      "    3.76618028e-01  2.09929079e-01  1.92003071e-01 -2.05962732e-02\n",
      "    3.26816142e-01  4.10733335e-02  1.22805238e-02 -1.13408111e-01\n",
      "   -1.72560126e-01 -3.02865088e-01  7.75098130e-02 -3.92891392e-02\n",
      "   -1.33233592e-02 -1.43133432e-01  2.76058242e-02  2.02381834e-01\n",
      "    2.05649614e-01 -6.57168552e-02  2.54188627e-01  5.67138940e-02\n",
      "   -2.98300505e-01 -1.19228289e-02  7.82501101e-02  1.04885742e-01\n",
      "    2.66655058e-01  1.19103938e-01  2.60718167e-02  6.33870214e-02\n",
      "    8.67330283e-02  8.26664567e-02  1.05109699e-01  5.85267246e-02\n",
      "   -3.89399007e-02  2.66388148e-01  1.20502323e-01  9.37259123e-02\n",
      "   -7.96645880e-04  1.48567200e-01 -4.51060310e-02 -6.35471418e-02\n",
      "   -8.84150043e-02 -1.23270303e-02 -2.66303495e-02  2.27599740e-01\n",
      "   -6.37963489e-02 -1.76638067e-02  6.85832947e-02  2.11111993e-01\n",
      "    9.57378000e-03  8.13341662e-02 -5.45619056e-02  2.20338404e-01\n",
      "   -8.69231299e-02  3.06526840e-01 -3.66681442e-02 -1.80967003e-02\n",
      "    1.50676638e-01  1.42651588e-01  3.42429057e-02  3.10985416e-01\n",
      "   -9.23321918e-02  2.69044548e-01  1.76857442e-01  9.68325809e-02\n",
      "    1.26304537e-01  1.12062737e-01  1.29566163e-01  1.65102839e-01\n",
      "    2.48226881e-01 -8.48986208e-03 -1.41445875e-01  8.53579193e-02\n",
      "   -1.03518479e-01  6.54646754e-02  9.45026577e-02 -3.10042724e-02]]\n",
      "\n",
      " [[-4.32924852e-02  3.89213897e-02  7.49387220e-02  3.77520025e-02\n",
      "    3.47446024e-01  4.00294900e-01  5.03599830e-03 -5.46851978e-02\n",
      "    1.01674832e-01  6.31408840e-02 -2.87560046e-01 -8.82441476e-02\n",
      "   -1.09065905e-01 -4.03330326e-02 -2.93803997e-02 -1.72445700e-01\n",
      "   -1.06202610e-01  8.65158588e-02  6.22151792e-02  2.02074647e-04\n",
      "   -4.94804606e-03  1.91196486e-01  2.38832198e-02  3.81246507e-02\n",
      "    8.42477828e-02 -1.43401206e-01  1.50846168e-01 -1.71674892e-01\n",
      "    4.81132306e-02  2.93472521e-02 -1.58685714e-01 -1.25515228e-02\n",
      "    1.18668713e-01 -1.90364033e-01  3.70952860e-02 -1.14414528e-01\n",
      "   -1.43491607e-02 -1.37057513e-01  1.62813291e-01 -1.07441828e-01\n",
      "   -1.56294823e-01  6.39887974e-02  9.29316878e-02 -4.12281454e-02\n",
      "    2.06893340e-01  7.58206546e-02 -3.29702109e-01 -7.93763250e-03\n",
      "   -7.42365420e-03  1.90336436e-01 -1.98065370e-01 -2.10603610e-01\n",
      "    3.05736177e-02 -6.11281842e-02 -1.51112929e-01 -8.52423012e-02\n",
      "   -1.66774243e-01  8.55028927e-02 -1.55878231e-01  9.42304805e-02\n",
      "   -1.24519013e-01 -1.24854803e-01 -4.54844609e-02 -4.21866998e-02\n",
      "   -2.76321143e-01  7.72480443e-02  2.05746412e-01 -1.30422086e-01\n",
      "   -1.70779839e-01  1.12287849e-01  1.23826355e-01  1.27145916e-01\n",
      "   -1.96599234e-02  8.95378515e-02  4.71159033e-02  9.35117528e-03\n",
      "    1.00035354e-01  1.41623616e-01  9.12173092e-02  1.75635964e-01\n",
      "    3.31230432e-01  3.09207350e-01  1.33720741e-01  1.10760421e-01\n",
      "   -1.48459494e-01 -8.95299837e-02  2.60240525e-01 -2.51011029e-02\n",
      "    1.40570849e-03 -5.59121296e-02 -7.27389678e-02  7.06698671e-02\n",
      "    2.22988069e-01  2.66283639e-02  1.51577860e-01  1.00251287e-01\n",
      "   -5.17548993e-02  1.12930700e-01  1.33596689e-01  1.59725219e-01\n",
      "    3.51122379e-01  3.08954924e-01  2.12751701e-02 -7.11928681e-02\n",
      "    9.77815688e-02  1.88870400e-01 -1.68708265e-01  4.14984114e-02\n",
      "    8.98988545e-03  2.52821535e-01  5.57817630e-02  5.46576828e-03\n",
      "    2.06234694e-01  2.65110612e-01  1.75757349e-01  2.28100568e-02\n",
      "    5.90402670e-02  1.02910250e-02 -3.92307118e-02  1.08673975e-01\n",
      "   -5.88054433e-02  9.54493135e-02  4.82179821e-02  1.13224700e-01\n",
      "    1.39534041e-01  9.76916328e-02  1.58851400e-01  1.52317688e-01\n",
      "    1.20807067e-01  1.50452420e-01  2.63489261e-02 -2.47548744e-02\n",
      "    1.49492607e-01  1.65176645e-01  3.22593778e-01  1.69875681e-01\n",
      "   -9.47402194e-02  2.19763011e-01 -7.68544525e-03 -5.51129803e-02\n",
      "    1.78599924e-01  6.66883588e-02 -6.97826520e-02  1.60712570e-01\n",
      "   -7.65099302e-02  6.94754049e-02 -1.64316654e-01  3.41271982e-02\n",
      "   -1.23765655e-01 -1.69926137e-01 -4.22448739e-02  1.93521082e-01]]], shape=(4, 1, 152), dtype=float32)\n",
      "(4, 1, 152) (4, 1, 200)\n",
      "(4, 1, 152) (4, 1, 200)\n"
     ]
    }
   ],
   "source": [
    "PE = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "dSob, dMask = PE(sample_x)\n",
    "print(dSob.shape, dMask.shape )\n",
    "print(dSob.shape, dMask.shape )\n",
    "del PE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "      super().__init__()\n",
    "      self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()   # So the default -1 axix is normalized across. No inter-token operatoin.\n",
    "      self.add = tf.keras.layers.Add()\n",
    "\n",
    "class CrossAttention(BaseAttention):\n",
    "    def call(self, x, context, mask):\n",
    "      attn_output, attn_scores = self.mha(\n",
    "          query=x,\n",
    "          key=context,\n",
    "          value=context,\n",
    "          attention_mask=mask,\n",
    "          return_attention_scores=True)\n",
    "    \n",
    "      # Cache the attention scores for plotting later.\n",
    "      self.last_attn_scores = attn_scores\n",
    "      x = self.add([x, attn_output])\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "  \n",
    "class GlobalSelfAttention(BaseAttention): \n",
    "    def call(self, x, mask):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          attention_mask=mask)    # intentional inter-token operation\n",
    "      x = self.add([x, attn_output])  # token-wise\n",
    "      x = self.layernorm(x)         # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class CausalSelfAttention(BaseAttention): # mask-agnostic\n",
    "    def call(self, x):\n",
    "      attn_output = self.mha(\n",
    "          query=x,\n",
    "          value=x,\n",
    "          key=x,\n",
    "          use_causal_mask = True)     # look-over mask is generagted and used, in decoder layers\n",
    "      x = self.add([x, attn_output])  # mask-agnostic\n",
    "      x = self.layernorm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class FeedForward(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.seq = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),    # across -1 axis\n",
    "        tf.keras.layers.Dense(d_model),    # across -1 axis\n",
    "        tf.keras.layers.Dropout(dropout_rate)    # mask-agnostic\n",
    "      ])\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "    def call(self, x):\n",
    "      x = self.add([x, self.seq(x)])  # mask-agnostic\n",
    "      x = self.layer_norm(x)  # normalize across the default -1 axis. No inter-token operatoin.\n",
    "      return x\n",
    "  \n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.self_attention = GlobalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "      # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      x = self.self_attention(x, mask)\n",
    "      x = self.ffn(x)\n",
    "      return x\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hyperparams.d_model,\n",
    "                      num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4,\n",
    "                      dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "    def call(self, x):\n",
    "      # print('enc in', x)\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      x, mask = self.pos_embedding(x)  # x: (batch, max_tokens, d_model), mask: (batch, max_tokens, max_tokens)\n",
    "      print('enc, pe out', x, mask)\n",
    "      x = self.dropout(x)\n",
    "      # print('enc drop', x)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, mask)\n",
    "        # print('enc layer', x, mask)\n",
    "      return x  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "                *,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dff,\n",
    "                dropout_rate=0.1):\n",
    "      super(DecoderLayer, self).__init__()\n",
    "\n",
    "      self.causal_self_attention = CausalSelfAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "      \n",
    "      self.cross_attention = CrossAttention(\n",
    "          num_heads=num_heads,\n",
    "          key_dim=d_model,\n",
    "          dropout=dropout_rate)\n",
    "\n",
    "      self.ffn = FeedForward(d_model, dff)\n",
    "\n",
    "    def call(self, x, context, cross_attention_mask):\n",
    "      # x: (batch, 1, d_model), context: (batch, max_tokens, d_mode)\n",
    "      x = self.causal_self_attention(x=x)\n",
    "      x = self.cross_attention(x, context, cross_attention_mask)\n",
    "\n",
    "      # Cache the last attention scores for plotting later\n",
    "      self.last_attn_scores = self.cross_attention.last_attn_scores\n",
    "\n",
    "      x = self.ffn(x)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "      return x\n",
    "  \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "\n",
    "      self.d_model = hyperparams.d_model\n",
    "      self.num_layers = hyperparams.num_layers\n",
    "\n",
    "      self.pos_embedding = PositionalEmbedding(hyperparams, isEncoder=False)\n",
    "\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.dec_layers = [\n",
    "          DecoderLayer(d_model=hyperparams.d_model, num_heads=hyperparams.num_heads,\n",
    "                      dff=hyperparams.d_model * 4, dropout_rate=dropout_rate)\n",
    "          for _ in range(hyperparams.num_layers)]\n",
    "\n",
    "      self.last_attn_scores = None\n",
    "\n",
    "    def call(self, x, context):\n",
    "      # x = (sequence, base_bb, mask)\n",
    "      # x[0]: (batch, max_tokens, bbab.len), x[1]: (batch, 1, bb.len), x[2]: (token, max_tokens, max_tokens)\n",
    "      # context: (batch, max_tokens, d_model)\n",
    "      # `x` is token-IDs shape (batch, target_seq_len)\n",
    "      x, ca_mask = self.pos_embedding(x)  # x: (batch, 1, d_model), ca_mask: (batch, 1, max_tokens)     \n",
    "      x = self.dropout(x)\n",
    "      for decoder_layer in self.dec_layers:\n",
    "        x  = decoder_layer(x, context, ca_mask)\n",
    "      self.last_attn_scores = self.dec_layers[-1].last_attn_scores\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hyperparams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.encoder = Encoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.decoder = Decoder(hyperparams, dropout_rate=dropout_rate)\n",
    "\n",
    "      self.final_layer = tf.keras.layers.Dense(hyperparams.d_model) #-------------- to modify\n",
    "\n",
    "    def call(self, inputs):\n",
    "      print('former, in', inputs)\n",
    "      # inputs = (sequence, base_bb, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), mask: (batch, max_token, max_token)\n",
    "      x = self.encoder(inputs)  # (batch, max_tokens, d_model)\n",
    "      # print('former enc out', x)\n",
    "      x = self.decoder(inputs, x)  # (batch, 1, d_model)\n",
    "      # print('former dex out', x)\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      # print('former, logits', logits)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "      # print('former logits2', logits)\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2925300e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2925200e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2924500e+05,  4.0000000e+00,  2.2400000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2833200e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832000e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2840700e+05,  4.0000000e+00,  1.7000000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1798800e+05,  3.0000000e+00,  1.7000000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0005800e+05,  1.0000000e+00,  1.6980000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000300e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000200e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0277000e+05,  1.0000000e+00,  4.3970000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1130400e+05,  2.0000000e+00,  4.3960000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1129300e+05,  2.0000000e+00,  4.3940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0079900e+05,  1.0000000e+00,  2.4430000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0078000e+05,  1.0000000e+00,  2.4290000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0075100e+05,  1.0000000e+00,  2.3770000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2875700e+05,  4.0000000e+00,  1.9110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2875600e+05,  4.0000000e+00,  1.9110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0775900e+05,  2.0000000e+00,  1.9060000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1791100e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2926700e+05,  4.0000000e+00,  2.2610000e+03,  3.7000000e+01,\n",
      "          2.9000000e+01, -9.2802173e-01,  9.3258522e-02,  5.6985599e-01,\n",
      "         -8.5993636e-01,  5.9215844e-02,  2.9751456e-01, -8.5993636e-01,\n",
      "         -4.2912181e-02,  2.2942922e-01, -8.2589370e-01,  5.9215844e-02,\n",
      "          9.3258522e-02]],\n",
      "\n",
      "       [[ 1.0006200e+05,  1.0000000e+00,  1.7010000e+03,  4.0000000e+01,\n",
      "          1.0500000e+02, -7.9185104e-01,  2.5173169e-02,  3.9964259e-01,\n",
      "         -8.9397907e-01,  2.5173169e-02,  5.6985599e-01, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.5780833e-01, -1.9950849e-01,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0277500e+05,  1.0000000e+00,  4.4030000e+03,  4.4000000e+01,\n",
      "          6.8000000e+01, -7.9185104e-01,  1.6134387e-01,  4.3368527e-01,\n",
      "         -7.9185104e-01,  1.6134387e-01,  5.0177062e-01, -7.2376567e-01,\n",
      "          9.3258522e-02, -4.2912181e-02, -7.5780833e-01,  9.3258522e-02,\n",
      "          2.9751456e-01]],\n",
      "\n",
      "       [[ 1.2875800e+05,  4.0000000e+00,  1.9130000e+03,  1.9000000e+01,\n",
      "          3.7000000e+01, -5.1950961e-01,  2.5173169e-02, -3.8333893e-01,\n",
      "         -5.8759499e-01, -7.6954857e-02, -2.1312556e-01, -5.1950961e-01,\n",
      "         -1.7908289e-01, -3.8333893e-01, -6.5568036e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]]\n",
      "\n",
      " [[1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  ...\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]]\n",
      "\n",
      " [[ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [-0.04929994 -0.04381379 -0.02064507 -0.02431922]\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  ...\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]]\n",
      "\n",
      " [[ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  [-0.02276673 -0.0346994  -0.03168336  0.02006685]\n",
      "  [-0.02276673 -0.0346994  -0.03168336  0.02006685]\n",
      "  ...\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]\n",
      "  [ 0.02976717 -0.0014158  -0.02941461 -0.00093478]]\n",
      "\n",
      " [[ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [-0.02276673 -0.0346994  -0.03168336  0.02006685]\n",
      "  ...\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [ 0.00234189  0.03515806  0.01204525  0.0241786 ]\n",
      "  [-0.04929994 -0.04381379 -0.02064507 -0.02431922]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[ 0.21295376  0.81641555 -0.24765097 ...  0.10146602  0.35351193\n",
      "   -0.07993751]\n",
      "  [ 0.568825    0.13151151 -0.24375549 ... -0.273957    0.5246883\n",
      "   -0.53398323]\n",
      "  [ 0.08137472 -0.5410685  -0.2326358  ...  0.02177332  0.46007708\n",
      "   -0.08084739]\n",
      "  ...\n",
      "  [-0.09793706  0.05835699 -0.05575206 ... -0.38449052 -0.34327328\n",
      "   -0.40342927]\n",
      "  [ 0.17611554 -0.17098266 -0.36850694 ... -0.38455784 -0.5471909\n",
      "   -0.21012913]\n",
      "  [-0.04115579 -0.1446165  -0.03810135 ...  0.28957158  0.0379547\n",
      "   -0.06300688]]\n",
      "\n",
      " [[ 0.36541498 -0.16044797 -0.40888965 ... -0.06988079  0.26481012\n",
      "   -0.11441136]\n",
      "  [ 0.00266445  0.49052605 -0.25250554 ...  0.84969836 -0.22618407\n",
      "    0.29691046]\n",
      "  [-0.21485354 -0.406972    0.02463208 ...  0.5575635  -0.12111297\n",
      "    0.04244689]\n",
      "  ...\n",
      "  [-0.3744101  -0.22207326  0.05360055 ... -0.04502309  0.4754829\n",
      "   -0.15977016]\n",
      "  [-0.09614071  0.34186304 -0.02297302 ...  0.1391013  -0.40583774\n",
      "    0.04045456]\n",
      "  [ 0.10622159  0.22287108 -0.11751205 ... -0.36185232  0.12110916\n",
      "   -0.32048154]]\n",
      "\n",
      " [[ 0.01635194 -0.06930792 -0.31362164 ...  0.62683016  0.5031914\n",
      "   -0.04107772]\n",
      "  [-0.5293628  -0.19217715 -0.3784858  ... -0.6073159  -0.328565\n",
      "    0.36624113]\n",
      "  [ 0.599815    0.0452055  -0.578882   ... -0.6479226  -0.37985614\n",
      "   -0.28455144]\n",
      "  ...\n",
      "  [ 0.56933045  0.10932552 -0.14436519 ... -0.27149612 -0.05948424\n",
      "   -0.27656642]\n",
      "  [ 0.03294111  0.20520975  0.20936146 ... -0.27671704  0.39774263\n",
      "   -0.28493655]\n",
      "  [ 0.25092188 -0.44197527 -0.29882908 ...  0.03077581  0.70340675\n",
      "   -0.17276981]]\n",
      "\n",
      " [[-0.09519555  0.03216015 -0.21998715 ...  0.03472316 -0.30252525\n",
      "   -0.15388992]\n",
      "  [ 0.668285    0.25043815 -0.37430757 ...  0.52929354  0.63755345\n",
      "    0.24391869]\n",
      "  [-0.00346678  0.14440241  0.00254156 ...  0.09212957  0.00453186\n",
      "   -0.12461134]\n",
      "  ...\n",
      "  [ 0.03086015 -0.27999946 -0.3723212  ... -0.3845623  -0.5471944\n",
      "   -0.21013187]\n",
      "  [ 0.04392966 -0.18070377 -0.16429974 ...  0.00749154  0.29327863\n",
      "   -0.18721992]\n",
      "  [ 0.34697613 -0.26455736  0.18431571 ... -0.31346312  0.9453578\n",
      "   -0.21152444]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[ 0.21295376  0.81641555 -0.24765097 ...  0.10146602  0.35351193\n",
      "   -0.07993751]\n",
      "  [ 0.568825    0.13151151 -0.24375549 ... -0.273957    0.5246883\n",
      "   -0.53398323]\n",
      "  [ 0.08137472 -0.5410685  -0.2326358  ...  0.02177332  0.46007708\n",
      "   -0.08084739]\n",
      "  ...\n",
      "  [-0.09793706  0.05835699 -0.05575206 ... -0.38449052 -0.34327328\n",
      "   -0.40342927]\n",
      "  [ 0.17611554 -0.17098266 -0.36850694 ... -0.38455784 -0.5471909\n",
      "   -0.21012913]\n",
      "  [-0.04115579 -0.1446165  -0.03810135 ...  0.28957158  0.0379547\n",
      "   -0.06300688]]\n",
      "\n",
      " [[ 0.36541498 -0.16044797 -0.40888965 ... -0.06988079  0.26481012\n",
      "   -0.11441136]\n",
      "  [ 0.00266445  0.49052605 -0.25250554 ...  0.84969836 -0.22618407\n",
      "    0.29691046]\n",
      "  [-0.21485354 -0.406972    0.02463208 ...  0.5575635  -0.12111297\n",
      "    0.04244689]\n",
      "  ...\n",
      "  [-0.3744101  -0.22207326  0.05360055 ... -0.04502309  0.4754829\n",
      "   -0.15977016]\n",
      "  [-0.09614071  0.34186304 -0.02297302 ...  0.1391013  -0.40583774\n",
      "    0.04045456]\n",
      "  [ 0.10622159  0.22287108 -0.11751205 ... -0.36185232  0.12110916\n",
      "   -0.32048154]]\n",
      "\n",
      " [[ 0.01635194 -0.06930792 -0.31362164 ...  0.62683016  0.5031914\n",
      "   -0.04107772]\n",
      "  [-0.5293628  -0.19217715 -0.3784858  ... -0.6073159  -0.328565\n",
      "    0.36624113]\n",
      "  [ 0.599815    0.0452055  -0.578882   ... -0.6479226  -0.37985614\n",
      "   -0.28455144]\n",
      "  ...\n",
      "  [ 0.56933045  0.10932552 -0.14436519 ... -0.27149612 -0.05948424\n",
      "   -0.27656642]\n",
      "  [ 0.03294111  0.20520975  0.20936146 ... -0.27671704  0.39774263\n",
      "   -0.28493655]\n",
      "  [ 0.25092188 -0.44197527 -0.29882908 ...  0.03077581  0.70340675\n",
      "   -0.17276981]]\n",
      "\n",
      " [[-0.09519555  0.03216015 -0.21998715 ...  0.03472316 -0.30252525\n",
      "   -0.15388992]\n",
      "  [ 0.668285    0.25043815 -0.37430757 ...  0.52929354  0.63755345\n",
      "    0.24391869]\n",
      "  [-0.00346678  0.14440241  0.00254156 ...  0.09212957  0.00453186\n",
      "   -0.12461134]\n",
      "  ...\n",
      "  [ 0.03086015 -0.27999946 -0.3723212  ... -0.3845623  -0.5471944\n",
      "   -0.21013187]\n",
      "  [ 0.04392966 -0.18070377 -0.16429974 ...  0.00749154  0.29327863\n",
      "   -0.18721992]\n",
      "  [ 0.34697613 -0.26455736  0.18431571 ... -0.31346312  0.9453578\n",
      "   -0.21152444]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[ 0.00397555 -0.04144856  0.03011568  0.0003934 ]]\n",
      "\n",
      " [[ 0.03353164  0.02855387 -0.0342214  -0.03367293]]\n",
      "\n",
      " [[ 0.03353164  0.02855387 -0.0342214  -0.03367293]]\n",
      "\n",
      " [[ 0.00397555 -0.04144856  0.03011568  0.0003934 ]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[ 0.02071478  0.04813518  0.06978477  0.19649175  0.2699177\n",
      "   -0.18418635 -0.17796084  0.15952528 -0.28777954 -0.10466998\n",
      "    0.09933613  0.06956063 -0.04352832 -0.10080092 -0.02124687\n",
      "   -0.23811153 -0.04746222 -0.10042884 -0.02413973 -0.17850298\n",
      "    0.30747175  0.27137     0.3162856  -0.1328033  -0.19620281\n",
      "   -0.08681664 -0.22432092 -0.01918399  0.07967945  0.05440876\n",
      "   -0.07590213  0.14723751 -0.32856905  0.1176897   0.08678398\n",
      "   -0.28361845 -0.04526554  0.05638336 -0.02771322 -0.17821181\n",
      "   -0.2019044   0.18326455  0.23550163 -0.15322137 -0.13248637\n",
      "    0.08272754 -0.06733815  0.05634322  0.06088947 -0.4611128\n",
      "   -0.00147575  0.15616643 -0.05196221  0.18167357 -0.04075666\n",
      "   -0.24440883  0.07395753 -0.18514086 -0.42640623  0.2008745\n",
      "   -0.01836648 -0.16863225  0.04959122  0.13414854  0.07709629\n",
      "   -0.12594251 -0.06027855  0.24112907 -0.26078424 -0.00189199\n",
      "   -0.05298381 -0.13632461 -0.24824572 -0.14541453 -0.02197966\n",
      "    0.16408297 -0.12584046 -0.1386019   0.07589174  0.20966169\n",
      "    0.18329674  0.31699857  0.14092681  0.34682173  0.25501537\n",
      "    0.19476034 -0.05393069  0.15966189  0.01336216  0.13390565\n",
      "   -0.18869466  0.04177725  0.04370454  0.11908283 -0.2585034\n",
      "    0.21242502  0.02514008 -0.0430266   0.04385841  0.11058156\n",
      "    0.01046041  0.27175978 -0.18202016  0.47100025  0.0680682\n",
      "    0.11676119  0.16534516 -0.07660548  0.013252    0.27703136\n",
      "   -0.06634065  0.01436494 -0.09498382  0.16154908  0.04023788\n",
      "    0.03780803 -0.1316984   0.01399719  0.00169931  0.2313382\n",
      "   -0.4022841   0.0690386  -0.08271884  0.12410043 -0.02230471\n",
      "   -0.10697827  0.11794561  0.11777273  0.35682857  0.05260605\n",
      "    0.24599203 -0.06498573  0.16523722  0.00600237  0.21926114\n",
      "   -0.28525153  0.00887202 -0.11545371 -0.00745135  0.2881503\n",
      "    0.11802769  0.15125972  0.01398771  0.02112466 -0.05011209\n",
      "    0.13570249  0.37031457  0.2973641   0.02309279 -0.03838988\n",
      "    0.38698232 -0.10693283]]\n",
      "\n",
      " [[-0.02968955  0.12187734  0.11152194  0.19782642  0.2787378\n",
      "   -0.10395628 -0.16547169  0.12685181 -0.20764074 -0.20192647\n",
      "    0.12377514  0.03422252 -0.00213906 -0.00648019 -0.05626721\n",
      "   -0.14990406 -0.05043937 -0.04187759  0.02308416 -0.14165148\n",
      "    0.2488267   0.31429654  0.2735468  -0.11429849 -0.07154584\n",
      "   -0.07213256 -0.17436817 -0.05520666  0.12733826  0.08017427\n",
      "    0.02822627  0.11920974 -0.29152715  0.09825724  0.21946532\n",
      "   -0.22734934 -0.10131674 -0.01480182 -0.06347358 -0.14688256\n",
      "   -0.10719171  0.13594913  0.22636606 -0.24503398 -0.19384417\n",
      "    0.10454447 -0.05083279  0.0104134   0.0247587  -0.3260322\n",
      "   -0.04679849  0.10539823 -0.11201394  0.1882255  -0.02973371\n",
      "   -0.21118075 -0.02560847 -0.1832989  -0.380332    0.18250474\n",
      "   -0.03827969 -0.16964923  0.11657932  0.11290363  0.0604439\n",
      "   -0.10923247 -0.06723902  0.26661673 -0.2393399  -0.04539824\n",
      "   -0.02268369 -0.10318393 -0.2741948  -0.15610012 -0.04531896\n",
      "    0.13215122 -0.13714224 -0.22483215  0.10991395  0.22072512\n",
      "    0.23638192  0.36291477  0.1891127   0.2843225   0.27723882\n",
      "    0.2133483  -0.03107911  0.09924174  0.05708079  0.11029319\n",
      "   -0.15160593  0.02915544 -0.04020098  0.1882278  -0.22000137\n",
      "    0.11744703  0.02004341  0.02097386  0.11204384  0.2114265\n",
      "   -0.04455564  0.21358621 -0.16690266  0.4769104   0.04518405\n",
      "    0.10801876  0.03745835 -0.08574883  0.08261162  0.2572572\n",
      "   -0.05617619  0.0715704  -0.02371568  0.12225208  0.04876593\n",
      "    0.14933088 -0.16111243 -0.02173728  0.06487814  0.25041887\n",
      "   -0.33327517  0.09099159 -0.09148059  0.11315506  0.04345548\n",
      "   -0.10455153  0.0716245   0.07181205  0.35779488  0.13856806\n",
      "    0.22700185 -0.15351069  0.07166748  0.01164223  0.25097737\n",
      "   -0.3075335   0.07509393 -0.04388953  0.02094218  0.3231617\n",
      "    0.10762964  0.20693749 -0.0204666  -0.00537729  0.02360272\n",
      "    0.14683849  0.3775605   0.3059142   0.04098096 -0.07842924\n",
      "    0.30807057 -0.11177514]]\n",
      "\n",
      " [[ 0.00523402  0.08353767  0.07498982  0.14882407  0.22899674\n",
      "   -0.07210556 -0.14309631  0.16997556 -0.22167797 -0.14120461\n",
      "    0.15338251 -0.06468669 -0.0479571  -0.15536153 -0.11402865\n",
      "   -0.175834   -0.04795213 -0.05017034  0.03909627 -0.19774757\n",
      "    0.27682942  0.2814455   0.2851919  -0.13886198 -0.07230862\n",
      "   -0.08596914 -0.23405671  0.01773991  0.1000191   0.02947672\n",
      "    0.04956008  0.16623843 -0.3102891   0.13600624  0.18671627\n",
      "   -0.1857016  -0.04608416  0.03735828  0.00653257 -0.12475239\n",
      "   -0.04661994  0.13014129  0.3043023  -0.19772051 -0.11961696\n",
      "    0.05905712 -0.03145552  0.00173642  0.02305918 -0.3578921\n",
      "    0.04324504  0.11300687 -0.0205012   0.15786456  0.02471262\n",
      "   -0.24975751 -0.02664744 -0.13866143 -0.35720587  0.11805715\n",
      "   -0.08383171 -0.17524718  0.07640985  0.11072721  0.09406018\n",
      "   -0.10847338 -0.04190031  0.24926043 -0.1630377  -0.05957003\n",
      "   -0.01813671 -0.11520365 -0.2532819  -0.11068353 -0.04549031\n",
      "    0.20032163 -0.12543371 -0.20762411  0.08773449  0.18296969\n",
      "    0.19270507  0.34400877  0.24262539  0.2792338   0.18548983\n",
      "    0.19174275  0.01096376  0.08873249  0.02541535  0.15567136\n",
      "   -0.19051278  0.01868035  0.03964996  0.18946302 -0.23914886\n",
      "    0.13941786  0.00164511  0.05998934 -0.00254259  0.2262589\n",
      "   -0.03276581  0.2856428  -0.18022346  0.40230483  0.03331554\n",
      "    0.13924392 -0.03475899 -0.03372286  0.04967438  0.34108523\n",
      "   -0.07669527  0.09799788 -0.00371899  0.06744059  0.03591296\n",
      "    0.12037461 -0.10072728 -0.05643909  0.05236639  0.21335983\n",
      "   -0.31801257  0.1085249  -0.0612321   0.1352605   0.07448279\n",
      "   -0.14486724  0.05717227  0.1305245   0.36825517  0.11407314\n",
      "    0.17202339 -0.09568956  0.08333176  0.01454929  0.17857522\n",
      "   -0.29964235 -0.0041319   0.0134877   0.12050425  0.31327206\n",
      "    0.07708265  0.21056414 -0.05345871  0.08550581 -0.01755148\n",
      "    0.14503008  0.30574137  0.25942668  0.00181773 -0.06930829\n",
      "    0.2998152  -0.06493308]]\n",
      "\n",
      " [[ 0.03352198  0.02282714  0.04508683  0.06687938  0.16691656\n",
      "   -0.10343031  0.1654056   0.18035905 -0.00105613 -0.08993533\n",
      "   -0.05114833 -0.00998098 -0.09606566 -0.05007082 -0.07057409\n",
      "    0.05563429 -0.12444981 -0.08892366 -0.12209953 -0.04063792\n",
      "    0.12897767  0.19742714  0.18523213  0.06268199  0.01626884\n",
      "   -0.06145012 -0.2652132   0.02311673 -0.0665909   0.00860295\n",
      "    0.05997331 -0.14179203 -0.17712812  0.09128401  0.01058837\n",
      "    0.02422034 -0.08189648 -0.06367146  0.06098295  0.04143266\n",
      "   -0.03199789  0.15446657 -0.0059859  -0.1690913  -0.05525168\n",
      "    0.19845524 -0.06203697 -0.01312026 -0.0764762  -0.20595345\n",
      "   -0.03557464  0.16369699  0.03063821  0.17505348 -0.04677662\n",
      "   -0.32098836 -0.03893715 -0.25427744 -0.16932659 -0.01393047\n",
      "   -0.04192773  0.02910945 -0.12826517  0.11055024  0.11225801\n",
      "   -0.12474284  0.07676579  0.0527433  -0.08189129 -0.14008273\n",
      "   -0.06073409 -0.03028854 -0.08127527 -0.20378906  0.16315615\n",
      "    0.04438771  0.02214094 -0.08850568 -0.06146172  0.16362111\n",
      "    0.26232502  0.21552771  0.02771362  0.1589427   0.24602973\n",
      "    0.0533064   0.0706303   0.02079098 -0.08604557  0.08558228\n",
      "   -0.165268   -0.0304039  -0.18620524  0.17196181 -0.01131187\n",
      "    0.13840626  0.07117265  0.08203575 -0.01242215  0.11558247\n",
      "    0.08034618  0.30410454 -0.06457394  0.33608162 -0.01970885\n",
      "    0.03382255  0.04379609  0.08144747  0.09800401  0.13511835\n",
      "   -0.0106625   0.0500108  -0.04286397  0.01868661  0.12705028\n",
      "    0.08479954 -0.03118984  0.10887962  0.11533704 -0.00127384\n",
      "   -0.22436473  0.1095286   0.01794275 -0.03886364 -0.09114591\n",
      "    0.06983896  0.09578785  0.14382286  0.16997752  0.11268111\n",
      "    0.36739546 -0.23318702  0.10859541 -0.06041432 -0.00253144\n",
      "   -0.05590541  0.05315546 -0.04965837  0.0166823   0.22585326\n",
      "    0.12071086  0.10432982  0.14191182 -0.11595524 -0.09191985\n",
      "    0.22743529  0.25018787  0.01813684  0.12789765 -0.13633543\n",
      "    0.17342669 -0.00151566]]], shape=(4, 1, 152), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.55823529e-01 -4.53936934e-01  2.62330055e-01  4.17594433e-01\n",
      "   1.20587111e-01 -1.62633729e+00  1.29770958e+00  5.05605161e-01\n",
      "  -1.44676232e+00 -1.73273265e-01 -1.23177075e+00 -1.27997720e+00\n",
      "   1.40111053e+00 -9.01132286e-01  1.66135740e+00 -2.39839107e-01\n",
      "   1.78078210e+00  1.14297819e+00 -8.56010914e-02 -6.06207192e-01\n",
      "   3.79374921e-01  1.08012676e+00 -3.41506541e-01 -5.98740399e-01\n",
      "  -2.82593071e-01  2.50816584e-01 -1.76731825e-01 -2.88212538e-01\n",
      "  -5.65832853e-01 -3.93904805e-01 -3.75673801e-01 -5.91326118e-01\n",
      "  -2.65877247e-02 -1.20144755e-01  7.12127209e-01 -7.28204310e-01\n",
      "  -9.13241625e-01  7.49707222e-01  1.09350532e-01  8.42621922e-01\n",
      "  -1.36897826e+00  1.30032229e+00 -3.90133262e-01  4.04217184e-01\n",
      "  -1.08355033e+00 -4.12627846e-01  9.61087108e-01  1.44934386e-01\n",
      "   4.78555501e-01 -2.93581188e-01 -1.67698753e+00  2.31093669e+00\n",
      "   8.80020201e-01  1.84200510e-01  2.28958368e+00  4.07648385e-02\n",
      "  -4.83162314e-01 -1.13738805e-01 -4.63590860e-01 -4.91540581e-01\n",
      "   5.62908173e-01  1.55031592e-01 -3.33455205e-01  4.69186723e-01\n",
      "  -3.56333971e-01 -1.73542309e+00  6.76185012e-01  2.38963887e-01\n",
      "   1.04907537e+00  7.02420652e-01 -1.43971658e+00  7.29751766e-01\n",
      "  -2.26108700e-01 -1.11369371e-01  5.80385923e-01 -6.60176933e-01\n",
      "   1.94002378e+00 -1.59942687e-01 -3.21616948e-01  1.80763769e+00\n",
      "  -5.74129105e-01  9.40569758e-01  1.67048049e+00 -3.24891686e-01\n",
      "  -2.36871004e+00  8.69034469e-01 -5.79485297e-01 -1.12686300e+00\n",
      "   6.94712400e-01 -4.73157585e-01  3.89025867e-01  9.83154058e-01\n",
      "  -4.97766197e-01 -2.31419615e-02 -2.01910421e-01 -6.87755585e-01\n",
      "   1.44455135e-01 -4.58395779e-02  3.43478024e-02  1.25982046e+00\n",
      "   6.11153305e-01 -9.40716088e-01  1.16812098e+00  2.75451034e-01\n",
      "   3.55173588e-01  1.06317174e+00  9.61626530e-01  6.87811255e-01\n",
      "  -1.49968243e+00  6.03711605e-02  4.73719835e-01  1.03462934e+00\n",
      "   4.76725847e-02  6.15215778e-01 -8.20286989e-01 -1.23123085e+00\n",
      "  -1.72309458e-01  2.03145862e-01  9.89561141e-01 -1.19268584e+00\n",
      "  -2.06815749e-02  1.14658102e-01 -2.55565017e-01 -7.74602056e-01\n",
      "   1.79482162e-01 -8.45936537e-01 -1.67789549e-01  3.95386517e-02\n",
      "  -8.17029417e-01 -8.49322140e-01 -9.23886895e-01 -1.42597210e+00\n",
      "   1.66852403e+00 -1.47877419e+00 -1.25183153e+00  8.54305744e-01\n",
      "   1.02954721e+00  1.76134312e+00  1.02984637e-01  7.51158357e-01\n",
      "  -1.06005922e-01 -8.13116610e-01  1.86264563e+00  6.27290845e-01\n",
      "   1.67285109e+00 -6.46442354e-01 -6.05417371e-01  2.95457244e-01\n",
      "   6.55816674e-01 -3.20556879e-01 -5.32024086e-01 -1.89965978e-01]\n",
      " [ 6.06766343e-02 -9.91962790e-01  2.34785318e-01  6.29825711e-01\n",
      "   1.62588820e-01 -9.25828397e-01  1.40250707e+00  5.42300880e-01\n",
      "  -1.87524676e+00 -2.80319899e-01 -1.32335114e+00 -1.12696612e+00\n",
      "   1.26859438e+00 -7.55643964e-01  1.33590293e+00 -5.57416975e-01\n",
      "   1.98907983e+00  8.08261156e-01 -2.09411711e-01 -8.12467456e-01\n",
      "   1.31572199e+00  9.85861957e-01  2.62079120e-01 -5.81630707e-01\n",
      "  -3.48437041e-01  9.94836316e-02 -9.96555761e-02  4.31149542e-01\n",
      "  -3.29191625e-01 -1.74900964e-01 -1.96473420e-01 -7.22730219e-01\n",
      "  -1.88123167e-01 -3.72969627e-01  7.95697749e-01 -5.20991385e-01\n",
      "  -9.07240868e-01  9.81806278e-01 -5.81428558e-02  7.39711642e-01\n",
      "  -1.44630527e+00  9.26845253e-01 -2.05910608e-01  3.75134915e-01\n",
      "  -1.26716495e+00  1.13933384e-02  1.12304783e+00  2.43021369e-01\n",
      "   5.91509223e-01 -9.11858976e-02 -1.82022631e+00  2.33321881e+00\n",
      "   9.28803921e-01  3.12820494e-01  2.47927141e+00 -1.49564952e-01\n",
      "  -3.44476014e-01 -3.06941807e-01 -5.07986307e-01 -6.98121965e-01\n",
      "   1.00840282e+00  4.36626345e-01 -4.17464733e-01  7.81975865e-01\n",
      "  -7.59799719e-01 -1.84731627e+00  6.49294138e-01 -8.38453397e-02\n",
      "   8.95819664e-01  8.36621881e-01 -1.41907549e+00  5.57574809e-01\n",
      "  -1.58406436e-01 -1.91548020e-01  3.95831108e-01 -7.02084661e-01\n",
      "   1.46339703e+00 -9.58643556e-02 -2.40475535e-01  1.30893207e+00\n",
      "  -8.20335746e-01  8.95339549e-01  1.78752506e+00 -3.93826365e-01\n",
      "  -2.36418533e+00  7.66585290e-01 -6.54259741e-01 -1.31837857e+00\n",
      "   7.21288681e-01  2.06444934e-02 -2.26783201e-01  1.04046881e+00\n",
      "  -6.20049536e-01  2.03461379e-01  4.55639631e-01 -8.95654380e-01\n",
      "   7.59999752e-02  5.35425544e-03  2.99420506e-01  1.33190787e+00\n",
      "   1.32194686e+00 -8.81837964e-01  1.07457864e+00  7.48027265e-02\n",
      "   1.46710977e-01  1.13726866e+00  1.36887121e+00  7.03222513e-01\n",
      "  -1.74759018e+00 -1.90016031e-02  4.21290904e-01  6.57201409e-01\n",
      "   1.72553569e-01  7.90769041e-01 -1.17687368e+00 -1.01528597e+00\n",
      "  -1.07854754e-01  2.56086528e-01  7.57050276e-01 -8.70401621e-01\n",
      "   2.03745693e-01 -2.01072305e-01  9.81623828e-02 -8.56228232e-01\n",
      "   4.30512488e-01 -6.09669805e-01 -2.45876312e-01  5.17840564e-01\n",
      "  -9.86279190e-01 -9.89861369e-01 -1.12658238e+00 -1.14051723e+00\n",
      "   1.26359820e+00 -1.59685564e+00 -1.60504603e+00  5.84970057e-01\n",
      "   6.59686387e-01  2.08414459e+00 -2.84346879e-01  1.37565231e+00\n",
      "   2.24645853e-01 -6.72472000e-01  2.05138922e+00  9.14888263e-01\n",
      "   1.59857249e+00 -5.50922811e-01 -5.87762535e-01  5.47285080e-01\n",
      "   3.61303627e-01  1.38524055e-01 -5.01699388e-01 -4.55623835e-01]\n",
      " [ 3.03827375e-01 -9.82471466e-01 -1.56062722e-01  3.82274270e-01\n",
      "   1.55318379e-02 -9.47956920e-01  1.68563128e+00  6.05699301e-01\n",
      "  -1.25641680e+00 -1.48257598e-01 -1.56237221e+00 -1.25626051e+00\n",
      "   1.39858830e+00 -8.16885531e-01  1.22154880e+00 -5.33192277e-01\n",
      "   2.13837385e+00  8.15467775e-01 -1.89670503e-01 -1.09599388e+00\n",
      "   8.02201092e-01  6.97454453e-01 -5.53060889e-01 -1.94870532e-01\n",
      "  -2.32773870e-01  6.75811023e-02  3.06605697e-01  3.46581101e-01\n",
      "  -6.48243427e-01  5.16956747e-02 -1.79876179e-01 -4.76334006e-01\n",
      "  -6.23258948e-03 -8.60999584e-01  1.07456684e+00 -5.00273943e-01\n",
      "  -8.07501316e-01  1.23526907e+00  4.48892593e-01  6.30523205e-01\n",
      "  -9.08412755e-01  1.09695017e+00 -5.07463813e-01  4.87340361e-01\n",
      "  -1.31160796e+00 -1.61294997e-01  1.09607100e+00 -1.56263858e-01\n",
      "   9.31813896e-01  4.07778710e-01 -1.97366261e+00  2.24262476e+00\n",
      "   8.64414215e-01  3.43482316e-01  2.04222703e+00 -3.62378657e-02\n",
      "  -3.50912511e-01  8.10057893e-02 -5.20086288e-01 -4.28051293e-01\n",
      "   8.97408068e-01  1.71037942e-01 -2.45379046e-01  5.32926202e-01\n",
      "  -8.69067192e-01 -1.42226386e+00  1.03729379e+00  1.31487340e-01\n",
      "   1.30286443e+00  1.01891434e+00 -1.05763793e+00  5.21077275e-01\n",
      "  -2.78200358e-01 -3.03166151e-01  4.98853624e-01 -7.66619086e-01\n",
      "   1.64502192e+00 -4.18468237e-01 -8.66753161e-01  1.29489183e+00\n",
      "  -8.21271300e-01  8.18829596e-01  1.26595724e+00 -2.94651330e-01\n",
      "  -2.44775534e+00  6.85685635e-01 -8.28527391e-01 -8.48773718e-01\n",
      "   2.13158309e-01  5.61509281e-02  1.03730045e-01  5.31050742e-01\n",
      "  -4.81060922e-01  1.48279399e-01  2.49769941e-01 -9.06952679e-01\n",
      "  -5.94878197e-03  1.21636070e-01  4.01780665e-01  1.12604368e+00\n",
      "   9.47854519e-01 -4.74084795e-01  1.10598767e+00  2.78085768e-01\n",
      "   1.68488920e-02  4.95032758e-01  1.03349781e+00 -2.23368406e-03\n",
      "  -1.96845078e+00 -4.85264093e-01  9.62565601e-01  8.82144749e-01\n",
      "   2.49889553e-01  8.80532861e-01 -8.65682423e-01 -8.92292261e-01\n",
      "   3.50666940e-02 -2.50720978e-02  8.32689762e-01 -8.71965289e-01\n",
      "   4.36333865e-01  1.95103288e-01 -2.62359470e-01 -8.59489262e-01\n",
      "   3.99513811e-01 -7.89872050e-01  8.24725032e-02  1.18269235e-01\n",
      "  -6.51581168e-01 -9.86066222e-01 -7.90665865e-01 -1.38029516e+00\n",
      "   1.58019900e+00 -1.77463794e+00 -1.15414703e+00  4.45276141e-01\n",
      "   8.10731292e-01  1.77423453e+00  9.11770463e-02  1.62635648e+00\n",
      "   4.23636287e-01 -5.74894369e-01  2.37787461e+00  8.61975670e-01\n",
      "   1.78303957e+00 -3.02520692e-01 -5.58313191e-01  5.18097162e-01\n",
      "   3.54582489e-01  1.16951942e-01 -8.30939054e-01 -5.70866823e-01]\n",
      " [ 4.96123046e-01  5.42387128e-01 -2.85559893e-02  4.33209240e-01\n",
      "   5.19531965e-02 -9.02643502e-01  1.27065206e+00  4.84548301e-01\n",
      "  -1.74112630e+00 -5.54988086e-01 -9.53114778e-02 -1.41253281e+00\n",
      "   1.52127230e+00 -3.15732807e-01  1.02579689e+00  3.41196835e-01\n",
      "   5.25136113e-01  9.63496864e-01 -7.84735203e-01 -1.18820667e+00\n",
      "  -6.82449043e-02  4.54698384e-01 -1.61904275e-01 -7.97595978e-02\n",
      "  -3.20489854e-01  4.53359753e-01 -7.05718338e-01  1.74578905e-01\n",
      "  -1.38113558e+00 -3.76982987e-02 -1.59306395e+00 -9.99602973e-01\n",
      "   9.77724254e-01 -3.34814489e-01  4.80753958e-01 -8.92257690e-01\n",
      "  -1.05030262e+00  1.97654939e+00  1.15740395e+00  4.66258228e-01\n",
      "  -1.09029698e+00  2.27246642e+00  5.38523197e-01  3.17360997e-01\n",
      "  -3.59586656e-01  1.84855059e-01  9.13157523e-01  4.14844155e-02\n",
      "   2.31677175e-01  2.40367070e-01 -1.82650077e+00  1.44161916e+00\n",
      "   1.30526674e+00 -2.59364009e-01  2.00180054e+00 -9.72930074e-01\n",
      "   1.31723142e+00 -6.21415436e-01 -8.23372245e-01 -6.82439804e-01\n",
      "   2.35706776e-01  5.69827557e-01 -6.75339460e-01  1.07636738e+00\n",
      "  -8.10008287e-01 -9.23259914e-01  4.20958132e-01  2.78987318e-01\n",
      "   1.34496939e+00  1.15974104e+00 -7.23971009e-01 -9.14881825e-02\n",
      "  -9.51366574e-02  3.10688168e-01 -4.66724277e-01 -7.81302810e-01\n",
      "   1.69504797e+00  1.49238586e-01  5.64471722e-01  2.59120655e+00\n",
      "  -5.83253324e-01  4.36586440e-02  1.55155754e+00  8.06724548e-01\n",
      "  -3.13825750e+00  1.15590262e+00  3.14346015e-01 -2.44906044e+00\n",
      "   2.08909810e-01 -8.92081141e-01  7.50396848e-02  3.58341396e-01\n",
      "  -1.04001784e+00 -1.30195403e+00 -2.27849782e-02 -1.47799289e+00\n",
      "   6.24131560e-01  9.28124428e-01 -9.08396900e-01  1.60241675e+00\n",
      "   7.53074944e-01 -5.36960363e-01  9.89600897e-01  1.75823891e+00\n",
      "   5.18259823e-01  6.09220266e-01  8.88844609e-01  1.23319376e+00\n",
      "  -1.40368962e+00  7.68982768e-02  2.01900053e+00  1.00959337e+00\n",
      "   1.15110278e-01  6.55404925e-02 -1.03056204e+00 -5.30591011e-01\n",
      "  -3.82682353e-01  6.84521556e-01  4.36718702e-01 -5.75684905e-01\n",
      "   2.73691416e-01 -7.15861917e-01 -6.02934897e-01 -6.17722869e-01\n",
      "   9.62017179e-01 -4.35163617e-01  6.06090665e-01  8.04204464e-01\n",
      "  -8.14186394e-01 -1.00059259e+00 -1.90343022e+00 -1.19161025e-01\n",
      "   9.71875310e-01 -7.22210884e-01  1.85372233e-02  1.65970638e-01\n",
      "   3.52596045e-01  1.52127433e+00 -5.81731141e-01  1.03894591e+00\n",
      "  -1.45359188e-02 -8.53311777e-01  2.84751326e-01  5.33641696e-01\n",
      "   1.12406647e+00  7.10531473e-01 -3.31530213e-01 -1.02981925e-02\n",
      "   2.80932099e-01 -2.61873364e-01 -1.03210449e+00  4.00324702e-01]], shape=(4, 152), dtype=float32)\n",
      "Model: \"transformer\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " encoder (Encoder)           multiple                  4490487   \n",
      "                                                                 \n",
      " decoder (Decoder)           multiple                  7831087   \n",
      "                                                                 \n",
      " dense_29 (Dense)            multiple                  23256     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,344,830\n",
      "Trainable params: 12,344,830\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sample_transformer = Transformer(hyperparams)\n",
    "y = sample_transformer(sample_x)\n",
    "print(y)\n",
    "sample_transformer.summary()\n",
    "del sample_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaptor(tf.keras.layers.Layer):\n",
    "  def __init__(self, nLayers, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    dims = [hyperparams.d_model + round( (d_output - hyperparams.d_model) * (layer+1) / (nLayers) ) for layer in range(nLayers)]\n",
    "    layers = [tf.keras.layers.Dense(dim, activation='relu') for dim in dims]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "  def call(self, x):\n",
    "    x = self.seq(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGroup(tf.keras.Model):\n",
    "  softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "  scalar_product = tf.keras.layers.Dot(axes=(-1, -1))\n",
    "\n",
    "  def __init__(self, bookie, nQueries, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.bookie = bookie\n",
    "    self.nQueries = nQueries\n",
    "    self.bookieBase = Adaptor(5, self.nQueries) # 55555555555555555555555\n",
    "    self.oh_1 = Adaptor(5, self.nQueries)       # 55555555555555555555555\n",
    "    return\n",
    "\n",
    "  def call(self, input):\n",
    "    # print('call, input', input)\n",
    "    # inputs.shape: (batch, d_model)\n",
    "    base = self.bookieBase(input)        # (batch, nQueries)\n",
    "    # print('call, base', base)\n",
    "    stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "    # print('call, stake_p', stake_p)\n",
    "    oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "    # print('call, oh_1_p', oh_1_p)\n",
    "    return (oh_1_p, stake_p)  # (batch, 3), (batch, nQueries) \n",
    "\n",
    "  #------------------------------- call used in UK.B.A.01 -------------------------  \n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)        # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "  #   profit_p = tf.math.multiply(oh_1_p, stake_p)   # (batch, nQueries)\n",
    "  #   return (profit_p, stake_p)  # (batch, nQueries), (batch, nQueries) \n",
    "\n",
    "  #------------------------------- call in UK.B.A.02, this version -----------\n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)             # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   oh_1_p = self.oh_1(base)                  # (batch, nQueries)\n",
    "  #   profit_p = self.scalar_product([oh_1_p, stake_p]) # (batch, 1)\n",
    "  #   return (profit_p, stake_p)  # (batch, 1), (batch, nQueries) \n",
    "  \n",
    "  #------------------------------- call in UK.B.A.03, a futrue version -----------\n",
    "  # # profit_p is free and independent of stake_p\n",
    "  # def call(self, input):\n",
    "  #   # inputs.shape: (batch, d_model)\n",
    "  #   base = self.bookieBase(input)             # (batch, nQueries)\n",
    "  #   stake_p = QGroup.softmax(base)            # (batch, nQueries)\n",
    "  #   profit_p = self.profit(base)              # (batch, 1)\n",
    "  #   return (profit_p, stake_p)  # (batch, 1), (batch, nQueries) \n",
    "\n",
    "  #----- ToDo: replace self.scalar_product layer with a tf scalar product function, for speed.\n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "    # ftGoals:  (batch, 2)\n",
    "    ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "    h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "    h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "    return h\n",
    "\n",
    "  def profit_true(self, ftGoals, odds):\n",
    "    stake_t = self.h_true(ftGoals)  # (batch, nQueries)\n",
    "    oh_1_true = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0  # (batch, nQueries)\n",
    "    profit_t = self.scalar_product([oh_1_true, stake_t])  # (batch, 1)\n",
    "    return profit_t\n",
    "\n",
    "  #-------------------- THis loss is NOT used in action versions.\n",
    "  def loss(self, profit_p, stake_p, ftGoals, odds, rambda):\n",
    "    # profit_p: (batch, 1)\n",
    "    # stake_p:  (batch, nQueries)\n",
    "    # ftGoals:  (batch, 2)\n",
    "    # odds:     (batch, nQueries)\n",
    "    # rambda:   ()\n",
    "    profit_t = self.profit_true(ftGoals, odds)  # (batch, 1)\n",
    "    if MAE_NOT_MSE_LOSS:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.abs(profit_t - profit_p), axis=None) # (), profit dimention.\n",
    "    else:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.pow(profit_t - profit_p, 2.0), axis=None) # (), profit dimention.\n",
    "\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0   # (batch, nQueries)\n",
    "    profit_back = self.scalar_product([oh_1_t, stake_p])    # (batch, 1)\n",
    "    profit_back = tf.reduce_mean(profit_back, axis=None)    # ()\n",
    "    loss = (1.0-rambda) * profit_p_err - profit_back * rambda # ()\n",
    "    return loss # ()\n",
    "  \n",
    "  def oh_1_loss(self, oh_1_p, ftGoals, odds):\n",
    "    # oh_1_p: (bacth, nQueries)\n",
    "    h_true = self.h_true(ftGoals)\n",
    "    oh_1_t = tf.multiply(odds, h_true)  # (batch, nQueries)\n",
    "    if MAE_NOT_MSE_LOSS:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.abs(oh_1_t - oh_1_p), axis=-1) # (batch, ), profit dimention.\n",
    "    else:\n",
    "        profit_p_err = tf.reduce_mean(tf.math.pow(oh_1_t - oh_1_p, 2.0), axis=-1) # (batch, ), profit dimention.\n",
    "    \n",
    "    return profit_p_err # (batch, )\n",
    "\n",
    "  #--------------------------------------- Used as profit_back_with_batch in UK.B.A.01\n",
    "  def profit_eval(self, ftGoals, odds, stake_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    profit_e = QGroup.scalar_product([oh_1_t, stake_p])\n",
    "    return profit_e   # (batch, 1)\n",
    "\n",
    "  #-------------------------------------- The same as 'profit_eval' above --------------\n",
    "  def profit_back_with_batch(self, ftGoals, odds, stake_p):\n",
    "    oh_1_t = tf.math.multiply(odds, self.h_true(ftGoals)) - 1.0\n",
    "    profit_back = self.scalar_product([oh_1_t, stake_p])    # (batch, 1)\n",
    "    return profit_back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3.0, 3.0, 4.0], [0.1, 0.7, 0.3]])\n",
    "one_hot_a = tf.squeeze(tf.one_hot(tf.nn.top_k(a).indices, tf.shape(a)[-1]), axis=1)\n",
    "print(one_hot_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGroup1X2(QGroup):\n",
    "  def __init__(self, bookie, dropout_rate=0.1):\n",
    "    super().__init__(bookie=bookie, nQueries=3, dropout_rate=dropout_rate)\n",
    "    self.qGroupName = bookie\n",
    "\n",
    "  def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup.\n",
    "    # ftGoals:  (batch, 2)\n",
    "    ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "    h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "    h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BettingEPL(tf.keras.Model):\n",
    "  def __init__(self, hyperparams, loss_rambda=1.0, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    self.transformer = Transformer(hyperparams, dropout_rate=dropout_rate)\n",
    "    self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "    self.qGroups = [QGroup1X2(bookie) for bookie in self.bookies]\n",
    "    self.rambda = loss_rambda     #----------------------- Sensitive rambda!!!, Automate optimizing it.\n",
    "    # self.shift_embedding = tf.keras.layers.Embedding(1, hyperparams.m365_size, mask_zero=False, embeddings_initializer = tf.keras.initializers.Ones())\n",
    "    # self.shift = None\n",
    "\n",
    "  def call(self, input):\n",
    "      # print('tf in', input)\n",
    "      x = self.transformer(input)\n",
    "      # print('tf out', x)\n",
    "      outputs = [qGroup(x) for qGroup in self.qGroups]\n",
    "      # print('call', outputs)\n",
    "      # self.shift = tf.squeeze(self.shift_embedding(0))  # squeeze((1, 1)) = ()\n",
    "      return outputs  # [ ( shape: (batch, 1), shape: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "  \n",
    "  def loss(self, y, outputs):   \n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( profit_p: (batch, 1), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, sum[qGropu.nQueries for qGroup in self.qGroups])\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)  # [ shape: (batch, qGroup.nQueries) for qGroup in self.qGroups ]\n",
    "      losses = [qGroup.loss(profit_p, stake_p, ftGoals, odds, self.rambda) for (qGroup, odds, (profit_p, stake_p)) in zip(self.qGroups, odds_by_qGroup, outputs)]\n",
    "      # losses: [()] * nQGroup\n",
    "      losses = tf.stack(losses, axis=0) # (nQGroups,)\n",
    "      loss_value = tf.math.mean_reduce(losses, axis=None)\n",
    "      return loss_value\n",
    "  \n",
    "  def profit_back_over_qGroups(self, y, outputs):\n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1)\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)\n",
    "      profit_back = [qGroup.profit_back_with_batch(ftGoals, odds, stake_p) for (qGroup, odds, (_, stake_p)) in zip(self.qGroups, odds_by_qGroup, outputs)]\n",
    "      # profit_back = [(batch, 1) for _ in self.qGroups]\n",
    "      profit_back = tf.concat(profit_back, axis=-1) # (batch, nQGroups)\n",
    "      return profit_back  # A function of stake_p and truth.\n",
    "  \n",
    "  #---------------------------------- The same as in UK.B.A.01, with a bit of code factorization.\n",
    "  def action_loss(self, y, outputs, qGroup_selectivity, game_selectivity):\n",
    "      # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "      # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "      ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, sum[qGropu.nQueries for qGroup in self.qGroups])\n",
    "      odds_by_qGroup = tf.split(odds, [qQroup.nQueries for qQroup in self.qGroups], axis=-1)  # [ shape: (batch, qGroup.nQueries) for qGroup in self.qGroups ]\n",
    "      oh_1_loss = [qGroup.oh_1_loss(oh_1_p, ftGoals, odds) for (qGroup, odds, (oh_1_p, _)) in zip(self.qGroups, odds_by_qGroup, outputs)] # [(batch,)] * nQGroups\n",
    "      oh_1_loss = tf.stack(oh_1_loss, axis=-1)  # sure (batch, nQGroups)\n",
    "      oh_1_loss = tf.math.reduce_mean(oh_1_loss, axis=None)  # ()\n",
    "\n",
    "      profit_p = [tf.math.reduce_sum(tf.multiply(oh_1_p, stake_p), axis=-1, keepdims=True) for (oh_1_p, stake_p) in outputs]   # [ shape: (batch, 1) for _ self.qGroups ]\n",
    "      profit_p = tf.concat(profit_p, axis=-1) # (batch, nQGroups). A funciton of profit_p.\n",
    "      # print('profits_games -1', profit_p)\n",
    "\n",
    "      least_profit = tf.reduce_min(profit_p, axis=-1)\n",
    "      delta = tf.reduce_max(profit_p, axis=-1) - least_profit\n",
    "      normal_profit = tf.transpose(tf.transpose(profit_p) - least_profit)\n",
    "      normal_profit = tf.transpose(tf.transpose(normal_profit)/(delta + 1e-9)) # no tf.keras.backend.epsilon\n",
    "      is_zero = ((1.0 - tf.reduce_max(normal_profit, axis=1)))\n",
    "      normal_profit = tf.transpose(tf.transpose(normal_profit) + is_zero) # (batch, nQGroups). A function of profit_p     \n",
    "      normal_profit = normal_profit * qGroup_selectivity + 1.0  #  (batch, nQGrous). self.shift #------- normal + shift\n",
    "\n",
    "      profits_games = tf.reduce_sum(profit_p, axis=-1)  # (batch,)\n",
    "      min = tf.reduce_min(profits_games); max = tf.reduce_max(profits_games)\n",
    "      profits_games = (profits_games - min) / (max-min + 1e-12) # (batch,)\n",
    "      profits_games = tf.math.sigmoid( 10.0 * tf.math.pow(profits_games, game_selectivity) - 5.0 ) # (batch,)\n",
    "      normal_profit = tf.multiply( normal_profit, profits_games[:, tf.newaxis] )\n",
    "\n",
    "      profit_back = self.profit_back_over_qGroups(y, outputs) # (batch, nQGroups) # A function of stake_p and truth.\n",
    "\n",
    "      mul = tf.multiply(normal_profit, profit_back)   # (batch, nQGroups)\n",
    "      mean_profit_per_game = tf.math.reduce_mean(mul, axis=None)  # ()\n",
    "      \n",
    "      return  (- mean_profit_per_game + oh_1_loss)\n",
    "\n",
    "  #---------------------------------- The same as in UK.B.A.01, with a bit of code factorization.\n",
    "  def back_test(self, y, outputs):\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( shape: (batch, 1), shape: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs) # (batch, nQGroups)\n",
    "\n",
    "    # find the most-profitable QGroup idx\n",
    "    profits_p = [tf.math.reduce_sum(profit_p, axis=-1, keepdims=True) for (profit_p, _) in outputs]   # [ shape: (batch, 1) for _ self.qGroups ]\n",
    "    profits_p = tf.concat(profits_p, axis=-1) # (batch, len(self.qGroups))    \n",
    "    bestQuery = tf.cast(tf.argmax(profits_p, axis=-1), dtype=tf.int32)   # (batch,)\n",
    "    range = tf.range(bestQuery.shape[0], dtype=tf.int32) # (batch,)\n",
    "    best_idx = tf.stack([range, bestQuery], axis=1) # (batch, 2)\n",
    "    \n",
    "    best_profits_eval = tf.gather_nd(profit_back, best_idx)  # (batch, )\n",
    "    profit_eval_mean = tf.math.reduce_mean(best_profits_eval)\n",
    "    return profit_eval_mean\n",
    "  \n",
    "  def back_test_over_chosen_games_and_qGroups(self, y, outputs):\n",
    "    # Choose (game, qGroup), which is greater than 0.05, or MIN_PROFIT_P_PER_GAME_PER_QGROUP\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_p = [tf.math.multiply(p_p, s_p) for (p_p, s_p) in outputs]  # [ (batch, nQueries) ] * nQGroups\n",
    "    profit_p = [tf.math.reduce_sum(p_p, axis=-1, keepdims=True) for p_p in profit_p] # [(batch, 1)] * nQGroups\n",
    "    profit_p = tf.concat(profit_p, axis=-1)   # (batch, nQGroups))\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs)   # (batch, nQGroups)\n",
    "\n",
    "    best_idx = tf.where(profit_p > MIN_PROFIT_P_PER_GAME_PER_QGROUP)  # (nBettings, 2). nBettings unknown yet.\n",
    "\n",
    "    nBettings = 0\n",
    "    profit_back_mean_per_betting = MIN_PROFIT\n",
    "    if best_idx.shape[0] > 0:\n",
    "      best_profits_back = tf.gather_nd(profit_back, best_idx)   # (nBettings, )\n",
    "      nBettings = best_profits_back.shape[0]\n",
    "      profit_back_mean_per_betting = tf.math.reduce_mean(best_profits_back)\n",
    "    return profit_back_mean_per_betting, nBettings\n",
    "  \n",
    "  def back_test_over_chosen_games_and_qGroups_for_distribution(self, y, outputs, keys):\n",
    "    # Choose (game, qGroup), which is greater than 0.05, or MIN_PROFIT_P_PER_GAME_PER_QGROUP\n",
    "    # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    # outputs: # [ ( oh_1_p: (batch, nQueries), stake_p: (batch, nQueries) ) for _ in self.qGroups ]\n",
    "    profit_p = [tf.math.multiply(p_p, s_p) for (p_p, s_p) in outputs]  # [ (batch, nQueries) ] * nQGroups\n",
    "    profit_p = [tf.math.reduce_sum(p_p, axis=-1, keepdims=True) for p_p in profit_p] # [(batch, 1)] * nQGroups\n",
    "    profit_p = tf.concat(profit_p, axis=-1)   # (batch, nQGroups))\n",
    "    profit_back = self.profit_back_over_qGroups(y, outputs)   # (batch, nQGroups)\n",
    "\n",
    "    profit_back_mean_per_betting_list = []\n",
    "    nBettings_list = []\n",
    "\n",
    "    for key in keys:\n",
    "      best_idx = tf.where(profit_p > key)  # (nBettings, 2). nBettings unknown yet.\n",
    "\n",
    "      nBettings = 0\n",
    "      profit_back_mean_per_betting = MIN_PROFIT\n",
    "      if best_idx.shape[0] > 0:\n",
    "        best_profits_back = tf.gather_nd(profit_back, best_idx)   # (nBettings, )\n",
    "        nBettings = best_profits_back.shape[0]\n",
    "        profit_back_mean_per_betting = tf.math.reduce_mean(best_profits_back)\n",
    "      profit_back_mean_per_betting_list.append(float(profit_back_mean_per_betting))\n",
    "      nBettings_list.append(nBettings)\n",
    "    return profit_back_mean_per_betting_list, nBettings_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[3.0, 3.0, 4.0], [0.1, 0.7, 0.3]])\n",
    "one_hot_a = tf.squeeze(tf.one_hot(tf.nn.top_k(a).indices, tf.shape(a)[-1]), axis=1)\n",
    "print(one_hot_a)\n",
    "# one_hot_a = [[ 0.  0.  1.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2925300e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2925200e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2924500e+05,  4.0000000e+00,  2.2400000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2833200e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832000e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2840700e+05,  4.0000000e+00,  1.7000000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1798800e+05,  3.0000000e+00,  1.7000000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0005800e+05,  1.0000000e+00,  1.6980000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000300e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000200e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0277000e+05,  1.0000000e+00,  4.3970000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1130400e+05,  2.0000000e+00,  4.3960000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1129300e+05,  2.0000000e+00,  4.3940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0079900e+05,  1.0000000e+00,  2.4430000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0078000e+05,  1.0000000e+00,  2.4290000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0075100e+05,  1.0000000e+00,  2.3770000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2875700e+05,  4.0000000e+00,  1.9110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2875600e+05,  4.0000000e+00,  1.9110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0775900e+05,  2.0000000e+00,  1.9060000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1791100e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2926700e+05,  4.0000000e+00,  2.2610000e+03,  3.7000000e+01,\n",
      "          2.9000000e+01, -9.2802173e-01,  9.3258522e-02,  5.6985599e-01,\n",
      "         -8.5993636e-01,  5.9215844e-02,  2.9751456e-01, -8.5993636e-01,\n",
      "         -4.2912181e-02,  2.2942922e-01, -8.2589370e-01,  5.9215844e-02,\n",
      "          9.3258522e-02]],\n",
      "\n",
      "       [[ 1.0006200e+05,  1.0000000e+00,  1.7010000e+03,  4.0000000e+01,\n",
      "          1.0500000e+02, -7.9185104e-01,  2.5173169e-02,  3.9964259e-01,\n",
      "         -8.9397907e-01,  2.5173169e-02,  5.6985599e-01, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.5780833e-01, -1.9950849e-01,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0277500e+05,  1.0000000e+00,  4.4030000e+03,  4.4000000e+01,\n",
      "          6.8000000e+01, -7.9185104e-01,  1.6134387e-01,  4.3368527e-01,\n",
      "         -7.9185104e-01,  1.6134387e-01,  5.0177062e-01, -7.2376567e-01,\n",
      "          9.3258522e-02, -4.2912181e-02, -7.5780833e-01,  9.3258522e-02,\n",
      "          2.9751456e-01]],\n",
      "\n",
      "       [[ 1.2875800e+05,  4.0000000e+00,  1.9130000e+03,  1.9000000e+01,\n",
      "          3.7000000e+01, -5.1950961e-01,  2.5173169e-02, -3.8333893e-01,\n",
      "         -5.8759499e-01, -7.6954857e-02, -2.1312556e-01, -5.1950961e-01,\n",
      "         -1.7908289e-01, -3.8333893e-01, -6.5568036e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]]\n",
      "\n",
      " [[1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  ...\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]]\n",
      "\n",
      " [[-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [ 0.04854006 -0.0453826   0.03589982 -0.04845235]\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  ...\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]]\n",
      "\n",
      " [[-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  [-0.02039307 -0.01909922 -0.0185283   0.02771081]\n",
      "  [-0.02039307 -0.01909922 -0.0185283   0.02771081]\n",
      "  ...\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]\n",
      "  [-0.01256289 -0.03302109  0.0056695   0.02791246]]\n",
      "\n",
      " [[-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.02039307 -0.01909922 -0.0185283   0.02771081]\n",
      "  ...\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [-0.00289329  0.03906638  0.0298515  -0.03308399]\n",
      "  [ 0.04854006 -0.0453826   0.03589982 -0.04845235]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[ 0.27308232 -0.13798949 -0.19498414 ...  0.45528594  0.02971879\n",
      "    0.156481  ]\n",
      "  [ 0.18248276 -0.65168107 -0.11309613 ...  0.06298272  0.1411733\n",
      "    0.2720444 ]\n",
      "  [-0.1271941  -0.41053268  0.07350342 ...  0.1726776  -0.00693962\n",
      "    0.4181122 ]\n",
      "  ...\n",
      "  [-0.14619564  0.12444735 -0.31473657 ...  0.5819242  -0.12661949\n",
      "    0.31417906]\n",
      "  [-0.22990775 -0.20225812 -0.14948994 ...  0.44654337 -0.20445862\n",
      "    0.7018085 ]\n",
      "  [-0.12893474 -0.3075125  -0.03632391 ...  0.07230838  0.40483236\n",
      "   -0.14026254]]\n",
      "\n",
      " [[-0.16453259 -0.44701543  0.2085033  ...  0.2444936   0.12976503\n",
      "    0.34916043]\n",
      "  [ 0.20468831  0.04149862  0.14003186 ...  0.52143264 -0.5520585\n",
      "    0.4838265 ]\n",
      "  [-0.2347037  -0.18997382  0.17644644 ...  0.08746368 -0.01445435\n",
      "   -0.10824683]\n",
      "  ...\n",
      "  [-0.18633986  0.04641479 -0.04544964 ...  0.27136517  0.38642985\n",
      "   -0.15717003]\n",
      "  [-0.12239756  0.09649057 -0.11820426 ...  0.5890739  -0.09589955\n",
      "    0.16113165]\n",
      "  [-0.10434492 -0.03078774 -0.04299887 ...  0.3771347   0.0392816\n",
      "    0.56559086]]\n",
      "\n",
      " [[ 0.14719343  0.0058962   0.3437075  ...  0.5512866  -0.19025502\n",
      "    0.18592837]\n",
      "  [-0.47514525  0.44031763 -0.02956858 ...  0.21705133  0.38500407\n",
      "    0.53715426]\n",
      "  [ 0.29320604 -0.21332578 -0.1904378  ...  0.72851557 -0.5787077\n",
      "    1.140282  ]\n",
      "  ...\n",
      "  [-0.30656338 -0.57314444 -0.15132496 ...  0.10371806  0.19773157\n",
      "    0.26288328]\n",
      "  [-0.500103   -0.2338316  -0.21730912 ...  0.13015008  0.748419\n",
      "   -0.383836  ]\n",
      "  [-0.3125146  -0.5349866   0.04509719 ... -0.02109763  0.33188283\n",
      "   -0.10092718]]\n",
      "\n",
      " [[ 0.1114345   0.06383678 -0.22752358 ...  0.73173445 -0.4104058\n",
      "    0.4921479 ]\n",
      "  [-0.25369084 -0.5148263   0.21105191 ... -0.07648533 -0.33764425\n",
      "    0.31667167]\n",
      "  [ 0.29323876 -0.160375   -0.1631036  ...  0.33182997  0.15544486\n",
      "    0.1465564 ]\n",
      "  ...\n",
      "  [-0.37516314 -0.31127492 -0.15330419 ...  0.44653893 -0.20446211\n",
      "    0.7018058 ]\n",
      "  [-0.17434484 -0.53681964 -0.12780832 ...  0.10989106  0.50645614\n",
      "   -0.15422814]\n",
      "  [-0.21453016 -0.49672955 -0.03436546 ... -0.26583663  0.39886707\n",
      "    0.28273112]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[ 0.27308232 -0.13798949 -0.19498414 ...  0.45528594  0.02971879\n",
      "    0.156481  ]\n",
      "  [ 0.18248276 -0.65168107 -0.11309613 ...  0.06298272  0.1411733\n",
      "    0.2720444 ]\n",
      "  [-0.1271941  -0.41053268  0.07350342 ...  0.1726776  -0.00693962\n",
      "    0.4181122 ]\n",
      "  ...\n",
      "  [-0.14619564  0.12444735 -0.31473657 ...  0.5819242  -0.12661949\n",
      "    0.31417906]\n",
      "  [-0.22990775 -0.20225812 -0.14948994 ...  0.44654337 -0.20445862\n",
      "    0.7018085 ]\n",
      "  [-0.12893474 -0.3075125  -0.03632391 ...  0.07230838  0.40483236\n",
      "   -0.14026254]]\n",
      "\n",
      " [[-0.16453259 -0.44701543  0.2085033  ...  0.2444936   0.12976503\n",
      "    0.34916043]\n",
      "  [ 0.20468831  0.04149862  0.14003186 ...  0.52143264 -0.5520585\n",
      "    0.4838265 ]\n",
      "  [-0.2347037  -0.18997382  0.17644644 ...  0.08746368 -0.01445435\n",
      "   -0.10824683]\n",
      "  ...\n",
      "  [-0.18633986  0.04641479 -0.04544964 ...  0.27136517  0.38642985\n",
      "   -0.15717003]\n",
      "  [-0.12239756  0.09649057 -0.11820426 ...  0.5890739  -0.09589955\n",
      "    0.16113165]\n",
      "  [-0.10434492 -0.03078774 -0.04299887 ...  0.3771347   0.0392816\n",
      "    0.56559086]]\n",
      "\n",
      " [[ 0.14719343  0.0058962   0.3437075  ...  0.5512866  -0.19025502\n",
      "    0.18592837]\n",
      "  [-0.47514525  0.44031763 -0.02956858 ...  0.21705133  0.38500407\n",
      "    0.53715426]\n",
      "  [ 0.29320604 -0.21332578 -0.1904378  ...  0.72851557 -0.5787077\n",
      "    1.140282  ]\n",
      "  ...\n",
      "  [-0.30656338 -0.57314444 -0.15132496 ...  0.10371806  0.19773157\n",
      "    0.26288328]\n",
      "  [-0.500103   -0.2338316  -0.21730912 ...  0.13015008  0.748419\n",
      "   -0.383836  ]\n",
      "  [-0.3125146  -0.5349866   0.04509719 ... -0.02109763  0.33188283\n",
      "   -0.10092718]]\n",
      "\n",
      " [[ 0.1114345   0.06383678 -0.22752358 ...  0.73173445 -0.4104058\n",
      "    0.4921479 ]\n",
      "  [-0.25369084 -0.5148263   0.21105191 ... -0.07648533 -0.33764425\n",
      "    0.31667167]\n",
      "  [ 0.29323876 -0.160375   -0.1631036  ...  0.33182997  0.15544486\n",
      "    0.1465564 ]\n",
      "  ...\n",
      "  [-0.37516314 -0.31127492 -0.15330419 ...  0.44653893 -0.20446211\n",
      "    0.7018058 ]\n",
      "  [-0.17434484 -0.53681964 -0.12780832 ...  0.10989106  0.50645614\n",
      "   -0.15422814]\n",
      "  [-0.21453016 -0.49672955 -0.03436546 ... -0.26583663  0.39886707\n",
      "    0.28273112]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[ 0.01834944 -0.00077932 -0.00605946  0.01935948]]\n",
      "\n",
      " [[ 0.03815344  0.03644428 -0.02380248 -0.02921872]]\n",
      "\n",
      " [[ 0.03815344  0.03644428 -0.02380248 -0.02921872]]\n",
      "\n",
      " [[ 0.01834944 -0.00077932 -0.00605946  0.01935948]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[ 1.31208912e-01  1.28820725e-03  2.04192519e-01  6.65997341e-02\n",
      "    2.34656166e-02 -1.87768430e-01  2.82916665e-01 -1.74474493e-01\n",
      "   -6.04804084e-02  3.45118552e-01  5.79635613e-04 -1.25587314e-01\n",
      "    2.69739926e-02  2.90305078e-01  3.01866293e-01 -1.96401194e-01\n",
      "   -2.17421893e-02 -1.42962307e-01 -2.40822151e-01 -3.93808596e-02\n",
      "   -1.09749921e-01 -5.01319580e-02  2.12867752e-01 -4.78101075e-02\n",
      "    2.29733676e-01  3.04812580e-01  1.17998533e-01 -6.54470250e-02\n",
      "    1.63226277e-02 -1.80161834e-01  2.31496602e-01 -1.93149209e-01\n",
      "   -2.05893680e-01  5.11846021e-02  1.08097911e-01 -2.33089760e-01\n",
      "   -8.96264762e-02 -1.48191050e-01 -3.25336568e-02 -2.68597215e-01\n",
      "   -2.34988511e-01  2.26338163e-01 -5.07866293e-02  2.36879870e-01\n",
      "   -1.07106611e-01 -6.01342618e-02 -2.48219579e-01 -1.50856763e-01\n",
      "    9.18379519e-05  1.47938654e-01  9.11003277e-02  9.33117196e-02\n",
      "    8.60314667e-02 -1.32750854e-01  3.95585626e-01  1.47293448e-01\n",
      "    1.61840856e-01  1.25226706e-01  6.02036342e-03  1.76027268e-02\n",
      "   -4.73223627e-05  2.26756036e-01  1.38928592e-02  5.48872463e-02\n",
      "    3.88625205e-01  3.97139825e-02  2.81416886e-02 -1.40981242e-01\n",
      "   -3.89401615e-01 -2.07427204e-01  2.43703127e-01 -3.05175874e-02\n",
      "   -7.27704987e-02 -2.26754516e-01 -4.17522043e-02 -9.05743167e-02\n",
      "   -2.86467373e-03  1.23931840e-02  2.10717097e-02  2.34267712e-01\n",
      "    1.43957123e-01 -8.86271670e-02  4.19808596e-01 -6.20611235e-02\n",
      "   -8.23094323e-02  4.31404501e-01  3.29611301e-02  4.45710301e-01\n",
      "   -1.92080796e-01  4.85944629e-01 -1.31856859e-01  2.95125067e-01\n",
      "    2.86899358e-01  6.15284219e-02  3.29521656e-01 -1.46528184e-01\n",
      "    4.59290408e-02 -1.09695569e-02  1.54841170e-02 -7.27982745e-02\n",
      "   -2.27276459e-02  2.83389300e-01  1.88614242e-02  2.96059757e-01\n",
      "    2.03072339e-01  9.07795355e-02  2.96060175e-01 -1.40617818e-01\n",
      "    5.75595386e-02  3.29259075e-02  5.89093268e-02  3.66612345e-01\n",
      "    3.04844558e-01  1.77623868e-01 -8.22084323e-02  3.37100297e-01\n",
      "    1.86249241e-02 -1.42849267e-01 -1.10241793e-01  2.70577259e-02\n",
      "    4.04559076e-02  4.53701943e-01  3.09275925e-01  9.06827524e-02\n",
      "    1.96562171e-01 -2.66693532e-03  7.45813549e-02  1.45904914e-01\n",
      "    2.87911892e-01  2.28853226e-01  1.91085003e-02  1.70249969e-01\n",
      "    4.11424637e-02 -5.50444350e-02  1.66928321e-01  7.88970590e-02\n",
      "    7.80014023e-02 -2.08770007e-01 -5.96302822e-02 -1.55200005e-01\n",
      "   -2.63512954e-02 -1.05035461e-01  1.24603875e-01  3.39752883e-01\n",
      "   -1.75318241e-01 -5.41811436e-03  1.44573659e-01  1.81049034e-02\n",
      "    9.81229097e-02  8.56088698e-02  9.99332294e-02  1.51316017e-01]]\n",
      "\n",
      " [[ 2.05618560e-01  2.06001978e-02  1.24200284e-01  1.44712254e-02\n",
      "   -6.16785809e-02 -6.96284622e-02  1.89404830e-01 -1.65132463e-01\n",
      "   -2.48934831e-02  3.10283542e-01 -3.95194478e-02 -1.10681936e-01\n",
      "   -2.69712508e-03  2.31549770e-01  2.64933318e-01 -1.40614182e-01\n",
      "   -6.87549170e-03 -4.67304625e-02 -2.26753354e-01 -1.20370416e-03\n",
      "   -6.16034493e-03 -7.59427920e-02  1.69790030e-01 -7.96020404e-02\n",
      "    2.21057579e-01  2.60386527e-01  5.13222702e-02 -5.53117990e-02\n",
      "   -2.28636488e-02 -2.61286944e-01  1.07978851e-01 -1.16028503e-01\n",
      "   -1.16060987e-01  7.88466260e-02  2.90057026e-02 -1.96251050e-01\n",
      "   -8.72427151e-02 -8.72477442e-02 -7.32790455e-02 -2.76745260e-01\n",
      "   -3.12855840e-01  1.92092344e-01 -7.92778283e-02  3.16798031e-01\n",
      "   -6.35308474e-02  2.36681383e-03 -2.24038765e-01 -5.61332703e-02\n",
      "    1.51401274e-02  1.62379324e-01  8.52286890e-02  5.52740134e-02\n",
      "    9.51525569e-02 -2.12596133e-01  2.90667325e-01  1.21922731e-01\n",
      "    1.39527887e-01  1.86247528e-01 -5.14832400e-02 -3.93419340e-03\n",
      "    2.32155491e-02  2.19063342e-01  5.33117913e-02  2.45040990e-02\n",
      "    2.72207707e-01  7.33983889e-02  9.57005247e-02 -9.21262205e-02\n",
      "   -3.44044149e-01 -1.67170137e-01  2.64909625e-01 -1.22174490e-02\n",
      "   -6.32804185e-02 -1.13625526e-01 -5.06620333e-02 -9.30284932e-02\n",
      "   -7.71887600e-04  1.17586955e-01  1.22285187e-01  1.72632784e-01\n",
      "    1.18212186e-01 -6.57264069e-02  4.13939446e-01 -4.43030223e-02\n",
      "   -5.73692396e-02  4.16273624e-01  3.06259580e-02  3.28601003e-01\n",
      "   -1.23927601e-01  4.32297796e-01 -2.35729337e-01  3.28296661e-01\n",
      "    2.44330853e-01  3.20043936e-02  1.77929968e-01 -1.72786206e-01\n",
      "    9.85043943e-02 -1.45051688e-01  6.10692352e-02 -3.52417976e-02\n",
      "   -5.14156595e-02  2.90444672e-01  1.02412589e-01  3.46443981e-01\n",
      "    1.43944472e-01  1.60626531e-01  3.41035306e-01 -2.19987303e-01\n",
      "    1.35786027e-01  9.14823934e-02  1.53546035e-02  3.92159522e-01\n",
      "    2.85207868e-01  1.35270461e-01 -5.91413006e-02  3.80399585e-01\n",
      "   -2.12424025e-02 -1.30141795e-01 -1.27947450e-01  1.34139284e-01\n",
      "    6.45220950e-02  4.51891184e-01  3.45277846e-01  3.48487236e-02\n",
      "    2.08784729e-01  8.10967013e-02  9.05651003e-02  5.02233729e-02\n",
      "    2.68638998e-01  2.05824777e-01  3.52022946e-02  1.31715089e-01\n",
      "    1.41069323e-01 -1.18018761e-02  2.76041448e-01  1.57062411e-01\n",
      "    1.33505657e-01 -1.85142219e-01 -2.98506990e-02 -1.43438309e-01\n",
      "   -4.84096259e-03 -6.27788827e-02  1.36388585e-01  4.11577702e-01\n",
      "   -9.66854617e-02  4.53518108e-02  8.87094066e-02  1.34464264e-01\n",
      "    7.73950145e-02  1.55015215e-01  2.14993656e-02  4.38285396e-02]]\n",
      "\n",
      " [[ 1.25382200e-01  6.02281168e-02  2.06240654e-01  9.81114432e-02\n",
      "   -1.54385883e-02 -1.59649253e-01  2.35476673e-01 -1.16556428e-01\n",
      "   -1.09244466e-01  2.77253956e-01  3.24401781e-02 -1.46476701e-01\n",
      "    2.19352283e-02  2.40542382e-01  2.95121014e-01 -1.42177522e-01\n",
      "    3.07474025e-02 -6.96870387e-02 -2.24265203e-01 -2.45682895e-02\n",
      "    5.15892804e-02 -9.11631994e-03  1.65833920e-01 -7.40296170e-02\n",
      "    2.37204805e-01  2.06422567e-01  9.61301401e-02 -1.17937895e-02\n",
      "   -8.76960624e-03 -1.73232704e-01  2.62791693e-01 -1.70644656e-01\n",
      "   -2.09991395e-01  3.06655057e-02 -3.35563123e-02 -1.89026281e-01\n",
      "   -5.79127595e-02 -1.52211010e-01 -9.65518281e-02 -2.72086024e-01\n",
      "   -2.25820899e-01  2.51731455e-01 -1.19365454e-01  2.41993397e-01\n",
      "   -4.59130816e-02 -1.45964716e-02 -2.20207706e-01 -1.23282366e-01\n",
      "    8.69554933e-03  8.96350443e-02  4.25368398e-02  8.62038136e-02\n",
      "    4.35140729e-02 -1.07808322e-01  3.22104424e-01  1.65435135e-01\n",
      "    4.27598655e-02  2.00445682e-01 -1.82491504e-02 -1.43113080e-02\n",
      "    7.61422515e-03  2.39449218e-01  6.83118179e-02  4.48000729e-02\n",
      "    2.15955302e-01  3.84231135e-02  6.86367080e-02 -1.28527656e-01\n",
      "   -3.31677973e-01 -1.25468925e-01  3.09100837e-01 -5.05536124e-02\n",
      "   -1.01611696e-01 -8.53680670e-02 -9.28074196e-02  6.12047501e-03\n",
      "    2.05811821e-02  1.46561027e-01  1.67520985e-01  1.97449893e-01\n",
      "    2.02060133e-01 -2.61996239e-02  3.91516358e-01 -4.52047661e-02\n",
      "   -1.40216738e-01  3.80669564e-01  5.69053255e-02  4.35094893e-01\n",
      "   -1.55614823e-01  4.00825202e-01 -1.73643768e-01  3.18311960e-01\n",
      "    2.59887308e-01  8.54397863e-02  2.64406770e-01 -1.15067936e-01\n",
      "    1.21087223e-01 -7.13023022e-02  1.01143010e-01 -4.09090519e-02\n",
      "   -3.39648947e-02  2.20150620e-01  4.06985134e-02  2.92167604e-01\n",
      "    2.77352184e-01  1.77192986e-01  2.85132527e-01 -1.82544291e-01\n",
      "    1.61493897e-01  2.00643837e-02 -1.20884553e-02  2.48323143e-01\n",
      "    2.82795966e-01  1.82931930e-01 -8.08629766e-02  3.97650242e-01\n",
      "    7.74882808e-02 -1.61681831e-01 -1.49067134e-01  8.20430219e-02\n",
      "    1.18210390e-02  4.47052062e-01  2.50826269e-01  4.98412438e-02\n",
      "    1.25047326e-01  5.36847562e-02  7.54505247e-02  6.34680092e-02\n",
      "    2.91475952e-01  2.04414710e-01  5.60624748e-02  1.48667857e-01\n",
      "    5.41354790e-02 -3.07003707e-02  2.66492665e-01  6.37454242e-02\n",
      "    1.56314969e-01 -9.08642039e-02 -5.73531166e-02 -1.55120879e-01\n",
      "   -1.23881102e-02 -7.60205761e-02  4.02551815e-02  3.45870912e-01\n",
      "   -1.15357630e-01 -4.12344187e-02  1.68818593e-01 -2.45921835e-02\n",
      "    7.15407580e-02  1.38384566e-01  4.61404622e-02  7.54977167e-02]]\n",
      "\n",
      " [[ 8.62458348e-02 -1.04049388e-02 -5.12383133e-02  9.55928192e-02\n",
      "   -2.63756633e-01 -1.75886586e-01  1.71125591e-01 -1.40288725e-01\n",
      "   -1.15867570e-01  2.38018364e-01  6.43629432e-02 -6.33887723e-02\n",
      "    1.37288421e-02  5.84351867e-02  7.92718828e-02 -1.85184255e-01\n",
      "    2.52617188e-02 -1.01070002e-01 -7.45890662e-02 -1.56037167e-01\n",
      "    6.00066110e-02  4.31739260e-03  3.30800593e-01 -1.32489549e-02\n",
      "    1.70638829e-01  1.29406378e-01  1.37422070e-01 -5.24725467e-02\n",
      "   -6.29851669e-02 -6.41166270e-02  5.12509607e-02 -1.71836182e-01\n",
      "   -7.16365203e-02 -4.49341796e-02  1.23357482e-01 -2.46721610e-01\n",
      "   -4.98554036e-02  1.92080848e-02  6.63033128e-03 -9.40211043e-02\n",
      "   -2.11485714e-01  2.60762244e-01 -1.42196953e-01  1.01583175e-01\n",
      "    7.06817284e-02 -1.86503641e-02 -1.45894259e-01 -1.04003713e-01\n",
      "   -7.80055225e-02  3.06772999e-02  1.04322203e-01 -2.74462029e-02\n",
      "   -9.25586596e-02 -2.60040224e-01  1.38049930e-01  9.61474925e-02\n",
      "    1.76056877e-01 -2.61069834e-02  5.09223752e-02  9.85986367e-03\n",
      "   -9.30096023e-03  1.58818737e-02  3.94442603e-02  9.95321348e-02\n",
      "    1.18268095e-01  1.17411114e-01  8.20294470e-02 -1.63429260e-01\n",
      "   -2.10166320e-01 -3.12239304e-03  9.00001973e-02 -2.71915924e-03\n",
      "    2.10136369e-01 -1.06549650e-01  9.40005481e-02 -3.04849795e-03\n",
      "    5.56468368e-02  8.07196721e-02  1.94700688e-01  2.75907278e-01\n",
      "    4.76756468e-02 -2.47651860e-02  2.29394525e-01 -1.17679603e-01\n",
      "    8.09885636e-02  1.81602746e-01  1.14051059e-01  2.03689173e-01\n",
      "    1.35585964e-02  2.75732070e-01  1.02611594e-01  5.99557236e-02\n",
      "    1.52898937e-01 -6.20133355e-02  7.50932768e-02  5.80284968e-02\n",
      "    1.55326232e-01 -9.04970244e-02  3.52113135e-02  1.54156238e-01\n",
      "   -1.34867281e-02  2.72519618e-01  3.18814367e-02  1.05311409e-01\n",
      "    6.69444352e-02  1.29104987e-01  2.89010733e-01 -1.01978548e-01\n",
      "   -2.37716362e-02 -2.03946978e-02  4.26070765e-02  4.25711334e-01\n",
      "    1.35546356e-01  1.04814097e-01  3.90697792e-02  1.16649784e-01\n",
      "   -1.89311236e-01 -3.56226191e-02 -7.94465020e-02  1.98882684e-01\n",
      "    8.38608220e-02  2.35149592e-01  3.80146205e-02  2.23603755e-01\n",
      "    2.40983069e-01  4.57741693e-02  5.81022799e-02 -6.57312945e-02\n",
      "    1.10008493e-01  8.32656026e-02  1.50149897e-01  6.87297955e-02\n",
      "    1.30718112e-01  2.85876282e-02  3.28492016e-01  1.03416018e-01\n",
      "    2.33411789e-03 -2.63785571e-03  7.39251226e-02 -9.76527482e-03\n",
      "    1.33910626e-01 -7.14527145e-02  2.21733540e-01  4.37133998e-01\n",
      "    3.59724909e-02  2.18128979e-01  2.16524273e-01  1.46454871e-01\n",
      "    1.44276455e-01  1.37490377e-01  1.12486839e-01  1.36119500e-02]]], shape=(4, 1, 152), dtype=float32)\n",
      "(4, 3) (4, 3)\n",
      "Model: \"betting_epl\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " transformer_1 (Transformer)  multiple                 12344830  \n",
      "                                                                 \n",
      " q_group1x2 (QGroup1X2)      multiple                  57932     \n",
      "                                                                 \n",
      " q_group1x2_1 (QGroup1X2)    multiple                  57932     \n",
      "                                                                 \n",
      " q_group1x2_2 (QGroup1X2)    multiple                  57932     \n",
      "                                                                 \n",
      " q_group1x2_3 (QGroup1X2)    multiple                  57932     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,576,558\n",
      "Trainable params: 12,576,558\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPL = BettingEPL(hyperparams, loss_rambda = LOSS_RAMBDA, dropout_rate=TRANSFORMER_DROP)\n",
    "\n",
    "x = (sequence, base_bb, mask)\n",
    "y = EPL(sample_x, training=True)\n",
    "# print(y)\n",
    "(profit_p, stake_p) = y[0]\n",
    "print(profit_p.shape, stake_p.shape)\n",
    "# print(profit_p, stake_p)   # profit_p tend to have the same sign in the same batch.\n",
    "# shift = tf.squeeze(EPL.layers[-1].get_weights()).numpy()\n",
    "# print('shift', shift)\n",
    "\n",
    "EPL.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_rate = CustomSchedule(hyperparams.d_model)\n",
    "\n",
    "learning_rate = LEARNING_RATE\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.95, beta_2=0.95, epsilon=1e-9)\n",
    "# optimizer = tf.keras.optimizers.Adadelta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def masked_loss_uk(label, y_pred):\n",
    "  # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch, 1)), y_pred: (batch, 3)\n",
    "  y_true = label[0]   # one_hot: (batch, 3)\n",
    "  seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "  mask = y_true != 0 \n",
    "  loss = loss_object(y_true, y_pred)\n",
    "  mask = tf.cast(mask, dtype=loss.dtype)\n",
    "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask) # eq. sum_loss / batch\n",
    "  return loss\n",
    "\n",
    "\n",
    "class recall():\n",
    "  def __init__(self, name='recall', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    # print('recall', y_true.shape, y_pred.shape, seq_len_mask.shape)\n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    # print('recall', true_positives.numpy())\n",
    "    possible_positives = tf.math.reduce_sum(y_true)\n",
    "    recall_keras = true_positives / (possible_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall_keras.numpy() / self.n\n",
    "\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = 0.0\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, name='precision', min_seq_len=5, **kwargs):\n",
    "    self.min_seq_len = min_seq_len\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update_state(self, label, y_pred):\n",
    "    # lable = (target(batch, 3), base_bb(batch, 1, 9), seq_len(batch,)), y_pred: (batch, 3)\n",
    "    y_true = label[0]   # one_hot: (batch, 3)\n",
    "    seq_len = label[2]  # (batch, 1)\n",
    "\n",
    "    seq_len_mask = tf.cast(seq_len >= self.min_seq_len, dtype=tf.float32)[:, tf.newaxis, tf.newaxis]\n",
    "    y_true = y_true * seq_len_mask\n",
    "    y_pred = y_pred * seq_len_mask \n",
    "\n",
    "    true_positives = tf.math.reduce_sum(y_true * y_pred)\n",
    "    predicted_positives = tf.math.reduce_sum(y_pred)\n",
    "    precision_keras = true_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision_keras.numpy() / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = 0.0\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def train_step(x, y, qGroup_selectivity, game_selectivity):\n",
    "    print('train step in', x)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = EPL(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        print('train step out', outputs)\n",
    "        loss_value = EPL.action_loss(y, outputs, qGroup_selectivity, game_selectivity)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, EPL.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, EPL.trainable_weights))\n",
    "    # recall_object.update_state(y, logits)\n",
    "    # precision_object.update_state(y, logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "def test_step(x, y, qGroup_selectivity, game_selectivity):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_value = EPL.action_loss(y, outputs, qGroup_selectivity, game_selectivity)\n",
    "    # recall_object.update_state(y, val_logits)\n",
    "    # precision_object.update_state(y, val_logits)\n",
    "    return loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  # gives a wrong result of tf.where(profit_p > MIN_PROFIT_P_PER_GAME_PER_QGROUP)\n",
    "def back_test_step(x, y):\n",
    "    outputs = EPL(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    profit_back_mean_per_betting, nBettings = EPL.back_test_over_chosen_games_and_qGroups(y, outputs)\n",
    "    return profit_back_mean_per_betting, nBettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def back_test_with_dataset(datsset):\n",
    "    profit_back_mean = 0.0\n",
    "    nBettingsTotal = 0\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        profit_back_mean_per_betting, nBettings = back_test_step(x, y)\n",
    "        # print('back_test_with_dataset', profit_back_mean_per_betting, nBettings)\n",
    "        if nBettings > 0:\n",
    "            profit_back_mean = (profit_back_mean * nBettingsTotal + profit_back_mean_per_betting * nBettings) / (nBettingsTotal + nBettings)\n",
    "            nBettingsTotal = nBettingsTotal + nBettings\n",
    "    return profit_back_mean, nBettingsTotal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Wierd: no work.\n",
    "def test_with_dataset(datsset, qGroup_selectivity, game_selectivity):\n",
    "    n = 0\n",
    "    val_loss = tf.Variable(0.0, dtype=tf.float32)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(datsset):\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        n += 1\n",
    "        val_loss = val_loss * (n-1) / n + test_step(x, y, qGroup_selectivity, game_selectivity) / n   ###\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self):\n",
    "        self.history = {'loss': [], 'val_loss': [], 'back100': [], 'nBettings': []}\n",
    "    def save(self, path):\n",
    "        data_helpers.SaveJsonData(self.history, path)\n",
    "    def load(self, path):\n",
    "        self.history = data_helpers.LoadJsonData(path)\n",
    "        if self.history is None:\n",
    "            self.history = {'loss': [], 'val_loss': [], 'back100': [], 'nBettings': []}\n",
    "    def to_back100(self, back):\n",
    "        return float(back if back >= 0 else back)\n",
    "    def append(self, loss, val_loss, back, nBettings):\n",
    "        self.history['loss'].append(self.round_sig(float(loss), 4))\n",
    "        self.history['val_loss'].append(self.round_sig(float(val_loss), 4))\n",
    "        self.history['back100'].append(self.round_sig(self.to_back100(back), 4))\n",
    "        self.history['nBettings'].append(int(nBettings))\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['back100'])\n",
    "        assert len(self.history['loss']) == len(self.history['nBettings'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_latest_item(self):\n",
    "        return (self.history['loss'][-1], self.history['val_loss'][-1], self.history['back100'][-1], self.history['nBettings'][-1])\n",
    "    def get_max_back(self):\n",
    "        return float('-inf') if self.len() <= 0 else max(self.history['back100'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_steps(epoch, step, loss, samples_seen):\n",
    "    # recall = recall_object.result()\n",
    "    # precision = precision_object.result()\n",
    "    # print(\"epoch: {}, step: {}, loss: {}, recall: {}, precision: {}, samples_seen: {}\".\n",
    "    #       format(epoch, step, float(loss_value), recall, precision, (step + 1) * hyperparams.batch_size))\n",
    "    print(\"epoch: {}, step: {}, loss: {}, samples_seen: {}          \".\n",
    "            format(epoch, step, float(loss), samples_seen), end='\\r')\n",
    "    # recall_object.reset()\n",
    "    # precision_object.reset()\n",
    "\n",
    "def show_history(history, baseline=0):\n",
    "    plt.figure(figsize=(15,6))\n",
    "\n",
    "    loss = history.history['loss'][baseline:]\n",
    "    val_loss = history.history['val_loss'][baseline:]\n",
    "    losses = loss + val_loss\n",
    "    back = [b100 * 100 for b100 in history.history['back100']][baseline:]\n",
    "    nBettings = history.history['nBettings'][baseline:]\n",
    "    minBack = min(back) if history.len() > 0 else 0.0; maxBack = max(back) if history.len() > 0 else 1.0\n",
    "    minLosses = min(losses) if history.len() > 0 else 0.0; maxLosses = max(losses) if history.len() > 0 else 1.0\n",
    "    loss = [(elem - minLosses) / (maxLosses-minLosses+1e-9) * (maxBack-minBack) + minBack for elem in loss]\n",
    "    val_loss = [(elem - minLosses) / (maxLosses-minLosses+1e-9) * (maxBack-minBack) + minBack for elem in val_loss]\n",
    "    minBettings = min(nBettings) if history.len() > 0 else 0.0; maxBettings = max(nBettings) if history.len() > 0 else 1.0\n",
    "    nBettings = [(elem - minBettings) / (maxBettings-minBettings+1e-9) * (maxBack-minBack) + minBack for elem in nBettings]\n",
    "    base = 0.0\n",
    "\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.plot(back)\n",
    "    plt.plot(nBettings, color='y', linewidth=0.5)\n",
    "    \n",
    "    bestBack = max(back) if history.len() - baseline > 0 else -1.0\n",
    "    bestBackIdx = back.index(bestBack) if back.count(bestBack) > 0 else 0\n",
    "\n",
    "    all = loss + val_loss + back + nBettings\n",
    "    if len(all) > 0:\n",
    "        ymin = min(all); ymax = max(all); xmin = 0; xmax = history.len() - baseline\n",
    "    else:\n",
    "        ymin = 0.0; ymax = 1.0; xmin = 0.0; xmax = 1.0\n",
    "     \n",
    "    plt.axvline(x=bestBackIdx, ymin=ymin, ymax=ymax, color='r', linewidth=0.3)\n",
    "    plt.axhline(y=bestBack, xmin=xmin, xmax=xmax, color='r', linewidth=0.3)\n",
    "    plt.axhline(y=base, color='b', linestyle='-', linewidth=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.title(TEST_ID + \": Avg profit per betting. max: {}, history len: {}\".format(bestBack, history.len()))\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train_loss', 'val_loss', '100 * val_profit', 'nBettings'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join('./data', 'checkpoints', TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join('./data', 'checkpoints', TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join('./data', 'checkpoints', TEST_ID + '_history.json')\n",
    "\n",
    "history = history_class()\n",
    "\n",
    "if RESET_HISTORY:\n",
    "    files = glob.glob(checkpointPath + \"*\")         # \"*.*\" doesn't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    files = glob.glob(historyPath + \"*\")            # \"*.*\" doens't work\n",
    "    result = [os.remove(file) for file in files]\n",
    "    EPL.save_weights(checkpointPath)\n",
    "    history.save(historyPath)\n",
    "\n",
    "try:\n",
    "    EPL.load_weights(checkpointPath)\n",
    "except:\n",
    "    print('Failed to load model weights.')\n",
    "\n",
    "history.load(historyPath)\n",
    "# if history.len() <= 0:\n",
    "#     print('Creating historic baseline...', end='')\n",
    "#     loss = test_with_dataset(train_batches)\n",
    "#     val_loss = test_with_dataset(test_batches)\n",
    "#     back = back_test_with_dataset(test_batches)\n",
    "#     history.append(loss, val_loss, back)\n",
    "#     history.save(historyPath)\n",
    "#     print('done')\n",
    "\n",
    "def save_checkpoint(loss, val_loss, back, nBettings):\n",
    "    EPL.save_weights(checkpointPath)\n",
    "    max_back = history.get_max_back()\n",
    "    if float(history.to_back100(back)) > max_back:\n",
    "        EPL.save_weights(checkpointPathBest)\n",
    "    history.append(loss, val_loss, back, nBettings)\n",
    "    history.save(historyPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan]\n",
      "[nan]\n",
      "[0.0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "for key, value in history.history.items():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_history(history, baseline=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheduler(epoch):\n",
    "    qGroup_selectivity = epoch / NORM_PP_PATIENCY   # from 0 to infinity, linearly, as epoch grows. 1.0 if epoch == NORM_PP_PATIENCY\n",
    "    game_selectivity = 0.1 + (1.0 - 0.1) * epoch / NORM_PP_PATIENCY   # from 0.1 to infinity, linearly, as epoch grows. 1.0 if epoch == NORM_PP_PATIENCY\n",
    "    return qGroup_selectivity, game_selectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3326300e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.3326200e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3326100e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3306600e+05,  4.0000000e+00,  4.9020000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3306500e+05,  4.0000000e+00,  4.8780000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.3306400e+05,  4.0000000e+00,  4.8780000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3207600e+05,  4.0000000e+00,  4.2640000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2166200e+05,  3.0000000e+00,  4.2640000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2165600e+05,  3.0000000e+00,  4.2640000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.2904300e+05,  4.0000000e+00,  2.1280000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2876700e+05,  4.0000000e+00,  1.9130000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2839800e+05,  4.0000000e+00,  1.6940000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3017900e+05,  4.0000000e+00,  2.9750000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3017000e+05,  4.0000000e+00,  2.9750000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3016500e+05,  4.0000000e+00,  2.9680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1794600e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1793100e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1791000e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0994200e+05,  2.0000000e+00,  3.5210000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0181000e+05,  1.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3094900e+05,  4.0000000e+00,  3.5170000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2875000e+05,  4.0000000e+00,  1.9100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1819100e+05,  3.0000000e+00,  1.8270000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2839200e+05,  4.0000000e+00,  1.6870000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2287200e+05,  3.0000000e+00,  5.0780000e+03,  4.5000000e+01,\n",
      "          9.3000000e+01, -3.1525359e-01,  9.3258522e-02, -1.9269995e-01,\n",
      "         -5.5355233e-01,  2.5173169e-02, -2.8121090e-01, -6.5568036e-01,\n",
      "         -4.2912181e-02, -2.4716823e-01, -4.5142427e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.3208600e+05,  4.0000000e+00,  4.2730000e+03,  3.6000000e+01,\n",
      "          7.5000000e+01, -5.3312671e-01,  1.6134387e-01, -1.9269995e-01,\n",
      "         -5.8759499e-01,  2.5173169e-02, -2.4716823e-01, -6.5568036e-01,\n",
      "         -4.2912181e-02, -2.4716823e-01, -5.8759499e-01,  1.6134387e-01,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.3018400e+05,  4.0000000e+00,  2.9780000e+03,  1.0900000e+02,\n",
      "          7.6000000e+01,  1.6134387e-01,  9.3258522e-02, -7.9185104e-01,\n",
      "          1.2730120e-01,  2.5173169e-02, -7.9185104e-01,  1.6134387e-01,\n",
      "          2.5173169e-02, -8.5993636e-01,  3.9964259e-01,  2.5173169e-02,\n",
      "         -9.0759611e-01]],\n",
      "\n",
      "       [[ 1.0995000e+05,  2.0000000e+00,  3.5280000e+03,  1.8000000e+01,\n",
      "          6.4000000e+01,  5.9215844e-02,  5.9215844e-02, -6.2163764e-01,\n",
      "         -1.7908289e-01,  2.5173169e-02, -6.5568036e-01, -3.8333893e-01,\n",
      "         -4.2912181e-02, -5.8759499e-01, -2.8121090e-01,  2.5173169e-02,\n",
      "         -6.2163764e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3326300e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.3326200e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3326100e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3306600e+05,  4.0000000e+00,  4.9020000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3306500e+05,  4.0000000e+00,  4.8780000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.3306400e+05,  4.0000000e+00,  4.8780000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3207600e+05,  4.0000000e+00,  4.2640000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2166200e+05,  3.0000000e+00,  4.2640000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2165600e+05,  3.0000000e+00,  4.2640000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.2904300e+05,  4.0000000e+00,  2.1280000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2876700e+05,  4.0000000e+00,  1.9130000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2839800e+05,  4.0000000e+00,  1.6940000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3017900e+05,  4.0000000e+00,  2.9750000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3017000e+05,  4.0000000e+00,  2.9750000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3016500e+05,  4.0000000e+00,  2.9680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1794600e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1793100e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1791000e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0994200e+05,  2.0000000e+00,  3.5210000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0181000e+05,  1.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3094900e+05,  4.0000000e+00,  3.5170000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2875000e+05,  4.0000000e+00,  1.9100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1819100e+05,  3.0000000e+00,  1.8270000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2839200e+05,  4.0000000e+00,  1.6870000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2287200e+05,  3.0000000e+00,  5.0780000e+03,  4.5000000e+01,\n",
      "          9.3000000e+01, -3.1525359e-01,  9.3258522e-02, -1.9269995e-01,\n",
      "         -5.5355233e-01,  2.5173169e-02, -2.8121090e-01, -6.5568036e-01,\n",
      "         -4.2912181e-02, -2.4716823e-01, -4.5142427e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.3208600e+05,  4.0000000e+00,  4.2730000e+03,  3.6000000e+01,\n",
      "          7.5000000e+01, -5.3312671e-01,  1.6134387e-01, -1.9269995e-01,\n",
      "         -5.8759499e-01,  2.5173169e-02, -2.4716823e-01, -6.5568036e-01,\n",
      "         -4.2912181e-02, -2.4716823e-01, -5.8759499e-01,  1.6134387e-01,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.3018400e+05,  4.0000000e+00,  2.9780000e+03,  1.0900000e+02,\n",
      "          7.6000000e+01,  1.6134387e-01,  9.3258522e-02, -7.9185104e-01,\n",
      "          1.2730120e-01,  2.5173169e-02, -7.9185104e-01,  1.6134387e-01,\n",
      "          2.5173169e-02, -8.5993636e-01,  3.9964259e-01,  2.5173169e-02,\n",
      "         -9.0759611e-01]],\n",
      "\n",
      "       [[ 1.0995000e+05,  2.0000000e+00,  3.5280000e+03,  1.8000000e+01,\n",
      "          6.4000000e+01,  5.9215844e-02,  5.9215844e-02, -6.2163764e-01,\n",
      "         -1.7908289e-01,  2.5173169e-02, -6.5568036e-01, -3.8333893e-01,\n",
      "         -4.2912181e-02, -5.8759499e-01, -2.8121090e-01,  2.5173169e-02,\n",
      "         -6.2163764e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[2.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1880800e+05,  3.0000000e+00,  2.2310000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1879700e+05,  3.0000000e+00,  2.2230000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0821600e+05,  2.0000000e+00,  2.2230000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1791900e+05,  3.0000000e+00,  1.5600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1790300e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730500e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2129600e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2129200e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2129100e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2939800e+05,  4.0000000e+00,  2.3230000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2921200e+05,  4.0000000e+00,  2.2230000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2893400e+05,  4.0000000e+00,  2.0730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2294100e+05,  3.0000000e+00,  5.1030000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2292800e+05,  3.0000000e+00,  5.0960000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2291800e+05,  3.0000000e+00,  5.0920000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3014900e+05,  4.0000000e+00,  2.9540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3013300e+05,  4.0000000e+00,  2.9500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2988400e+05,  4.0000000e+00,  2.6600000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1944400e+05,  3.0000000e+00,  2.6390000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0884400e+05,  2.0000000e+00,  2.6320000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2983800e+05,  4.0000000e+00,  2.6310000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2836200e+05,  4.0000000e+00,  1.6500000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2834300e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0822800e+05,  2.0000000e+00,  2.2340000e+03,  5.4000000e+01,\n",
      "          7.0000000e+00, -9.0759611e-01,  9.3258522e-02,  7.9453760e-01,\n",
      "         -8.9397907e-01,  5.9215844e-02,  4.3368527e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.8249000e-01,  5.9215844e-02,\n",
      "          7.0602667e-01]],\n",
      "\n",
      "       [[ 1.2130100e+05,  3.0000000e+00,  4.0200000e+03,  4.1000000e+01,\n",
      "          1.0200000e+02, -8.5312784e-01,  2.9751456e-01,  3.9964259e-01,\n",
      "         -8.2589370e-01,  5.9215844e-02,  1.6134387e-01, -8.9397907e-01,\n",
      "          2.5173169e-02,  2.2942922e-01, -8.5312784e-01,  2.9751456e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.2294400e+05,  3.0000000e+00,  5.1080000e+03,  1.6000000e+01,\n",
      "          9.9000000e+01, -8.5312784e-01,  1.6134387e-01,  5.6985599e-01,\n",
      "         -8.5993636e-01,  9.3258522e-02,  3.6559993e-01, -9.2802173e-01,\n",
      "          9.3258522e-02,  2.9751456e-01, -7.9185104e-01,  9.3258522e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.1945500e+05,  3.0000000e+00,  2.6460000e+03,  7.0000000e+01,\n",
      "          1.7000000e+01, -7.2376567e-01,  2.5173169e-02,  2.5173169e-02,\n",
      "         -7.2376567e-01, -4.2912181e-02,  2.5173169e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01, -4.2912181e-02,\n",
      "         -4.2912181e-02]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1880800e+05,  3.0000000e+00,  2.2310000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1879700e+05,  3.0000000e+00,  2.2230000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0821600e+05,  2.0000000e+00,  2.2230000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1791900e+05,  3.0000000e+00,  1.5600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1790300e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730500e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2129600e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2129200e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2129100e+05,  3.0000000e+00,  4.0190000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2939800e+05,  4.0000000e+00,  2.3230000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2921200e+05,  4.0000000e+00,  2.2230000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2893400e+05,  4.0000000e+00,  2.0730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2294100e+05,  3.0000000e+00,  5.1030000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2292800e+05,  3.0000000e+00,  5.0960000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2291800e+05,  3.0000000e+00,  5.0920000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3014900e+05,  4.0000000e+00,  2.9540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3013300e+05,  4.0000000e+00,  2.9500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2988400e+05,  4.0000000e+00,  2.6600000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1944400e+05,  3.0000000e+00,  2.6390000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0884400e+05,  2.0000000e+00,  2.6320000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2983800e+05,  4.0000000e+00,  2.6310000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2836200e+05,  4.0000000e+00,  1.6500000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2834300e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0822800e+05,  2.0000000e+00,  2.2340000e+03,  5.4000000e+01,\n",
      "          7.0000000e+00, -9.0759611e-01,  9.3258522e-02,  7.9453760e-01,\n",
      "         -8.9397907e-01,  5.9215844e-02,  4.3368527e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.8249000e-01,  5.9215844e-02,\n",
      "          7.0602667e-01]],\n",
      "\n",
      "       [[ 1.2130100e+05,  3.0000000e+00,  4.0200000e+03,  4.1000000e+01,\n",
      "          1.0200000e+02, -8.5312784e-01,  2.9751456e-01,  3.9964259e-01,\n",
      "         -8.2589370e-01,  5.9215844e-02,  1.6134387e-01, -8.9397907e-01,\n",
      "          2.5173169e-02,  2.2942922e-01, -8.5312784e-01,  2.9751456e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.2294400e+05,  3.0000000e+00,  5.1080000e+03,  1.6000000e+01,\n",
      "          9.9000000e+01, -8.5312784e-01,  1.6134387e-01,  5.6985599e-01,\n",
      "         -8.5993636e-01,  9.3258522e-02,  3.6559993e-01, -9.2802173e-01,\n",
      "          9.3258522e-02,  2.9751456e-01, -7.9185104e-01,  9.3258522e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.1945500e+05,  3.0000000e+00,  2.6460000e+03,  7.0000000e+01,\n",
      "          1.7000000e+01, -7.2376567e-01,  2.5173169e-02,  2.5173169e-02,\n",
      "         -7.2376567e-01, -4.2912181e-02,  2.5173169e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01, -4.2912181e-02,\n",
      "         -4.2912181e-02]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[3.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3139700e+05,  4.0000000e+00,  3.7620000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3139600e+05,  4.0000000e+00,  3.7620000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3139500e+05,  4.0000000e+00,  3.7590000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3120000e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119900e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119800e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3240400e+05,  4.0000000e+00,  4.4450000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3237500e+05,  4.0000000e+00,  4.4340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3236100e+05,  4.0000000e+00,  4.4310000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1853300e+05,  3.0000000e+00,  2.0790000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1839700e+05,  3.0000000e+00,  1.9480000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1805600e+05,  3.0000000e+00,  1.7500000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0866600e+05,  2.0000000e+00,  2.5410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0865400e+05,  2.0000000e+00,  2.5340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0863900e+05,  2.0000000e+00,  2.5330000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0001700e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001000e+05,  1.0000000e+00,  1.5610000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0000900e+05,  1.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3122700e+05,  4.0000000e+00,  3.6850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119300e+05,  4.0000000e+00,  3.6710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3118900e+05,  4.0000000e+00,  3.6710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2840600e+05,  4.0000000e+00,  1.6940000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837200e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833400e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1037700e+05,  2.0000000e+00,  3.7660000e+03,  7.2000000e+01,\n",
      "          7.7000000e+01, -1.0846181e+00,  4.3368527e-01,  1.9315630e+00,\n",
      "         -1.0982351e+00,  2.2942922e-01,  1.5911362e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  1.1145388e+00, -1.0846181e+00,  3.9964259e-01,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.2200000e+05,  3.0000000e+00,  4.4550000e+03,  9.3000000e+01,\n",
      "          7.4000000e+01, -9.9610710e-01,  3.9964259e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  9.3258522e-02,  8.4219736e-01, -9.6206439e-01,\n",
      "          9.3258522e-02,  4.3368527e-01, -9.6206439e-01,  1.6134387e-01,\n",
      "          1.2507094e+00]],\n",
      "\n",
      "       [[ 1.0093600e+05,  1.0000000e+00,  2.5430000e+03,  4.4000000e+01,\n",
      "          6.4000000e+01, -6.2163764e-01,  2.5173169e-02, -4.2912181e-02,\n",
      "         -7.5780833e-01, -8.8695055e-03,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.2376567e-01, -4.2912181e-02,\n",
      "         -4.2912181e-02]],\n",
      "\n",
      "       [[ 1.3123200e+05,  4.0000000e+00,  3.6860000e+03,  8.3000000e+01,\n",
      "          8.4000000e+01, -8.5312784e-01,  2.2942922e-01,  4.3368527e-01,\n",
      "         -9.2802173e-01,  1.9538654e-01,  3.9964259e-01, -8.9397907e-01,\n",
      "          2.5173169e-02,  2.2942922e-01, -9.2802173e-01,  1.6134387e-01,\n",
      "          2.9751456e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3139700e+05,  4.0000000e+00,  3.7620000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3139600e+05,  4.0000000e+00,  3.7620000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3139500e+05,  4.0000000e+00,  3.7590000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3120000e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119900e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119800e+05,  4.0000000e+00,  3.6750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3240400e+05,  4.0000000e+00,  4.4450000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3237500e+05,  4.0000000e+00,  4.4340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3236100e+05,  4.0000000e+00,  4.4310000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1853300e+05,  3.0000000e+00,  2.0790000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1839700e+05,  3.0000000e+00,  1.9480000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1805600e+05,  3.0000000e+00,  1.7500000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0866600e+05,  2.0000000e+00,  2.5410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0865400e+05,  2.0000000e+00,  2.5340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0863900e+05,  2.0000000e+00,  2.5330000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0001700e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001000e+05,  1.0000000e+00,  1.5610000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0000900e+05,  1.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3122700e+05,  4.0000000e+00,  3.6850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3119300e+05,  4.0000000e+00,  3.6710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3118900e+05,  4.0000000e+00,  3.6710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2840600e+05,  4.0000000e+00,  1.6940000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837200e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833400e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1037700e+05,  2.0000000e+00,  3.7660000e+03,  7.2000000e+01,\n",
      "          7.7000000e+01, -1.0846181e+00,  4.3368527e-01,  1.9315630e+00,\n",
      "         -1.0982351e+00,  2.2942922e-01,  1.5911362e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  1.1145388e+00, -1.0846181e+00,  3.9964259e-01,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.2200000e+05,  3.0000000e+00,  4.4550000e+03,  9.3000000e+01,\n",
      "          7.4000000e+01, -9.9610710e-01,  3.9964259e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  9.3258522e-02,  8.4219736e-01, -9.6206439e-01,\n",
      "          9.3258522e-02,  4.3368527e-01, -9.6206439e-01,  1.6134387e-01,\n",
      "          1.2507094e+00]],\n",
      "\n",
      "       [[ 1.0093600e+05,  1.0000000e+00,  2.5430000e+03,  4.4000000e+01,\n",
      "          6.4000000e+01, -6.2163764e-01,  2.5173169e-02, -4.2912181e-02,\n",
      "         -7.5780833e-01, -8.8695055e-03,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.2376567e-01, -4.2912181e-02,\n",
      "         -4.2912181e-02]],\n",
      "\n",
      "       [[ 1.3123200e+05,  4.0000000e+00,  3.6860000e+03,  8.3000000e+01,\n",
      "          8.4000000e+01, -8.5312784e-01,  2.2942922e-01,  4.3368527e-01,\n",
      "         -9.2802173e-01,  1.9538654e-01,  3.9964259e-01, -8.9397907e-01,\n",
      "          2.5173169e-02,  2.2942922e-01, -9.2802173e-01,  1.6134387e-01,\n",
      "          2.9751456e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2085800e+05,  3.0000000e+00,  3.6940000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3124600e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1020800e+05,  2.0000000e+00,  3.6880000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2846100e+05,  4.0000000e+00,  1.7420000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1799100e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1796600e+05,  3.0000000e+00,  1.6870000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1245100e+05,  2.0000000e+00,  5.1620000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1244400e+05,  2.0000000e+00,  5.1590000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1243800e+05,  2.0000000e+00,  5.1410000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1967900e+05,  3.0000000e+00,  2.9240000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1959500e+05,  3.0000000e+00,  2.8760000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1835300e+05,  3.0000000e+00,  1.9140000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.1874300e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1874100e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1873200e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0732700e+05,  2.0000000e+00,  1.5310000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790500e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.3332400e+05,  4.0000000e+00,  5.0920000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3331100e+05,  4.0000000e+00,  5.0850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3330900e+05,  4.0000000e+00,  5.0850000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3189600e+05,  4.0000000e+00,  4.1300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0243900e+05,  1.0000000e+00,  4.0760000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0175400e+05,  1.0000000e+00,  3.4230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.3126200e+05,  4.0000000e+00,  3.6960000e+03,  3.6000000e+01,\n",
      "          1.4000000e+01, -3.6291334e-01,  9.3258522e-02, -3.6291334e-01,\n",
      "         -5.1950961e-01,  2.5173169e-02, -3.8333893e-01, -4.8546696e-01,\n",
      "         -4.2912181e-02, -4.8546696e-01, -5.3993523e-01, -4.2912181e-02,\n",
      "         -3.6972186e-01]],\n",
      "\n",
      "       [[ 1.1246200e+05,  2.0000000e+00,  5.1660000e+03,  5.0000000e+01,\n",
      "          8.9000000e+01, -6.5568036e-01,  1.6134387e-01,  2.9751456e-01,\n",
      "         -6.8972301e-01, -4.2912181e-02,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -6.5568036e-01,  9.3258522e-02,\n",
      "          9.3258522e-02]],\n",
      "\n",
      "       [[ 1.1874700e+05,  3.0000000e+00,  2.1950000e+03,  1.8000000e+01,\n",
      "          1.1100000e+02, -9.2802173e-01,  9.3258522e-02,  5.6985599e-01,\n",
      "         -8.9397907e-01,  5.9215844e-02,  3.9964259e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.2802173e-01,  9.3258522e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.2297800e+05,  3.0000000e+00,  5.1150000e+03,  4.5000000e+01,\n",
      "          7.8000000e+01, -4.8546696e-01,  2.2942922e-01, -1.1099753e-01,\n",
      "         -6.2163764e-01, -4.2912181e-02, -1.1099753e-01, -5.8759499e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.8546696e-01,  1.6134387e-01,\n",
      "         -2.4716823e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2085800e+05,  3.0000000e+00,  3.6940000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3124600e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1020800e+05,  2.0000000e+00,  3.6880000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2846100e+05,  4.0000000e+00,  1.7420000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1799100e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1796600e+05,  3.0000000e+00,  1.6870000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1245100e+05,  2.0000000e+00,  5.1620000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1244400e+05,  2.0000000e+00,  5.1590000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1243800e+05,  2.0000000e+00,  5.1410000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1967900e+05,  3.0000000e+00,  2.9240000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1959500e+05,  3.0000000e+00,  2.8760000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1835300e+05,  3.0000000e+00,  1.9140000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.1874300e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1874100e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1873200e+05,  3.0000000e+00,  2.1910000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0732700e+05,  2.0000000e+00,  1.5310000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790500e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.3332400e+05,  4.0000000e+00,  5.0920000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3331100e+05,  4.0000000e+00,  5.0850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3330900e+05,  4.0000000e+00,  5.0850000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3189600e+05,  4.0000000e+00,  4.1300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0243900e+05,  1.0000000e+00,  4.0760000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0175400e+05,  1.0000000e+00,  3.4230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.3126200e+05,  4.0000000e+00,  3.6960000e+03,  3.6000000e+01,\n",
      "          1.4000000e+01, -3.6291334e-01,  9.3258522e-02, -3.6291334e-01,\n",
      "         -5.1950961e-01,  2.5173169e-02, -3.8333893e-01, -4.8546696e-01,\n",
      "         -4.2912181e-02, -4.8546696e-01, -5.3993523e-01, -4.2912181e-02,\n",
      "         -3.6972186e-01]],\n",
      "\n",
      "       [[ 1.1246200e+05,  2.0000000e+00,  5.1660000e+03,  5.0000000e+01,\n",
      "          8.9000000e+01, -6.5568036e-01,  1.6134387e-01,  2.9751456e-01,\n",
      "         -6.8972301e-01, -4.2912181e-02,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -6.5568036e-01,  9.3258522e-02,\n",
      "          9.3258522e-02]],\n",
      "\n",
      "       [[ 1.1874700e+05,  3.0000000e+00,  2.1950000e+03,  1.8000000e+01,\n",
      "          1.1100000e+02, -9.2802173e-01,  9.3258522e-02,  5.6985599e-01,\n",
      "         -8.9397907e-01,  5.9215844e-02,  3.9964259e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.2802173e-01,  9.3258522e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.2297800e+05,  3.0000000e+00,  5.1150000e+03,  4.5000000e+01,\n",
      "          7.8000000e+01, -4.8546696e-01,  2.2942922e-01, -1.1099753e-01,\n",
      "         -6.2163764e-01, -4.2912181e-02, -1.1099753e-01, -5.8759499e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.8546696e-01,  1.6134387e-01,\n",
      "         -2.4716823e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[3.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0000000e+05,  1.0000000e+00,  1.4710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[ 1.2126900e+05,  3.0000000e+00,  3.9980000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2126500e+05,  3.0000000e+00,  3.9790000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2125700e+05,  3.0000000e+00,  3.9790000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1880700e+05,  3.0000000e+00,  2.2310000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0781500e+05,  2.0000000e+00,  1.9490000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0758800e+05,  2.0000000e+00,  1.8160000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0108400e+05,  1.0000000e+00,  2.6630000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0108300e+05,  1.0000000e+00,  2.6610000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0107600e+05,  1.0000000e+00,  2.6600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.0002600e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0002200e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001300e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3124200e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3124100e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3123600e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.3085700e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3085600e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3085500e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03,  1.7000000e+01,\n",
      "          8.8000000e+01, -5.1950961e-01,  5.9215844e-02, -3.6972186e-01,\n",
      "         -5.5355233e-01,  5.9215844e-02, -3.4929624e-01, -4.8546696e-01,\n",
      "         -1.1099753e-01, -4.8546696e-01, -3.8333893e-01, -4.2912181e-02,\n",
      "         -5.1950961e-01]],\n",
      "\n",
      "       [[ 1.2127600e+05,  3.0000000e+00,  4.0150000e+03,  1.7000000e+01,\n",
      "          2.6000000e+01, -7.9185104e-01,  2.2942922e-01,  2.9751456e-01,\n",
      "         -8.2589370e-01,  2.5173169e-02,  2.2942922e-01, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -8.2589370e-01,  2.2942922e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0109400e+05,  1.0000000e+00,  2.6670000e+03,  1.0100000e+02,\n",
      "          4.0000000e+00, -3.1525359e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -3.8333893e-01, -8.8695055e-03, -4.5142427e-01, -4.5142427e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.5142427e-01, -4.2912181e-02,\n",
      "         -4.5142427e-01]],\n",
      "\n",
      "       [[ 1.2085300e+05,  3.0000000e+00,  3.6940000e+03,  2.5000000e+01,\n",
      "          1.0200000e+02, -9.7568148e-01,  2.9751456e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  1.6134387e-01,  6.0389864e-01, -9.2802173e-01,\n",
      "          2.5173169e-02,  3.6559993e-01, -9.2802173e-01,  2.2942922e-01,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 0, ..., 0, 0, 0],\n",
      "        [1, 1, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0000000e+05,  1.0000000e+00,  1.4710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        ...,\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "        [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00, ...,\n",
      "          0.0000000e+00,  0.0000000e+00,  0.0000000e+00]],\n",
      "\n",
      "       [[ 1.2126900e+05,  3.0000000e+00,  3.9980000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2126500e+05,  3.0000000e+00,  3.9790000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2125700e+05,  3.0000000e+00,  3.9790000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1880700e+05,  3.0000000e+00,  2.2310000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0781500e+05,  2.0000000e+00,  1.9490000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0758800e+05,  2.0000000e+00,  1.8160000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0108400e+05,  1.0000000e+00,  2.6630000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0108300e+05,  1.0000000e+00,  2.6610000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0107600e+05,  1.0000000e+00,  2.6600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.0002600e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0002200e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001300e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3124200e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3124100e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3123600e+05,  4.0000000e+00,  3.6930000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.3085700e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3085600e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3085500e+05,  4.0000000e+00,  3.3950000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03,  1.7000000e+01,\n",
      "          8.8000000e+01, -5.1950961e-01,  5.9215844e-02, -3.6972186e-01,\n",
      "         -5.5355233e-01,  5.9215844e-02, -3.4929624e-01, -4.8546696e-01,\n",
      "         -1.1099753e-01, -4.8546696e-01, -3.8333893e-01, -4.2912181e-02,\n",
      "         -5.1950961e-01]],\n",
      "\n",
      "       [[ 1.2127600e+05,  3.0000000e+00,  4.0150000e+03,  1.7000000e+01,\n",
      "          2.6000000e+01, -7.9185104e-01,  2.2942922e-01,  2.9751456e-01,\n",
      "         -8.2589370e-01,  2.5173169e-02,  2.2942922e-01, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -8.2589370e-01,  2.2942922e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0109400e+05,  1.0000000e+00,  2.6670000e+03,  1.0100000e+02,\n",
      "          4.0000000e+00, -3.1525359e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -3.8333893e-01, -8.8695055e-03, -4.5142427e-01, -4.5142427e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.5142427e-01, -4.2912181e-02,\n",
      "         -4.5142427e-01]],\n",
      "\n",
      "       [[ 1.2085300e+05,  3.0000000e+00,  3.6940000e+03,  2.5000000e+01,\n",
      "          1.0200000e+02, -9.7568148e-01,  2.9751456e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  1.6134387e-01,  6.0389864e-01, -9.2802173e-01,\n",
      "          2.5173169e-02,  3.6559993e-01, -9.2802173e-01,  2.2942922e-01,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 0, ..., 0, 0, 0],\n",
      "        [1, 1, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0],\n",
      "        [0, 0, 0, ..., 0, 0, 0]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[1.]\n",
      "  [4.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]\n",
      "  [0.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 0 ... 0 0 0]\n",
      "  [1 1 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[3.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2024200e+05,  3.0000000e+00,  3.3040000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2023800e+05,  3.0000000e+00,  3.3030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3063200e+05,  4.0000000e+00,  3.2990000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2843800e+05,  4.0000000e+00,  1.7220000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0742600e+05,  2.0000000e+00,  1.7220000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2842600e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1267400e+05,  2.0000000e+00,  5.3410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1267300e+05,  2.0000000e+00,  5.3410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1265100e+05,  2.0000000e+00,  5.3330000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1947100e+05,  3.0000000e+00,  2.6600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1891300e+05,  3.0000000e+00,  2.2920000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1876700e+05,  3.0000000e+00,  2.2080000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.0231000e+05,  1.0000000e+00,  3.9690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0230700e+05,  1.0000000e+00,  3.9690000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0230200e+05,  1.0000000e+00,  3.9680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0021700e+05,  1.0000000e+00,  1.8360000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0014500e+05,  1.0000000e+00,  1.7850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0011800e+05,  1.0000000e+00,  1.7640000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3217200e+05,  4.0000000e+00,  4.3150000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3215900e+05,  4.0000000e+00,  4.3120000e+03, ...,\n",
      "          3.6071527e+00,  6.7223716e+00, -2.8060716e-01],\n",
      "        [ 1.1113400e+05,  2.0000000e+00,  4.3080000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2834600e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          2.7973680e+00,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.2832800e+05,  4.0000000e+00,  1.5290000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2025500e+05,  3.0000000e+00,  3.3110000e+03,  7.0000000e+01,\n",
      "          3.4000000e+01, -9.8249000e-01,  2.9751456e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  1.2730120e-01,  7.0602667e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.0759611e-01,  2.5173169e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.1268800e+05,  2.0000000e+00,  5.3480000e+03,  1.6000000e+01,\n",
      "          9.0000000e+00, -7.9185104e-01,  2.9751456e-01,  5.6985599e-01,\n",
      "         -8.2589370e-01,  5.9215844e-02,  4.3368527e-01, -7.9185104e-01,\n",
      "          2.5173169e-02,  9.3258522e-02, -7.5780833e-01,  2.5173169e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0232000e+05,  1.0000000e+00,  3.9760000e+03,  1.0500000e+02,\n",
      "          9.5000000e+01, -6.5568036e-01,  2.5173169e-02,  2.2942922e-01,\n",
      "         -7.2376567e-01, -4.2912181e-02,  2.2942922e-01, -6.5568036e-01,\n",
      "          2.5173169e-02, -1.1099753e-01, -7.5780833e-01,  9.3258522e-02,\n",
      "          2.9751456e-01]],\n",
      "\n",
      "       [[ 1.3218400e+05,  4.0000000e+00,  4.3190000e+03,  7.5000000e+01,\n",
      "          7.8000000e+01, -7.9185104e-01,  2.2942922e-01,  2.9751456e-01,\n",
      "         -8.2589370e-01,  1.6134387e-01,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.5780833e-01,  5.9215844e-02,\n",
      "          3.9964259e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2024200e+05,  3.0000000e+00,  3.3040000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2023800e+05,  3.0000000e+00,  3.3030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3063200e+05,  4.0000000e+00,  3.2990000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2843800e+05,  4.0000000e+00,  1.7220000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0742600e+05,  2.0000000e+00,  1.7220000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2842600e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1267400e+05,  2.0000000e+00,  5.3410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1267300e+05,  2.0000000e+00,  5.3410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1265100e+05,  2.0000000e+00,  5.3330000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1947100e+05,  3.0000000e+00,  2.6600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1891300e+05,  3.0000000e+00,  2.2920000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1876700e+05,  3.0000000e+00,  2.2080000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00]],\n",
      "\n",
      "       [[ 1.0231000e+05,  1.0000000e+00,  3.9690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0230700e+05,  1.0000000e+00,  3.9690000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0230200e+05,  1.0000000e+00,  3.9680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0021700e+05,  1.0000000e+00,  1.8360000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0014500e+05,  1.0000000e+00,  1.7850000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0011800e+05,  1.0000000e+00,  1.7640000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3217200e+05,  4.0000000e+00,  4.3150000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3215900e+05,  4.0000000e+00,  4.3120000e+03, ...,\n",
      "          3.6071527e+00,  6.7223716e+00, -2.8060716e-01],\n",
      "        [ 1.1113400e+05,  2.0000000e+00,  4.3080000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2834600e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          2.7973680e+00,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.2832800e+05,  4.0000000e+00,  1.5290000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2025500e+05,  3.0000000e+00,  3.3110000e+03,  7.0000000e+01,\n",
      "          3.4000000e+01, -9.8249000e-01,  2.9751456e-01,  1.0804961e+00,\n",
      "         -9.6206439e-01,  1.2730120e-01,  7.0602667e-01, -9.2802173e-01,\n",
      "         -4.2912181e-02,  4.3368527e-01, -9.0759611e-01,  2.5173169e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.1268800e+05,  2.0000000e+00,  5.3480000e+03,  1.6000000e+01,\n",
      "          9.0000000e+00, -7.9185104e-01,  2.9751456e-01,  5.6985599e-01,\n",
      "         -8.2589370e-01,  5.9215844e-02,  4.3368527e-01, -7.9185104e-01,\n",
      "          2.5173169e-02,  9.3258522e-02, -7.5780833e-01,  2.5173169e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0232000e+05,  1.0000000e+00,  3.9760000e+03,  1.0500000e+02,\n",
      "          9.5000000e+01, -6.5568036e-01,  2.5173169e-02,  2.2942922e-01,\n",
      "         -7.2376567e-01, -4.2912181e-02,  2.2942922e-01, -6.5568036e-01,\n",
      "          2.5173169e-02, -1.1099753e-01, -7.5780833e-01,  9.3258522e-02,\n",
      "          2.9751456e-01]],\n",
      "\n",
      "       [[ 1.3218400e+05,  4.0000000e+00,  4.3190000e+03,  7.5000000e+01,\n",
      "          7.8000000e+01, -7.9185104e-01,  2.2942922e-01,  2.9751456e-01,\n",
      "         -8.2589370e-01,  1.6134387e-01,  1.6134387e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.5780833e-01,  5.9215844e-02,\n",
      "          3.9964259e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1834100e+05,  3.0000000e+00,  1.9110000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1833800e+05,  3.0000000e+00,  1.9100000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1833300e+05,  3.0000000e+00,  1.9040000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1790200e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1269200e+05,  2.0000000e+00,  5.3480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1268700e+05,  2.0000000e+00,  5.3480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2325700e+05,  3.0000000e+00,  5.3440000e+03, ...,\n",
      "          3.6801368e-01,  6.7223716e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0904400e+05,  2.0000000e+00,  2.9050000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0885800e+05,  2.0000000e+00,  2.6460000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0855700e+05,  2.0000000e+00,  2.4950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0736300e+05,  2.0000000e+00,  1.6810000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837300e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837200e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000600e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1888800e+05,  3.0000000e+00,  2.2810000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1888700e+05,  3.0000000e+00,  2.2810000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0067100e+05,  1.0000000e+00,  2.2770000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0001300e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000600e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000200e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1834600e+05,  3.0000000e+00,  1.9130000e+03,  1.4000000e+01,\n",
      "          6.6000000e+01, -1.0573839e+00,  2.9751456e-01,  9.1028273e-01,\n",
      "         -1.1322777e+00,  3.9964259e-01,  1.7613496e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  9.7836804e-01, -1.0641924e+00,  1.6134387e-01,\n",
      "          1.1145388e+00]],\n",
      "\n",
      "       [[ 1.1270500e+05,  2.0000000e+00,  5.3550000e+03,  7.1000000e+01,\n",
      "          1.4000000e+01, -7.2376567e-01,  2.9751456e-01,  3.9964259e-01,\n",
      "         -8.2589370e-01,  1.6134387e-01,  2.9751456e-01, -7.9185104e-01,\n",
      "          2.5173169e-02,  9.3258522e-02, -6.8972301e-01,  2.5173169e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.2837400e+05,  4.0000000e+00,  1.6830000e+03,  2.8000000e+01,\n",
      "          2.9000000e+01, -7.2376567e-01,  5.9215844e-02, -4.2912181e-02,\n",
      "         -6.8972301e-01,  2.5173169e-02, -1.1099753e-01, -7.2376567e-01,\n",
      "         -1.1099753e-01, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.0068000e+05,  1.0000000e+00,  2.2830000e+03,  1.0500000e+02,\n",
      "          1.0000000e+01, -1.7908289e-01,  2.5173169e-02, -5.3993523e-01,\n",
      "         -2.8121090e-01, -4.2912181e-02, -5.1950961e-01, -3.8333893e-01,\n",
      "         -4.2912181e-02, -4.5142427e-01, -3.6972186e-01, -4.2912181e-02,\n",
      "         -5.3993523e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1834100e+05,  3.0000000e+00,  1.9110000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1833800e+05,  3.0000000e+00,  1.9100000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1833300e+05,  3.0000000e+00,  1.9040000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1790200e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1269200e+05,  2.0000000e+00,  5.3480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1268700e+05,  2.0000000e+00,  5.3480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2325700e+05,  3.0000000e+00,  5.3440000e+03, ...,\n",
      "          3.6801368e-01,  6.7223716e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0904400e+05,  2.0000000e+00,  2.9050000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0885800e+05,  2.0000000e+00,  2.6460000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0855700e+05,  2.0000000e+00,  2.4950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0736300e+05,  2.0000000e+00,  1.6810000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837300e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2837200e+05,  4.0000000e+00,  1.6540000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000600e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831600e+05,  4.0000000e+00,  1.4700000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1888800e+05,  3.0000000e+00,  2.2810000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1888700e+05,  3.0000000e+00,  2.2810000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0067100e+05,  1.0000000e+00,  2.2770000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0001300e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000600e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000200e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1834600e+05,  3.0000000e+00,  1.9130000e+03,  1.4000000e+01,\n",
      "          6.6000000e+01, -1.0573839e+00,  2.9751456e-01,  9.1028273e-01,\n",
      "         -1.1322777e+00,  3.9964259e-01,  1.7613496e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  9.7836804e-01, -1.0641924e+00,  1.6134387e-01,\n",
      "          1.1145388e+00]],\n",
      "\n",
      "       [[ 1.1270500e+05,  2.0000000e+00,  5.3550000e+03,  7.1000000e+01,\n",
      "          1.4000000e+01, -7.2376567e-01,  2.9751456e-01,  3.9964259e-01,\n",
      "         -8.2589370e-01,  1.6134387e-01,  2.9751456e-01, -7.9185104e-01,\n",
      "          2.5173169e-02,  9.3258522e-02, -6.8972301e-01,  2.5173169e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.2837400e+05,  4.0000000e+00,  1.6830000e+03,  2.8000000e+01,\n",
      "          2.9000000e+01, -7.2376567e-01,  5.9215844e-02, -4.2912181e-02,\n",
      "         -6.8972301e-01,  2.5173169e-02, -1.1099753e-01, -7.2376567e-01,\n",
      "         -1.1099753e-01, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.0068000e+05,  1.0000000e+00,  2.2830000e+03,  1.0500000e+02,\n",
      "          1.0000000e+01, -1.7908289e-01,  2.5173169e-02, -5.3993523e-01,\n",
      "         -2.8121090e-01, -4.2912181e-02, -5.1950961e-01, -3.8333893e-01,\n",
      "         -4.2912181e-02, -4.5142427e-01, -3.6972186e-01, -4.2912181e-02,\n",
      "         -5.3993523e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[1.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3001100e+05,  4.0000000e+00,  2.8770000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3000300e+05,  4.0000000e+00,  2.8320000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1958600e+05,  3.0000000e+00,  2.8320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1799600e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1799200e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1795300e+05,  3.0000000e+00,  1.6540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2895300e+05,  4.0000000e+00,  2.0860000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1854700e+05,  3.0000000e+00,  2.0860000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0793300e+05,  2.0000000e+00,  2.0820000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0731600e+05,  2.0000000e+00,  1.5020000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790600e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3050000e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.3049800e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3049700e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3023600e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.3023300e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3023200e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3096100e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3095900e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3095500e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2873600e+05,  4.0000000e+00,  1.9040000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2866000e+05,  4.0000000e+00,  1.8550000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.3002400e+05,  4.0000000e+00,  2.8840000e+03,  6.6000000e+01,\n",
      "          3.0000000e+01, -9.8249000e-01,  1.6134387e-01,  9.1028273e-01,\n",
      "         -9.6206439e-01,  1.2730120e-01,  6.7198402e-01, -9.9610710e-01,\n",
      "          9.3258522e-02,  5.6985599e-01, -1.0165327e+00,  9.3258522e-02,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.1856200e+05,  3.0000000e+00,  2.0930000e+03,  1.1100000e+02,\n",
      "          7.8000000e+01, -6.5568036e-01,  2.5173169e-02, -1.1099753e-01,\n",
      "         -7.5780833e-01,  2.5173169e-02,  2.5173169e-02, -7.9185104e-01,\n",
      "         -4.2912181e-02, -4.2912181e-02, -6.8972301e-01,  5.9215844e-02,\n",
      "         -1.9950849e-01]],\n",
      "\n",
      "       [[ 1.2009500e+05,  3.0000000e+00,  3.2360000e+03,  5.4000000e+01,\n",
      "          1.7000000e+01, -1.0505754e+00,  3.9964259e-01,  1.5911362e+00,\n",
      "         -1.0641924e+00,  2.2942922e-01,  1.2507094e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  1.1145388e+00, -1.0846181e+00,  2.2942922e-01,\n",
      "          1.2507094e+00]],\n",
      "\n",
      "       [[ 1.3097200e+05,  4.0000000e+00,  3.5280000e+03,  3.7000000e+01,\n",
      "          2.8000000e+01, -2.4716823e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -3.8333893e-01,  2.5173169e-02, -5.1950961e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -6.5568036e-01, -1.9269995e-01,  5.9215844e-02,\n",
      "         -6.8972301e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3001100e+05,  4.0000000e+00,  2.8770000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3000300e+05,  4.0000000e+00,  2.8320000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1958600e+05,  3.0000000e+00,  2.8320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1799600e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1799200e+05,  3.0000000e+00,  1.7010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1795300e+05,  3.0000000e+00,  1.6540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2895300e+05,  4.0000000e+00,  2.0860000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1854700e+05,  3.0000000e+00,  2.0860000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0793300e+05,  2.0000000e+00,  2.0820000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0731600e+05,  2.0000000e+00,  1.5020000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790600e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730400e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3050000e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.3049800e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3049700e+05,  4.0000000e+00,  3.2290000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3023600e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.3023300e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3023200e+05,  4.0000000e+00,  3.0100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3096100e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3095900e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3095500e+05,  4.0000000e+00,  3.5210000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2873600e+05,  4.0000000e+00,  1.9040000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2866000e+05,  4.0000000e+00,  1.8550000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832100e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.3002400e+05,  4.0000000e+00,  2.8840000e+03,  6.6000000e+01,\n",
      "          3.0000000e+01, -9.8249000e-01,  1.6134387e-01,  9.1028273e-01,\n",
      "         -9.6206439e-01,  1.2730120e-01,  6.7198402e-01, -9.9610710e-01,\n",
      "          9.3258522e-02,  5.6985599e-01, -1.0165327e+00,  9.3258522e-02,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.1856200e+05,  3.0000000e+00,  2.0930000e+03,  1.1100000e+02,\n",
      "          7.8000000e+01, -6.5568036e-01,  2.5173169e-02, -1.1099753e-01,\n",
      "         -7.5780833e-01,  2.5173169e-02,  2.5173169e-02, -7.9185104e-01,\n",
      "         -4.2912181e-02, -4.2912181e-02, -6.8972301e-01,  5.9215844e-02,\n",
      "         -1.9950849e-01]],\n",
      "\n",
      "       [[ 1.2009500e+05,  3.0000000e+00,  3.2360000e+03,  5.4000000e+01,\n",
      "          1.7000000e+01, -1.0505754e+00,  3.9964259e-01,  1.5911362e+00,\n",
      "         -1.0641924e+00,  2.2942922e-01,  1.2507094e+00, -1.0982351e+00,\n",
      "          2.2942922e-01,  1.1145388e+00, -1.0846181e+00,  2.2942922e-01,\n",
      "          1.2507094e+00]],\n",
      "\n",
      "       [[ 1.3097200e+05,  4.0000000e+00,  3.5280000e+03,  3.7000000e+01,\n",
      "          2.8000000e+01, -2.4716823e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -3.8333893e-01,  2.5173169e-02, -5.1950961e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -6.5568036e-01, -1.9269995e-01,  5.9215844e-02,\n",
      "         -6.8972301e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0995100e+05,  2.0000000e+00,  3.5280000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2055700e+05,  3.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2055100e+05,  3.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2842600e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0739000e+05,  2.0000000e+00,  1.7010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0736800e+05,  2.0000000e+00,  1.6870000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1895800e+05,  3.0000000e+00,  2.3100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1895500e+05,  3.0000000e+00,  2.3100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1894700e+05,  3.0000000e+00,  2.3030000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0733800e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733500e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733200e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2316000e+05,  3.0000000e+00,  5.2210000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2314200e+05,  3.0000000e+00,  5.2130000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2312900e+05,  3.0000000e+00,  5.2010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1931200e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1906500e+05,  3.0000000e+00,  2.4430000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1892000e+05,  3.0000000e+00,  2.2960000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3021500e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3021200e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3021000e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2870000e+05,  4.0000000e+00,  1.8790000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2861500e+05,  4.0000000e+00,  1.8360000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2860100e+05,  4.0000000e+00,  1.8270000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0997100e+05,  2.0000000e+00,  3.5450000e+03,  8.7000000e+01,\n",
      "          8.0000000e+01, -2.4716823e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01,  2.5173169e-02, -5.8759499e-01, -4.8546696e-01,\n",
      "         -4.2912181e-02, -4.8546696e-01, -4.5142427e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.0837200e+05,  2.0000000e+00,  2.3110000e+03,  7.1000000e+01,\n",
      "          1.0800000e+02, -7.9185104e-01,  5.9215844e-02,  1.6134387e-01,\n",
      "         -7.5780833e-01,  5.9215844e-02, -8.8695055e-03, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.9185104e-01,  2.5173169e-02,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.2316800e+05,  3.0000000e+00,  5.2240000e+03,  3.4000000e+01,\n",
      "          3.1000000e+01, -6.5568036e-01,  2.2942922e-01,  2.2942922e-01,\n",
      "         -7.9185104e-01,  2.5173169e-02, -2.4716823e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  1.6134387e-01, -6.5568036e-01,  1.6134387e-01,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.3022500e+05,  4.0000000e+00,  3.0050000e+03,  5.7000000e+01,\n",
      "          4.9000000e+01, -1.9950849e-01,  1.6134387e-01, -6.5568036e-01,\n",
      "         -1.7908289e-01,  2.5173169e-02, -6.5568036e-01, -1.1099753e-01,\n",
      "         -4.2912181e-02, -7.2376567e-01, -1.7908289e-01,  1.6134387e-01,\n",
      "         -7.5780833e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0995100e+05,  2.0000000e+00,  3.5280000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2055700e+05,  3.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2055100e+05,  3.0000000e+00,  3.5210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2842600e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0739000e+05,  2.0000000e+00,  1.7010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0736800e+05,  2.0000000e+00,  1.6870000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1895800e+05,  3.0000000e+00,  2.3100000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1895500e+05,  3.0000000e+00,  2.3100000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1894700e+05,  3.0000000e+00,  2.3030000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0733800e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733500e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733200e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2316000e+05,  3.0000000e+00,  5.2210000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2314200e+05,  3.0000000e+00,  5.2130000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2312900e+05,  3.0000000e+00,  5.2010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1931200e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1906500e+05,  3.0000000e+00,  2.4430000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1892000e+05,  3.0000000e+00,  2.2960000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3021500e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3021200e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3021000e+05,  4.0000000e+00,  3.0030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2870000e+05,  4.0000000e+00,  1.8790000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2861500e+05,  4.0000000e+00,  1.8360000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2860100e+05,  4.0000000e+00,  1.8270000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0997100e+05,  2.0000000e+00,  3.5450000e+03,  8.7000000e+01,\n",
      "          8.0000000e+01, -2.4716823e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01,  2.5173169e-02, -5.8759499e-01, -4.8546696e-01,\n",
      "         -4.2912181e-02, -4.8546696e-01, -4.5142427e-01,  2.5173169e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.0837200e+05,  2.0000000e+00,  2.3110000e+03,  7.1000000e+01,\n",
      "          1.0800000e+02, -7.9185104e-01,  5.9215844e-02,  1.6134387e-01,\n",
      "         -7.5780833e-01,  5.9215844e-02, -8.8695055e-03, -7.9185104e-01,\n",
      "         -4.2912181e-02,  2.5173169e-02, -7.9185104e-01,  2.5173169e-02,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.2316800e+05,  3.0000000e+00,  5.2240000e+03,  3.4000000e+01,\n",
      "          3.1000000e+01, -6.5568036e-01,  2.2942922e-01,  2.2942922e-01,\n",
      "         -7.9185104e-01,  2.5173169e-02, -2.4716823e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  1.6134387e-01, -6.5568036e-01,  1.6134387e-01,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.3022500e+05,  4.0000000e+00,  3.0050000e+03,  5.7000000e+01,\n",
      "          4.9000000e+01, -1.9950849e-01,  1.6134387e-01, -6.5568036e-01,\n",
      "         -1.7908289e-01,  2.5173169e-02, -6.5568036e-01, -1.1099753e-01,\n",
      "         -4.2912181e-02, -7.2376567e-01, -1.7908289e-01,  1.6134387e-01,\n",
      "         -7.5780833e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1155400e+05,  2.0000000e+00,  4.5450000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1154200e+05,  2.0000000e+00,  4.5440000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0291300e+05,  1.0000000e+00,  4.5390000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0032200e+05,  1.0000000e+00,  1.9390000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0031200e+05,  1.0000000e+00,  1.9350000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0004400e+05,  1.0000000e+00,  1.6940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3187100e+05,  4.0000000e+00,  4.1060000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3186900e+05,  4.0000000e+00,  4.1050000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3186800e+05,  4.0000000e+00,  4.1050000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0810500e+05,  2.0000000e+00,  2.1630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0789900e+05,  2.0000000e+00,  2.0670000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0745700e+05,  2.0000000e+00,  1.7420000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2926100e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2926000e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2925900e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2833900e+05,  4.0000000e+00,  1.5900000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832300e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2930800e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2930700e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2930600e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2904000e+05,  4.0000000e+00,  2.1280000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2903700e+05,  4.0000000e+00,  2.1240000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2903500e+05,  4.0000000e+00,  2.1210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0292400e+05,  1.0000000e+00,  4.5680000e+03,  1.2000000e+01,\n",
      "          4.4000000e+01, -3.8333893e-01,  9.3258522e-02, -2.8121090e-01,\n",
      "         -3.8333893e-01,  5.9215844e-02, -3.4929624e-01, -5.1950961e-01,\n",
      "          2.5173169e-02, -3.1525359e-01, -3.6972186e-01, -4.2912181e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.2147000e+05,  3.0000000e+00,  4.1230000e+03,  1.0300000e+02,\n",
      "          1.7000000e+01,  2.2942922e-01,  5.9215844e-02, -7.2376567e-01,\n",
      "          1.6134387e-01,  2.5173169e-02, -7.9185104e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -6.5568036e-01,  2.2942922e-01,  1.6134387e-01,\n",
      "         -7.5780833e-01]],\n",
      "\n",
      "       [[ 1.2927400e+05,  4.0000000e+00,  2.2680000e+03,  6.3000000e+01,\n",
      "          7.0000000e+01, -4.5142427e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01,  2.5173169e-02, -5.8759499e-01, -5.8759499e-01,\n",
      "         -1.1099753e-01, -3.1525359e-01, -5.1950961e-01, -4.2912181e-02,\n",
      "         -3.8333893e-01]],\n",
      "\n",
      "       [[ 1.0831100e+05,  2.0000000e+00,  2.2840000e+03,  2.4000000e+01,\n",
      "          8.9000000e+01, -1.0233412e+00,  2.2942922e-01,  9.1028273e-01,\n",
      "         -1.0641924e+00,  1.6134387e-01,  1.4209229e+00, -1.0301497e+00,\n",
      "          9.3258522e-02,  8.4219736e-01, -1.0233412e+00,  9.3258522e-02,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.1155400e+05,  2.0000000e+00,  4.5450000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1154200e+05,  2.0000000e+00,  4.5440000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0291300e+05,  1.0000000e+00,  4.5390000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0032200e+05,  1.0000000e+00,  1.9390000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0031200e+05,  1.0000000e+00,  1.9350000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0004400e+05,  1.0000000e+00,  1.6940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3187100e+05,  4.0000000e+00,  4.1060000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3186900e+05,  4.0000000e+00,  4.1050000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3186800e+05,  4.0000000e+00,  4.1050000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0810500e+05,  2.0000000e+00,  2.1630000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0789900e+05,  2.0000000e+00,  2.0670000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0745700e+05,  2.0000000e+00,  1.7420000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2926100e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2926000e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2925900e+05,  4.0000000e+00,  2.2470000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2833900e+05,  4.0000000e+00,  1.5900000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832300e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2930800e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2930700e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2930600e+05,  4.0000000e+00,  2.2830000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2904000e+05,  4.0000000e+00,  2.1280000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2903700e+05,  4.0000000e+00,  2.1240000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2903500e+05,  4.0000000e+00,  2.1210000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0292400e+05,  1.0000000e+00,  4.5680000e+03,  1.2000000e+01,\n",
      "          4.4000000e+01, -3.8333893e-01,  9.3258522e-02, -2.8121090e-01,\n",
      "         -3.8333893e-01,  5.9215844e-02, -3.4929624e-01, -5.1950961e-01,\n",
      "          2.5173169e-02, -3.1525359e-01, -3.6972186e-01, -4.2912181e-02,\n",
      "         -2.4716823e-01]],\n",
      "\n",
      "       [[ 1.2147000e+05,  3.0000000e+00,  4.1230000e+03,  1.0300000e+02,\n",
      "          1.7000000e+01,  2.2942922e-01,  5.9215844e-02, -7.2376567e-01,\n",
      "          1.6134387e-01,  2.5173169e-02, -7.9185104e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -6.5568036e-01,  2.2942922e-01,  1.6134387e-01,\n",
      "         -7.5780833e-01]],\n",
      "\n",
      "       [[ 1.2927400e+05,  4.0000000e+00,  2.2680000e+03,  6.3000000e+01,\n",
      "          7.0000000e+01, -4.5142427e-01,  5.9215844e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01,  2.5173169e-02, -5.8759499e-01, -5.8759499e-01,\n",
      "         -1.1099753e-01, -3.1525359e-01, -5.1950961e-01, -4.2912181e-02,\n",
      "         -3.8333893e-01]],\n",
      "\n",
      "       [[ 1.0831100e+05,  2.0000000e+00,  2.2840000e+03,  2.4000000e+01,\n",
      "          8.9000000e+01, -1.0233412e+00,  2.2942922e-01,  9.1028273e-01,\n",
      "         -1.0641924e+00,  1.6134387e-01,  1.4209229e+00, -1.0301497e+00,\n",
      "          9.3258522e-02,  8.4219736e-01, -1.0233412e+00,  9.3258522e-02,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[1.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[2.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0806600e+05,  2.0000000e+00,  2.1500000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1864600e+05,  3.0000000e+00,  2.1490000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01,  1.0223861e+01],\n",
      "        [ 1.0806400e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0002200e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001700e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2257400e+05,  3.0000000e+00,  4.8260000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.3296900e+05,  4.0000000e+00,  4.8230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3296800e+05,  4.0000000e+00,  4.8230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2881800e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2849000e+05,  4.0000000e+00,  1.7530000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2842100e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0081400e+05,  1.0000000e+00,  2.4500000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0081200e+05,  1.0000000e+00,  2.4500000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2950900e+05,  4.0000000e+00,  2.4470000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1793500e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0734000e+05,  2.0000000e+00,  1.5930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733700e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3242400e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3242300e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3242200e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2842000e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.2841200e+05,  4.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731300e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0806900e+05,  2.0000000e+00,  2.1520000e+03,  5.1000000e+01,\n",
      "          9.1000000e+01, -3.6972186e-01, -4.2912181e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01, -4.2912181e-02, -5.5355233e-01, -3.8333893e-01,\n",
      "         -1.1099753e-01, -5.8759499e-01, -3.1525359e-01, -1.1099753e-01,\n",
      "         -5.3993523e-01]],\n",
      "\n",
      "       [[ 1.2258100e+05,  3.0000000e+00,  4.8300000e+03,  8.0000000e+01,\n",
      "          5.6000000e+01, -6.5568036e-01,  1.6134387e-01,  2.9751456e-01,\n",
      "         -7.2376567e-01,  2.5173169e-02,  2.5173169e-02, -7.9185104e-01,\n",
      "          2.5173169e-02, -4.2912181e-02, -6.8972301e-01,  5.9215844e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0082800e+05,  1.0000000e+00,  2.4570000e+03,  1.0700000e+02,\n",
      "          1.0400000e+02, -9.0759611e-01,  9.3258522e-02,  7.9453760e-01,\n",
      "         -9.2802173e-01,  5.9215844e-02,  7.4006933e-01, -9.6206439e-01,\n",
      "          9.3258522e-02,  7.0602667e-01, -9.9610710e-01,  2.5173169e-02,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.2201600e+05,  3.0000000e+00,  4.4620000e+03,  2.2000000e+01,\n",
      "          8.7000000e+01, -1.9269995e-01,  1.6134387e-01, -5.3312671e-01,\n",
      "         -2.1312556e-01,  2.5173169e-02, -5.8759499e-01, -4.5142427e-01,\n",
      "         -1.1099753e-01, -5.5355233e-01, -1.1099753e-01,  2.5173169e-02,\n",
      "         -5.1950961e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0806600e+05,  2.0000000e+00,  2.1500000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.1864600e+05,  3.0000000e+00,  2.1490000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01,  1.0223861e+01],\n",
      "        [ 1.0806400e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0002200e+05,  1.0000000e+00,  1.6230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0001700e+05,  1.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2257400e+05,  3.0000000e+00,  4.8260000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.3296900e+05,  4.0000000e+00,  4.8230000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3296800e+05,  4.0000000e+00,  4.8230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2881800e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2849000e+05,  4.0000000e+00,  1.7530000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2842100e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0081400e+05,  1.0000000e+00,  2.4500000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0081200e+05,  1.0000000e+00,  2.4500000e+03, ...,\n",
      "          2.7973680e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2950900e+05,  4.0000000e+00,  2.4470000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1793500e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0734000e+05,  2.0000000e+00,  1.5930000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0733700e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3242400e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3242300e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3242200e+05,  4.0000000e+00,  4.4590000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2842000e+05,  4.0000000e+00,  1.7030000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00,  3.2208822e+00],\n",
      "        [ 1.2841200e+05,  4.0000000e+00,  1.7010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731300e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0806900e+05,  2.0000000e+00,  2.1520000e+03,  5.1000000e+01,\n",
      "          9.1000000e+01, -3.6972186e-01, -4.2912181e-02, -4.5142427e-01,\n",
      "         -2.8121090e-01, -4.2912181e-02, -5.5355233e-01, -3.8333893e-01,\n",
      "         -1.1099753e-01, -5.8759499e-01, -3.1525359e-01, -1.1099753e-01,\n",
      "         -5.3993523e-01]],\n",
      "\n",
      "       [[ 1.2258100e+05,  3.0000000e+00,  4.8300000e+03,  8.0000000e+01,\n",
      "          5.6000000e+01, -6.5568036e-01,  1.6134387e-01,  2.9751456e-01,\n",
      "         -7.2376567e-01,  2.5173169e-02,  2.5173169e-02, -7.9185104e-01,\n",
      "          2.5173169e-02, -4.2912181e-02, -6.8972301e-01,  5.9215844e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0082800e+05,  1.0000000e+00,  2.4570000e+03,  1.0700000e+02,\n",
      "          1.0400000e+02, -9.0759611e-01,  9.3258522e-02,  7.9453760e-01,\n",
      "         -9.2802173e-01,  5.9215844e-02,  7.4006933e-01, -9.6206439e-01,\n",
      "          9.3258522e-02,  7.0602667e-01, -9.9610710e-01,  2.5173169e-02,\n",
      "          9.1028273e-01]],\n",
      "\n",
      "       [[ 1.2201600e+05,  3.0000000e+00,  4.4620000e+03,  2.2000000e+01,\n",
      "          8.7000000e+01, -1.9269995e-01,  1.6134387e-01, -5.3312671e-01,\n",
      "         -2.1312556e-01,  2.5173169e-02, -5.8759499e-01, -4.5142427e-01,\n",
      "         -1.1099753e-01, -5.5355233e-01, -1.1099753e-01,  2.5173169e-02,\n",
      "         -5.1950961e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[3.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2985200e+05,  4.0000000e+00,  2.6380000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2985100e+05,  4.0000000e+00,  2.6380000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2984500e+05,  4.0000000e+00,  2.6320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.2880900e+05,  4.0000000e+00,  1.9460000e+03, ...,\n",
      "          3.6071527e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0027900e+05,  1.0000000e+00,  1.8970000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0022300e+05,  1.0000000e+00,  1.8410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2929800e+05,  4.0000000e+00,  2.2790000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2929200e+05,  4.0000000e+00,  2.2750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2929100e+05,  4.0000000e+00,  2.2750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2901200e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  6.7223716e+00],\n",
      "        [ 1.2900900e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2900600e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2913800e+05,  4.0000000e+00,  2.1890000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2913700e+05,  4.0000000e+00,  2.1880000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2913600e+05,  4.0000000e+00,  2.1880000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000900e+05,  1.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000000e+05,  1.0000000e+00,  1.4710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1966400e+05,  3.0000000e+00,  2.9190000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0122600e+05,  1.0000000e+00,  2.9190000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0122200e+05,  1.0000000e+00,  2.9190000e+03, ...,\n",
      "          2.7973680e+00,  3.2208822e+00,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.0733600e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0731400e+05,  2.0000000e+00,  1.5020000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731200e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1943700e+05,  3.0000000e+00,  2.6390000e+03,  1.7000000e+01,\n",
      "          5.0000000e+01, -7.9185104e-01,  2.5173169e-02,  1.6134387e-01,\n",
      "         -7.2376567e-01,  2.5173169e-02, -4.2912181e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.0830200e+05,  2.0000000e+00,  2.2830000e+03,  3.4000000e+01,\n",
      "          8.2000000e+01,  1.9315630e+00,  5.6985599e-01, -1.1731290e+00,\n",
      "          2.2719898e+00,  2.2942922e-01, -1.1390864e+00,  1.9315630e+00,\n",
      "          5.0177062e-01, -1.2003632e+00,  1.9315630e+00,  3.9964259e-01,\n",
      "         -1.1731290e+00]],\n",
      "\n",
      "       [[ 1.0056700e+05,  1.0000000e+00,  2.1910000e+03,  6.4000000e+01,\n",
      "          6.1000000e+01, -6.2163764e-01,  5.9215844e-02, -4.2912181e-02,\n",
      "         -6.5568036e-01, -4.2912181e-02, -4.2912181e-02, -5.8759499e-01,\n",
      "         -1.1099753e-01, -3.1525359e-01, -6.2163764e-01,  2.5173169e-02,\n",
      "         -2.8121090e-01]],\n",
      "\n",
      "       [[ 1.0123000e+05,  1.0000000e+00,  2.9220000e+03,  4.0000000e+00,\n",
      "          1.0600000e+02, -1.2480229e+00,  1.2507094e+00,  4.6549768e+00,\n",
      "         -1.2684485e+00,  9.1028273e-01,  3.9741235e+00, -1.2480229e+00,\n",
      "          7.0602667e-01,  3.2932699e+00, -1.2752570e+00,  9.1028273e-01,\n",
      "          3.2932699e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2985200e+05,  4.0000000e+00,  2.6380000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2985100e+05,  4.0000000e+00,  2.6380000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2984500e+05,  4.0000000e+00,  2.6320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.2880900e+05,  4.0000000e+00,  1.9460000e+03, ...,\n",
      "          3.6071527e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0027900e+05,  1.0000000e+00,  1.8970000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0022300e+05,  1.0000000e+00,  1.8410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2929800e+05,  4.0000000e+00,  2.2790000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2929200e+05,  4.0000000e+00,  2.2750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2929100e+05,  4.0000000e+00,  2.2750000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2901200e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  6.7223716e+00],\n",
      "        [ 1.2900900e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2900600e+05,  4.0000000e+00,  2.1110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2913800e+05,  4.0000000e+00,  2.1890000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2913700e+05,  4.0000000e+00,  2.1880000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2913600e+05,  4.0000000e+00,  2.1880000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0000900e+05,  1.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000500e+05,  1.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0000000e+05,  1.0000000e+00,  1.4710000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1966400e+05,  3.0000000e+00,  2.9190000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0122600e+05,  1.0000000e+00,  2.9190000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0122200e+05,  1.0000000e+00,  2.9190000e+03, ...,\n",
      "          2.7973680e+00,  3.2208822e+00,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.0733600e+05,  2.0000000e+00,  1.5630000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0731400e+05,  2.0000000e+00,  1.5020000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731200e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1943700e+05,  3.0000000e+00,  2.6390000e+03,  1.7000000e+01,\n",
      "          5.0000000e+01, -7.9185104e-01,  2.5173169e-02,  1.6134387e-01,\n",
      "         -7.2376567e-01,  2.5173169e-02, -4.2912181e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]],\n",
      "\n",
      "       [[ 1.0830200e+05,  2.0000000e+00,  2.2830000e+03,  3.4000000e+01,\n",
      "          8.2000000e+01,  1.9315630e+00,  5.6985599e-01, -1.1731290e+00,\n",
      "          2.2719898e+00,  2.2942922e-01, -1.1390864e+00,  1.9315630e+00,\n",
      "          5.0177062e-01, -1.2003632e+00,  1.9315630e+00,  3.9964259e-01,\n",
      "         -1.1731290e+00]],\n",
      "\n",
      "       [[ 1.0056700e+05,  1.0000000e+00,  2.1910000e+03,  6.4000000e+01,\n",
      "          6.1000000e+01, -6.2163764e-01,  5.9215844e-02, -4.2912181e-02,\n",
      "         -6.5568036e-01, -4.2912181e-02, -4.2912181e-02, -5.8759499e-01,\n",
      "         -1.1099753e-01, -3.1525359e-01, -6.2163764e-01,  2.5173169e-02,\n",
      "         -2.8121090e-01]],\n",
      "\n",
      "       [[ 1.0123000e+05,  1.0000000e+00,  2.9220000e+03,  4.0000000e+00,\n",
      "          1.0600000e+02, -1.2480229e+00,  1.2507094e+00,  4.6549768e+00,\n",
      "         -1.2684485e+00,  9.1028273e-01,  3.9741235e+00, -1.2480229e+00,\n",
      "          7.0602667e-01,  3.2932699e+00, -1.2752570e+00,  9.1028273e-01,\n",
      "          3.2932699e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[1.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0270600e+05,  1.0000000e+00,  4.3690000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0269800e+05,  1.0000000e+00,  4.3630000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2182000e+05,  3.0000000e+00,  4.3540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0766100e+05,  2.0000000e+00,  1.8480000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0744900e+05,  2.0000000e+00,  1.7320000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0743500e+05,  2.0000000e+00,  1.7290000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2881700e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2881600e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2881500e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1790400e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790200e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1186400e+05,  2.0000000e+00,  4.7580000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2245500e+05,  3.0000000e+00,  4.7520000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2244800e+05,  3.0000000e+00,  4.7520000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2970700e+05,  4.0000000e+00,  2.5550000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2958200e+05,  4.0000000e+00,  2.4850000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2952900e+05,  4.0000000e+00,  2.4570000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1141000e+05,  2.0000000e+00,  4.4530000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1123500e+05,  2.0000000e+00,  4.3550000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1122200e+05,  2.0000000e+00,  4.3540000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1907400e+05,  3.0000000e+00,  2.4430000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1903600e+05,  3.0000000e+00,  2.4110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1897800e+05,  3.0000000e+00,  2.3230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0271400e+05,  1.0000000e+00,  4.3720000e+03,  6.1000000e+01,\n",
      "          9.5000000e+01, -1.3229167e+00,  1.9315630e+00,  8.0592442e+00,\n",
      "         -1.3161082e+00,  1.9315630e+00,  5.3358307e+00, -1.3024912e+00,\n",
      "          1.2507094e+00,  4.6549768e+00, -1.3229167e+00,  1.9315630e+00,\n",
      "          6.0166841e+00]],\n",
      "\n",
      "       [[ 1.1841300e+05,  3.0000000e+00,  1.9490000e+03,  9.9000000e+01,\n",
      "          1.4000000e+01, -7.9185104e-01,  5.9215844e-02,  1.6134387e-01,\n",
      "         -7.2376567e-01, -8.8695055e-03, -8.8695055e-03, -7.9185104e-01,\n",
      "         -4.2912181e-02, -4.2912181e-02, -7.5780833e-01, -4.2912181e-02,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.2246600e+05,  3.0000000e+00,  4.7670000e+03,  9.9000000e+01,\n",
      "          9.0000000e+01, -1.1322777e+00,  9.1028273e-01,  2.9528432e+00,\n",
      "         -1.1663204e+00,  3.9964259e-01,  2.6124165e+00, -1.1322777e+00,\n",
      "          3.6559993e-01,  1.2507094e+00, -1.1118522e+00,  3.9964259e-01,\n",
      "          2.6124165e+00]],\n",
      "\n",
      "       [[ 1.0305000e+05,  1.0000000e+00,  4.6910000e+03,  8.1000000e+01,\n",
      "          9.8000000e+01, -7.2376567e-01,  1.6134387e-01,  2.2942922e-01,\n",
      "         -6.8972301e-01,  9.3258522e-02,  9.3258522e-02, -7.9185104e-01,\n",
      "          9.3258522e-02,  9.3258522e-02, -6.8972301e-01,  9.3258522e-02,\n",
      "          1.6134387e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.0270600e+05,  1.0000000e+00,  4.3690000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0269800e+05,  1.0000000e+00,  4.3630000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2182000e+05,  3.0000000e+00,  4.3540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0766100e+05,  2.0000000e+00,  1.8480000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0744900e+05,  2.0000000e+00,  1.7320000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0743500e+05,  2.0000000e+00,  1.7290000e+03, ...,\n",
      "         -4.4177109e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2881700e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2881600e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2881500e+05,  4.0000000e+00,  1.9480000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1790400e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1790200e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1186400e+05,  2.0000000e+00,  4.7580000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2245500e+05,  3.0000000e+00,  4.7520000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2244800e+05,  3.0000000e+00,  4.7520000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2970700e+05,  4.0000000e+00,  2.5550000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2958200e+05,  4.0000000e+00,  2.4850000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2952900e+05,  4.0000000e+00,  2.4570000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1141000e+05,  2.0000000e+00,  4.4530000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1123500e+05,  2.0000000e+00,  4.3550000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1122200e+05,  2.0000000e+00,  4.3540000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        ...,\n",
      "        [ 1.1907400e+05,  3.0000000e+00,  2.4430000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1903600e+05,  3.0000000e+00,  2.4110000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1897800e+05,  3.0000000e+00,  2.3230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.0271400e+05,  1.0000000e+00,  4.3720000e+03,  6.1000000e+01,\n",
      "          9.5000000e+01, -1.3229167e+00,  1.9315630e+00,  8.0592442e+00,\n",
      "         -1.3161082e+00,  1.9315630e+00,  5.3358307e+00, -1.3024912e+00,\n",
      "          1.2507094e+00,  4.6549768e+00, -1.3229167e+00,  1.9315630e+00,\n",
      "          6.0166841e+00]],\n",
      "\n",
      "       [[ 1.1841300e+05,  3.0000000e+00,  1.9490000e+03,  9.9000000e+01,\n",
      "          1.4000000e+01, -7.9185104e-01,  5.9215844e-02,  1.6134387e-01,\n",
      "         -7.2376567e-01, -8.8695055e-03, -8.8695055e-03, -7.9185104e-01,\n",
      "         -4.2912181e-02, -4.2912181e-02, -7.5780833e-01, -4.2912181e-02,\n",
      "          2.5173169e-02]],\n",
      "\n",
      "       [[ 1.2246600e+05,  3.0000000e+00,  4.7670000e+03,  9.9000000e+01,\n",
      "          9.0000000e+01, -1.1322777e+00,  9.1028273e-01,  2.9528432e+00,\n",
      "         -1.1663204e+00,  3.9964259e-01,  2.6124165e+00, -1.1322777e+00,\n",
      "          3.6559993e-01,  1.2507094e+00, -1.1118522e+00,  3.9964259e-01,\n",
      "          2.6124165e+00]],\n",
      "\n",
      "       [[ 1.0305000e+05,  1.0000000e+00,  4.6910000e+03,  8.1000000e+01,\n",
      "          9.8000000e+01, -7.2376567e-01,  1.6134387e-01,  2.2942922e-01,\n",
      "         -6.8972301e-01,  9.3258522e-02,  9.3258522e-02, -7.9185104e-01,\n",
      "          9.3258522e-02,  9.3258522e-02, -6.8972301e-01,  9.3258522e-02,\n",
      "          1.6134387e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]]\n",
      "\n",
      " [[2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[1.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[3.]]\n",
      "\n",
      " [[1.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3361800e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3361600e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3361500e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3325600e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3325300e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00,  6.7223716e+00],\n",
      "        [ 1.3325200e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2878400e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2878300e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2877900e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2831900e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3038000e+05,  4.0000000e+00,  3.1670000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1997800e+05,  3.0000000e+00,  3.1670000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3037700e+05,  4.0000000e+00,  3.1640000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1792500e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833600e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832400e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3002400e+05,  4.0000000e+00,  2.8840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3002000e+05,  4.0000000e+00,  2.8840000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3000900e+05,  4.0000000e+00,  2.8770000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1795000e+05,  3.0000000e+00,  1.6540000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1794600e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1793000e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1261700e+05,  2.0000000e+00,  5.2760000e+03,  1.0800000e+02,\n",
      "          1.4000000e+01, -4.2912181e-02,  2.2942922e-01, -5.1950961e-01,\n",
      "         -2.4716823e-01,  5.9215844e-02, -5.1950961e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -5.8759499e-01, -4.2912181e-02, -4.2912181e-02,\n",
      "         -5.1950961e-01]],\n",
      "\n",
      "       [[ 1.0777700e+05,  2.0000000e+00,  1.9390000e+03,  1.7000000e+01,\n",
      "          1.0600000e+02, -1.1099753e-01,  5.9215844e-02, -7.2376567e-01,\n",
      "         -7.6954857e-02,  5.9215844e-02, -7.2376567e-01,  2.5173169e-02,\n",
      "          2.5173169e-02, -8.5993636e-01, -2.4716823e-01,  1.6134387e-01,\n",
      "         -7.2376567e-01]],\n",
      "\n",
      "       [[ 1.3039700e+05,  4.0000000e+00,  3.1730000e+03,  1.5000000e+01,\n",
      "          7.3000000e+01, -9.7568148e-01,  1.6134387e-01,  7.9453760e-01,\n",
      "         -9.6206439e-01,  1.2730120e-01,  7.0602667e-01, -9.2802173e-01,\n",
      "          2.5173169e-02,  3.6559993e-01, -9.2802173e-01,  5.9215844e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.3003800e+05,  4.0000000e+00,  2.9050000e+03,  1.0900000e+02,\n",
      "          1.6000000e+01, -7.9185104e-01,  5.9215844e-02,  2.2942922e-01,\n",
      "         -7.5780833e-01,  2.5173169e-02,  2.5173169e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3361800e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3361600e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3361500e+05,  4.0000000e+00,  5.2730000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3325600e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3325300e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00,  6.7223716e+00],\n",
      "        [ 1.3325200e+05,  4.0000000e+00,  5.0750000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2878400e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2878300e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2877900e+05,  4.0000000e+00,  1.9320000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2831900e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831800e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2831700e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3038000e+05,  4.0000000e+00,  3.1670000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1997800e+05,  3.0000000e+00,  3.1670000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3037700e+05,  4.0000000e+00,  3.1640000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1792500e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833600e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2832400e+05,  4.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3002400e+05,  4.0000000e+00,  2.8840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3002000e+05,  4.0000000e+00,  2.8840000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3000900e+05,  4.0000000e+00,  2.8770000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1795000e+05,  3.0000000e+00,  1.6540000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1794600e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1793000e+05,  3.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1261700e+05,  2.0000000e+00,  5.2760000e+03,  1.0800000e+02,\n",
      "          1.4000000e+01, -4.2912181e-02,  2.2942922e-01, -5.1950961e-01,\n",
      "         -2.4716823e-01,  5.9215844e-02, -5.1950961e-01, -2.4716823e-01,\n",
      "         -4.2912181e-02, -5.8759499e-01, -4.2912181e-02, -4.2912181e-02,\n",
      "         -5.1950961e-01]],\n",
      "\n",
      "       [[ 1.0777700e+05,  2.0000000e+00,  1.9390000e+03,  1.7000000e+01,\n",
      "          1.0600000e+02, -1.1099753e-01,  5.9215844e-02, -7.2376567e-01,\n",
      "         -7.6954857e-02,  5.9215844e-02, -7.2376567e-01,  2.5173169e-02,\n",
      "          2.5173169e-02, -8.5993636e-01, -2.4716823e-01,  1.6134387e-01,\n",
      "         -7.2376567e-01]],\n",
      "\n",
      "       [[ 1.3039700e+05,  4.0000000e+00,  3.1730000e+03,  1.5000000e+01,\n",
      "          7.3000000e+01, -9.7568148e-01,  1.6134387e-01,  7.9453760e-01,\n",
      "         -9.6206439e-01,  1.2730120e-01,  7.0602667e-01, -9.2802173e-01,\n",
      "          2.5173169e-02,  3.6559993e-01, -9.2802173e-01,  5.9215844e-02,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.3003800e+05,  4.0000000e+00,  2.9050000e+03,  1.0900000e+02,\n",
      "          1.6000000e+01, -7.9185104e-01,  5.9215844e-02,  2.2942922e-01,\n",
      "         -7.5780833e-01,  2.5173169e-02,  2.5173169e-02, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  2.5173169e-02,\n",
      "         -1.1099753e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2182000e+05,  3.0000000e+00,  4.3540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0268800e+05,  1.0000000e+00,  4.3520000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1120400e+05,  2.0000000e+00,  4.3500000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0069900e+05,  1.0000000e+00,  2.2930000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0828200e+05,  2.0000000e+00,  2.2700000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0813500e+05,  2.0000000e+00,  2.1860000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0211000e+05,  1.0000000e+00,  3.7540000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0210900e+05,  1.0000000e+00,  3.7540000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3135700e+05,  4.0000000e+00,  3.7380000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0042800e+05,  1.0000000e+00,  2.0730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0038900e+05,  1.0000000e+00,  2.0180000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0004400e+05,  1.0000000e+00,  1.6940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0824200e+05,  2.0000000e+00,  2.2360000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1881200e+05,  3.0000000e+00,  2.2340000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0822600e+05,  2.0000000e+00,  2.2340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0732300e+05,  2.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730900e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1191600e+05,  2.0000000e+00,  4.7830000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3289900e+05,  4.0000000e+00,  4.7820000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3289200e+05,  4.0000000e+00,  4.7820000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2916000e+05,  4.0000000e+00,  2.1950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2914000e+05,  4.0000000e+00,  2.1910000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2907700e+05,  4.0000000e+00,  2.1560000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1123700e+05,  2.0000000e+00,  4.3680000e+03,  1.1000000e+01,\n",
      "          1.0400000e+02, -8.5312784e-01,  1.6134387e-01,  5.6985599e-01,\n",
      "         -8.5993636e-01,  5.9215844e-02,  5.6985599e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  1.6134387e-01, -8.2589370e-01,  2.2942922e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0211200e+05,  1.0000000e+00,  3.7550000e+03,  2.7000000e+01,\n",
      "          1.2000000e+01, -1.3773850e+00,  3.2932699e+00,  1.0782659e+01,\n",
      "         -1.3910021e+00,  3.4634833e+00,  8.0592442e+00, -1.3705765e+00,\n",
      "          2.2719898e+00,  8.0592442e+00, -1.3910021e+00,  1.9315630e+00,\n",
      "          8.0592442e+00]],\n",
      "\n",
      "       [[ 1.0824600e+05,  2.0000000e+00,  2.2390000e+03,  5.9000000e+01,\n",
      "          8.2000000e+01,  3.9964259e-01,  1.6134387e-01, -8.5993636e-01,\n",
      "          1.9538654e-01, -4.2912181e-02, -7.9185104e-01,  2.5173169e-02,\n",
      "         -4.2912181e-02, -7.9185104e-01,  3.9964259e-01,  2.5173169e-02,\n",
      "         -9.0759611e-01]],\n",
      "\n",
      "       [[ 1.3291100e+05,  4.0000000e+00,  4.7900000e+03,  7.5000000e+01,\n",
      "          1.9000000e+01, -8.5312784e-01,  3.9964259e-01,  7.9453760e-01,\n",
      "         -9.4163883e-01,  1.6134387e-01,  6.3794130e-01, -9.6206439e-01,\n",
      "          1.6134387e-01,  3.6559993e-01, -9.0759611e-01,  1.6134387e-01,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2182000e+05,  3.0000000e+00,  4.3540000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0268800e+05,  1.0000000e+00,  4.3520000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1120400e+05,  2.0000000e+00,  4.3500000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0069900e+05,  1.0000000e+00,  2.2930000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0828200e+05,  2.0000000e+00,  2.2700000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0813500e+05,  2.0000000e+00,  2.1860000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0211000e+05,  1.0000000e+00,  3.7540000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0210900e+05,  1.0000000e+00,  3.7540000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3135700e+05,  4.0000000e+00,  3.7380000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0042800e+05,  1.0000000e+00,  2.0730000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.0038900e+05,  1.0000000e+00,  2.0180000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0004400e+05,  1.0000000e+00,  1.6940000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0824200e+05,  2.0000000e+00,  2.2360000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1881200e+05,  3.0000000e+00,  2.2340000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0822600e+05,  2.0000000e+00,  2.2340000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0732300e+05,  2.0000000e+00,  1.5300000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0731100e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730900e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1191600e+05,  2.0000000e+00,  4.7830000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3289900e+05,  4.0000000e+00,  4.7820000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3289200e+05,  4.0000000e+00,  4.7820000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2916000e+05,  4.0000000e+00,  2.1950000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2914000e+05,  4.0000000e+00,  2.1910000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2907700e+05,  4.0000000e+00,  2.1560000e+03, ...,\n",
      "         -1.2515559e+00,  3.2208822e+00, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.1123700e+05,  2.0000000e+00,  4.3680000e+03,  1.1000000e+01,\n",
      "          1.0400000e+02, -8.5312784e-01,  1.6134387e-01,  5.6985599e-01,\n",
      "         -8.5993636e-01,  5.9215844e-02,  5.6985599e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  1.6134387e-01, -8.2589370e-01,  2.2942922e-01,\n",
      "          4.3368527e-01]],\n",
      "\n",
      "       [[ 1.0211200e+05,  1.0000000e+00,  3.7550000e+03,  2.7000000e+01,\n",
      "          1.2000000e+01, -1.3773850e+00,  3.2932699e+00,  1.0782659e+01,\n",
      "         -1.3910021e+00,  3.4634833e+00,  8.0592442e+00, -1.3705765e+00,\n",
      "          2.2719898e+00,  8.0592442e+00, -1.3910021e+00,  1.9315630e+00,\n",
      "          8.0592442e+00]],\n",
      "\n",
      "       [[ 1.0824600e+05,  2.0000000e+00,  2.2390000e+03,  5.9000000e+01,\n",
      "          8.2000000e+01,  3.9964259e-01,  1.6134387e-01, -8.5993636e-01,\n",
      "          1.9538654e-01, -4.2912181e-02, -7.9185104e-01,  2.5173169e-02,\n",
      "         -4.2912181e-02, -7.9185104e-01,  3.9964259e-01,  2.5173169e-02,\n",
      "         -9.0759611e-01]],\n",
      "\n",
      "       [[ 1.3291100e+05,  4.0000000e+00,  4.7900000e+03,  7.5000000e+01,\n",
      "          1.9000000e+01, -8.5312784e-01,  3.9964259e-01,  7.9453760e-01,\n",
      "         -9.4163883e-01,  1.6134387e-01,  6.3794130e-01, -9.6206439e-01,\n",
      "          1.6134387e-01,  3.6559993e-01, -9.0759611e-01,  1.6134387e-01,\n",
      "          9.1028273e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [1.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[2.]]\n",
      "\n",
      " [[1.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[4.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3081800e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3081700e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3081600e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3062100e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3062000e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3061900e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0806400e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0805600e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0805400e+05,  2.0000000e+00,  2.1420000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0730900e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730800e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730500e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1240100e+05,  2.0000000e+00,  5.1230000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1239700e+05,  2.0000000e+00,  5.1170000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1237600e+05,  2.0000000e+00,  5.1150000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0111900e+05,  1.0000000e+00,  2.6890000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0105400e+05,  1.0000000e+00,  2.6320000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0093100e+05,  1.0000000e+00,  2.5410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1932000e+05,  3.0000000e+00,  2.5720000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1931400e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1930900e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1794300e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833800e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1790600e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2041500e+05,  3.0000000e+00,  3.3770000e+03,  1.9000000e+01,\n",
      "          5.0000000e+01, -7.2376567e-01,  1.6134387e-01,  1.6134387e-01,\n",
      "         -7.9185104e-01,  2.5173169e-02,  1.2730120e-01, -7.9185104e-01,\n",
      "          2.5173169e-02, -4.2912181e-02, -7.2376567e-01,  9.3258522e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0807100e+05,  2.0000000e+00,  2.1520000e+03,  5.4000000e+01,\n",
      "          2.0000000e+01, -9.0759611e-01,  1.6134387e-01,  3.9964259e-01,\n",
      "         -9.6206439e-01,  2.5173169e-02,  8.4219736e-01, -8.9397907e-01,\n",
      "         -4.2912181e-02,  2.2942922e-01, -9.0759611e-01,  2.5173169e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.1240300e+05,  2.0000000e+00,  5.1240000e+03,  1.0000000e+01,\n",
      "          8.2000000e+01, -7.9185104e-01,  2.9751456e-01,  5.6985599e-01,\n",
      "         -8.2589370e-01,  9.3258522e-02,  3.9964259e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  2.9751456e-01, -7.9185104e-01,  2.5173169e-02,\n",
      "          5.6985599e-01]],\n",
      "\n",
      "       [[ 1.0873900e+05,  2.0000000e+00,  2.5760000e+03,  5.2000000e+01,\n",
      "          3.1000000e+01, -7.2376567e-01,  2.5173169e-02,  2.5173169e-02,\n",
      "         -6.5568036e-01, -8.8695055e-03, -1.4504020e-01, -7.2376567e-01,\n",
      "         -1.1099753e-01, -4.2912181e-02, -6.8972301e-01, -4.2912181e-02,\n",
      "         -1.1099753e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.3081800e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3081700e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3081600e+05,  4.0000000e+00,  3.3740000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3062100e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3062000e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3061900e+05,  4.0000000e+00,  3.2840000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.0806400e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0805600e+05,  2.0000000e+00,  2.1490000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0805400e+05,  2.0000000e+00,  2.1420000e+03, ...,\n",
      "          1.1777985e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0730900e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730800e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0730500e+05,  2.0000000e+00,  1.5010000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1240100e+05,  2.0000000e+00,  5.1230000e+03, ...,\n",
      "          1.9875833e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1239700e+05,  2.0000000e+00,  5.1170000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1237600e+05,  2.0000000e+00,  5.1150000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.0111900e+05,  1.0000000e+00,  2.6890000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.0105400e+05,  1.0000000e+00,  2.6320000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.0093100e+05,  1.0000000e+00,  2.5410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.1932000e+05,  3.0000000e+00,  2.5720000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1931400e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.1930900e+05,  3.0000000e+00,  2.5690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1794300e+05,  3.0000000e+00,  1.6500000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2833800e+05,  4.0000000e+00,  1.5600000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.1790600e+05,  3.0000000e+00,  1.5010000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2041500e+05,  3.0000000e+00,  3.3770000e+03,  1.9000000e+01,\n",
      "          5.0000000e+01, -7.2376567e-01,  1.6134387e-01,  1.6134387e-01,\n",
      "         -7.9185104e-01,  2.5173169e-02,  1.2730120e-01, -7.9185104e-01,\n",
      "          2.5173169e-02, -4.2912181e-02, -7.2376567e-01,  9.3258522e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0807100e+05,  2.0000000e+00,  2.1520000e+03,  5.4000000e+01,\n",
      "          2.0000000e+01, -9.0759611e-01,  1.6134387e-01,  3.9964259e-01,\n",
      "         -9.6206439e-01,  2.5173169e-02,  8.4219736e-01, -8.9397907e-01,\n",
      "         -4.2912181e-02,  2.2942922e-01, -9.0759611e-01,  2.5173169e-02,\n",
      "          3.9964259e-01]],\n",
      "\n",
      "       [[ 1.1240300e+05,  2.0000000e+00,  5.1240000e+03,  1.0000000e+01,\n",
      "          8.2000000e+01, -7.9185104e-01,  2.9751456e-01,  5.6985599e-01,\n",
      "         -8.2589370e-01,  9.3258522e-02,  3.9964259e-01, -8.5993636e-01,\n",
      "          2.5173169e-02,  2.9751456e-01, -7.9185104e-01,  2.5173169e-02,\n",
      "          5.6985599e-01]],\n",
      "\n",
      "       [[ 1.0873900e+05,  2.0000000e+00,  2.5760000e+03,  5.2000000e+01,\n",
      "          3.1000000e+01, -7.2376567e-01,  2.5173169e-02,  2.5173169e-02,\n",
      "         -6.5568036e-01, -8.8695055e-03, -1.4504020e-01, -7.2376567e-01,\n",
      "         -1.1099753e-01, -4.2912181e-02, -6.8972301e-01, -4.2912181e-02,\n",
      "         -1.1099753e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]]\n",
      "\n",
      " [[2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]\n",
      "  [1.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [1.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[2.]]\n",
      "\n",
      " [[2.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n",
      "train step in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2233400e+05,  3.0000000e+00,  4.6790000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2232700e+05,  3.0000000e+00,  4.6760000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2232300e+05,  3.0000000e+00,  4.6760000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1953800e+05,  3.0000000e+00,  2.7180000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2988400e+05,  4.0000000e+00,  2.6600000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2963700e+05,  4.0000000e+00,  2.5200000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3313800e+05,  4.0000000e+00,  4.9930000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3313200e+05,  4.0000000e+00,  4.9930000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3311900e+05,  4.0000000e+00,  4.9910000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2101400e+05,  3.0000000e+00,  3.7760000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2067300e+05,  3.0000000e+00,  3.5910000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2034300e+05,  3.0000000e+00,  3.3460000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2021100e+05,  3.0000000e+00,  3.2820000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2018900e+05,  3.0000000e+00,  3.2760000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2018600e+05,  3.0000000e+00,  3.2690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2847800e+05,  4.0000000e+00,  1.7500000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2844500e+05,  4.0000000e+00,  1.7290000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2834000e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3344100e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3344000e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3343900e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3324400e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3324300e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3324200e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2235100e+05,  3.0000000e+00,  4.6830000e+03,  9.3000000e+01,\n",
      "          9.9000000e+01, -3.6291334e-01,  5.9215844e-02, -3.6291334e-01,\n",
      "         -4.1738161e-01,  2.5173169e-02, -4.1738161e-01, -5.1950961e-01,\n",
      "         -4.2912181e-02, -4.5142427e-01, -5.8759499e-01, -4.2912181e-02,\n",
      "         -2.8121090e-01]],\n",
      "\n",
      "       [[ 1.3313900e+05,  4.0000000e+00,  4.9940000e+03,  2.0000000e+00,\n",
      "          1.1200000e+02, -5.1950961e-01,  1.6134387e-01,  2.5173169e-02,\n",
      "         -6.2163764e-01,  2.5173169e-02, -1.7908289e-01, -5.8759499e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.5142427e-01, -4.2912181e-02,\n",
      "         -1.7908289e-01]],\n",
      "\n",
      "       [[ 1.3063500e+05,  4.0000000e+00,  3.2990000e+03,  1.0000000e+02,\n",
      "          2.8000000e+01, -6.5568036e-01,  1.6134387e-01,  2.5173169e-02,\n",
      "         -6.5568036e-01,  5.9215844e-02, -2.1312556e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  9.3258522e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0354900e+05,  1.0000000e+00,  5.1420000e+03,  1.0100000e+02,\n",
      "          6.1000000e+01,  3.9964259e-01,  3.9964259e-01, -7.5780833e-01,\n",
      "          2.2942922e-01,  2.9751456e-01, -7.9185104e-01,  2.9751456e-01,\n",
      "          9.3258522e-02, -7.9185104e-01,  2.9751456e-01,  2.2942922e-01,\n",
      "         -7.9185104e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "former, in (<tf.Tensor: shape=(4, 200, 35), dtype=float32, numpy=\n",
      "array([[[ 1.2233400e+05,  3.0000000e+00,  4.6790000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2232700e+05,  3.0000000e+00,  4.6760000e+03, ...,\n",
      "          1.9875833e+00,  3.2208822e+00, -2.8060716e-01],\n",
      "        [ 1.2232300e+05,  3.0000000e+00,  4.6760000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.1953800e+05,  3.0000000e+00,  2.7180000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2988400e+05,  4.0000000e+00,  2.6600000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2963700e+05,  4.0000000e+00,  2.5200000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3313800e+05,  4.0000000e+00,  4.9930000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3313200e+05,  4.0000000e+00,  4.9930000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3311900e+05,  4.0000000e+00,  4.9910000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2101400e+05,  3.0000000e+00,  3.7760000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01,  3.2208822e+00],\n",
      "        [ 1.2067300e+05,  3.0000000e+00,  3.5910000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2034300e+05,  3.0000000e+00,  3.3460000e+03, ...,\n",
      "          3.6801368e-01,  3.2208822e+00, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.2021100e+05,  3.0000000e+00,  3.2820000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2018900e+05,  3.0000000e+00,  3.2760000e+03, ...,\n",
      "         -1.2515559e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2018600e+05,  3.0000000e+00,  3.2690000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.2847800e+05,  4.0000000e+00,  1.7500000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2844500e+05,  4.0000000e+00,  1.7290000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.2834000e+05,  4.0000000e+00,  1.6230000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]],\n",
      "\n",
      "       [[ 1.3344100e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3344000e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3343900e+05,  4.0000000e+00,  5.1410000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        ...,\n",
      "        [ 1.3324400e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "         -4.4177109e-01, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3324300e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "          1.1777985e+00, -2.8060716e-01, -2.8060716e-01],\n",
      "        [ 1.3324200e+05,  4.0000000e+00,  5.0680000e+03, ...,\n",
      "          3.6801368e-01, -2.8060716e-01, -2.8060716e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 1, 17), dtype=float32, numpy=\n",
      "array([[[ 1.2235100e+05,  3.0000000e+00,  4.6830000e+03,  9.3000000e+01,\n",
      "          9.9000000e+01, -3.6291334e-01,  5.9215844e-02, -3.6291334e-01,\n",
      "         -4.1738161e-01,  2.5173169e-02, -4.1738161e-01, -5.1950961e-01,\n",
      "         -4.2912181e-02, -4.5142427e-01, -5.8759499e-01, -4.2912181e-02,\n",
      "         -2.8121090e-01]],\n",
      "\n",
      "       [[ 1.3313900e+05,  4.0000000e+00,  4.9940000e+03,  2.0000000e+00,\n",
      "          1.1200000e+02, -5.1950961e-01,  1.6134387e-01,  2.5173169e-02,\n",
      "         -6.2163764e-01,  2.5173169e-02, -1.7908289e-01, -5.8759499e-01,\n",
      "         -4.2912181e-02, -3.8333893e-01, -4.5142427e-01, -4.2912181e-02,\n",
      "         -1.7908289e-01]],\n",
      "\n",
      "       [[ 1.3063500e+05,  4.0000000e+00,  3.2990000e+03,  1.0000000e+02,\n",
      "          2.8000000e+01, -6.5568036e-01,  1.6134387e-01,  2.5173169e-02,\n",
      "         -6.5568036e-01,  5.9215844e-02, -2.1312556e-01, -7.2376567e-01,\n",
      "         -4.2912181e-02, -1.1099753e-01, -7.2376567e-01,  9.3258522e-02,\n",
      "          2.2942922e-01]],\n",
      "\n",
      "       [[ 1.0354900e+05,  1.0000000e+00,  5.1420000e+03,  1.0100000e+02,\n",
      "          6.1000000e+01,  3.9964259e-01,  3.9964259e-01, -7.5780833e-01,\n",
      "          2.2942922e-01,  2.9751456e-01, -7.9185104e-01,  2.9751456e-01,\n",
      "          9.3258522e-02, -7.9185104e-01,  2.9751456e-01,  2.2942922e-01,\n",
      "         -7.9185104e-01]]], dtype=float32)>, <tf.Tensor: shape=(4, 200, 200), dtype=int32, numpy=\n",
      "array([[[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]],\n",
      "\n",
      "       [[1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1],\n",
      "        [1, 1, 1, ..., 1, 1, 1]]])>)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [2.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [2.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]]\n",
      "\n",
      " [[3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [3.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]\n",
      "\n",
      " [[4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]\n",
      "  [4.]]], shape=(4, 200, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]\n",
      "  [nan nan nan nan]]], shape=(4, 200, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32)\n",
      "enc, pe out tf.Tensor(\n",
      "[[[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]\n",
      "\n",
      " [[nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  ...\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]\n",
      "  [nan nan nan ... nan nan nan]]], shape=(4, 200, 152), dtype=float32) tf.Tensor(\n",
      "[[[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]\n",
      "\n",
      " [[1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  ...\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]\n",
      "  [1 1 1 ... 1 1 1]]], shape=(4, 200, 200), dtype=int32)\n",
      "pe 2.7.1 1 tf.Tensor(\n",
      "[[[3.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[4.]]\n",
      "\n",
      " [[1.]]], shape=(4, 1, 1), dtype=float32)\n",
      "pe 2.7.1 2 tf.Tensor(\n",
      "[[[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan]]], shape=(4, 1, 4), dtype=float32)\n",
      "pe 4 tf.Tensor(\n",
      "[[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]\n",
      "\n",
      " [[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "   nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan]]], shape=(4, 1, 152), dtype=float32)\n",
      "train step out [(<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>), (<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>, <tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
      "array([[nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan],\n",
      "       [nan, nan, nan]], dtype=float32)>)]\n"
     ]
    }
   ],
   "source": [
    "epochs = 500;  prev_loss = float(\"inf\")\n",
    "for epoch in range(history.len(), epochs):\n",
    "    start_time = time.time()\n",
    "    m = 0; epoch_loss = 0.0\n",
    "    n = 0; loss = tf.Variable(0.0, dtype=tf.float32); samples_seen = 0\n",
    "    qGroup_selectivity, game_selectivity = scheduler(epoch)   # noralized_profit_pred_multiplier is scheduled here.\n",
    "\n",
    "    baseId_1 = None\n",
    "    train_batches = make_train_batches(train_ds)\n",
    "    for step, ((baseId, sequence, base_bb, mask), (base_label, seq_len_org)) in enumerate(train_batches):\n",
    "        if baseId_1 is None: baseId_1 = baseId[0]\n",
    "        # print('train', sequence.shape)\n",
    "        x = (sequence, base_bb, mask); y = base_label\n",
    "        batch_loss = train_step(x, y, qGroup_selectivity, game_selectivity)\n",
    "        n += 1; loss = loss * (n-1) / n + batch_loss / n\n",
    "        m += 1; epoch_loss = epoch_loss * (m-1)/m + batch_loss / m\n",
    "\n",
    "        samples_seen += sequence.shape[0]\n",
    "        if step % 50 == 0:\n",
    "            show_steps(epoch, step, loss, samples_seen)\n",
    "            n = 0; loss = 0.0\n",
    "\n",
    "    show_steps(epoch, step, loss, samples_seen)\n",
    "    val_loss = test_with_dataset(test_batches, qGroup_selectivity, game_selectivity)\n",
    "    profit_back_mean, nBettingsTotal = back_test_with_dataset(test_batches)\n",
    "    # back2 = back_test_with_dataset2(test_batches)\n",
    "    save_checkpoint(epoch_loss, val_loss, profit_back_mean, nBettingsTotal)     #------------------------------------------- comeback\n",
    "\n",
    "    eM365W = EPL.layers[0].layers[0].get_weights()[6]; eM365W = list(tf.reshape(eM365W, (-1,)).numpy())\n",
    "    # shift = tf.squeeze(EPL.layers[-1].get_weights()).numpy()\n",
    "    # dM365W =EPL.layers[0].layers[1].get_weights()[4]; dM365W = list(tf.reshape(dM365W, (-1,)).numpy())\n",
    "\n",
    "    print(\"epoch: {}, loss: {}, val_loss: {}, back_test: {}, nBettings: {}, baseId_1: {}, memory365: {:.4f},  time taken: {:.0f}s          \"\n",
    "          .format(epoch, float(epoch_loss), float(val_loss), float(profit_back_mean), nBettingsTotal, baseId_1, eM365W[0] * hyperparams.initial_m365, (time.time() - start_time)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
