{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Checklist\n",
    "1. Make sure your \"Regional Format\" is set to \"English (United Kingdom)\" before Excel opens files that contain English date-like objects,\n",
    "   if you want them to be parsed as English datetime. English (United States) regional format will convert them to American Datetime as possible.\n",
    "   You have no way to prevent Excel from converting date-like strings or objects to datetime when opening a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import gc\n",
    "\n",
    "from config import config\n",
    "import data_helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MODE = True\n",
    "OPERATION = 'PRETRAIN'  # 'PRETRAIN'  'TRAIN_C'   'FINETUNE'   'TEST'\n",
    "\n",
    "TEST_ID = 'YA.data'\n",
    "THEME = '15_FN'\n",
    "\n",
    "HISTORY_LEN = 400\n",
    "SEQ_YEAR_SPAN = 15\n",
    "\n",
    "STARTING_PERCENT = 0\n",
    "ENDING_PERCENT = 0\n",
    "TRAIN_PERCENT= 94   # chronically the earliest part over the databased past rows.  In the end of this part, there is BACK part with the same size as VALID part.\n",
    "VALID_PERCENT = 4   # chronically the next part to train part over the databased past rows\n",
    "TEST_PERCENT = 100 - TRAIN_PERCENT - VALID_PERCENT # chronically the last part over the databased past rows. This part will be added with post-database rows, either past or known coming\n",
    "\n",
    "BATCH_SIZE = 30       # max 20 for LOSS_TYPE 'entropy'\n",
    "BUFFER_SIZE = 35000\n",
    "\n",
    "TEAM_EMBS = 50  #\n",
    "DECODE_BASE_DATE = False\n",
    "EMBED_AB_COLS = False    # True: Pls choose a small LR so that we have plenty of train epochs and the embedded values have enough chance to seek their proper places.\n",
    "ODDS_IN_ENCODER = False   #--------------\n",
    "ODDS_IN_DECODER = True\n",
    "ADDITIONAL_D_MODEL = 0   #------------ Try increase it when underfitting.\n",
    "\n",
    "TRANSFORMER_LAYERS = 30\n",
    "TRANSFORMER_HEADS = 6\n",
    "BALANCE_POS_CODE = True        # True: Weakens positional code compared to embedded values.\n",
    "DROPOUT = 0.0  # 0.1\n",
    "ADAPTORS_LAYERS = 0 #------------ 10\n",
    "ADAPTORS_WIDTH_FACTOR = 30  # 30\n",
    "\n",
    "# This is an exponential curve that hits STARTING_LEARNING_RATE at step zero and EXAMPLE_LEARNING_RATE at step EXAMPLE_LEARNING_STEP.\n",
    "# lr(step) = STARTING_LEARNING_RATE * pow( pow(EXAMPLE_LEARNING_RATE/STARTING_LEARNING_RATE, 1/EXAMPLE_LEARNING_STEP), step )\n",
    "STARTING_LEARNING_RATE = 2e-7\n",
    "EXAMPLE_LEARNING_RATE = STARTING_LEARNING_RATE * 0.2\n",
    "EXAMPLE_LEARNING_STEP = 100\n",
    "\n",
    "MODEL_ACTIVATION = 'softmax'    # 'softmax', 'sigmoid', 'relu', 'open'\n",
    "LOSS_TYPE = 'profit'           # 'profit', 'entropy'\n",
    "VECTOR_BETTING = False\n",
    "MODEL_TYPE_CHECK = True\n",
    "GET_VAL_LOSS_0 = False\n",
    "IGNORE_HISTORY = False\n",
    "SIMPLIFY_ADAPTOR = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#-------------------- England ---------------------\\nTime range of data: 2004/2005 - 2024/2025\\nLeagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\\n!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\\nBookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\\nWilliam Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\\nWilliam Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\\nWilliam Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\\nWilliam Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\\nWilliam Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\\nBWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COUNTRY = 'England'\n",
    "NUMBER_BOOKIES = 5  # Take William Hills, Bet&Win, and Bet365. Other bookies' odds list change over years and leagues.\n",
    "PREFERED_ORDER = ['B365', 'WH']   # Implies the order only.\n",
    "BOOKIE_TO_EXCLUDE = []    # 'BWIN' odds don't show up since mid Febrary 2025. This may reduce the effective NUMBER_BOOKIES.\n",
    "DIVIISONS = ['E0', 'E1', 'E2', 'E3']    # 'EC', the Conference league, is excluded as some odds makers are not archived for the league since 2013.\n",
    "\n",
    "\"\"\"\n",
    "#-------------------- England ---------------------\n",
    "Time range of data: 2004/2005 - 2024/2025\n",
    "Leagues - Premiere, League 1, League 2, Championship, Conference (dropped out)\n",
    "!!! Conference will be dropped out because they lack Shoot_cols and ShootT_cols since 2026. Those columns are important.\n",
    "Bookies - Bookie1 : WH(William Hills), Bookie2: BW(Bet&Win), Bookie3 : B365(Bet365), Bookie4 : Mixed\n",
    "William Hills, Bet365 on Premier: https://www.oddschecker.com/football/english/premier-league\n",
    "William Hills, Bet365 on Championship: https://www.oddschecker.com/football/english/championship\n",
    "William Hills, Bet365 on League-1: https://www.oddschecker.com/football/english/league-1\n",
    "William Hills, Bet365 on League-2: https://www.oddschecker.com/football/english/league-2\n",
    "William Hills, Bet365 on Conference: https://www.oddschecker.com/football/english/non-league/national-league\n",
    "BWin(Bet&Win) on Premier and Chanpionship: https://sports.bwin.fr/fr/sports/football-4/paris-sportifs/angleterre-14\n",
    "\"\"\"\n",
    "\n",
    "# COUNTRY = 'Scotland'\n",
    "# NUMBER_BOOKIES = 3  # Take ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed): np.random.seed(seed); tf.random.set_seed(seed); random.seed(seed)\n",
    "set_seed(23)    # For serendipity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ODDS_COLS = []\n",
    "for b in range(NUMBER_BOOKIES):\n",
    "    ODDS_COLS += ['HDA'+str(b)+'H', 'HDA'+str(b)+'D', 'HDA'+str(b)+'A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_cols = ['id']\n",
    "Div_cols = ['Div']\n",
    "Date_cols = ['Date']\n",
    "Team_cols = ['HomeTeam', 'AwayTeam']\n",
    "Odds_cols = ODDS_COLS\n",
    "BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "\n",
    "Half_Goal_cols = ['HTHG', 'HTAG']\n",
    "Full_Goal_cols = ['FTHG', 'FTAG']\n",
    "Goal_cols = Half_Goal_cols + Full_Goal_cols\n",
    "Shoot_cols = ['HS', 'AS']\n",
    "ShootT_cols = ['HST', 'AST']\n",
    "Corner_cols = ['HC', 'AC']\n",
    "Faul_cols = ['HF', 'AF']\n",
    "Yellow_cols = ['HY', 'AY']    # H/A Yellow Cards, H/A Red Cards\n",
    "AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "Required_Non_Odds_cols = Div_cols + Date_cols + Team_cols + Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "\n",
    "# underscore_prefixed lists have discontinued columns.\n",
    "BBAB_cols = BB_cols + AB_cols\n",
    "\n",
    "# Make sure Odds_cols comes first !!!\n",
    "_Cols_to_Always_Normalize = Odds_cols\n",
    "\n",
    "_Label_cols = Full_Goal_cols + Odds_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A', 'HDA2H', 'HDA2D', 'HDA2A', 'HDA3H', 'HDA3D', 'HDA3A', 'HDA4H', 'HDA4D', 'HDA4A', 'HTHG', 'HTAG', 'FTHG', 'FTAG', 'HS', 'AS', 'HST', 'AST', 'HC', 'AC', 'HF', 'AF', 'HY', 'AY']\n",
      "['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A', 'HDA2H', 'HDA2D', 'HDA2A', 'HDA3H', 'HDA3D', 'HDA3A', 'HDA4H', 'HDA4D', 'HDA4A']\n",
      "['FTHG', 'FTAG', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D', 'HDA1A', 'HDA2H', 'HDA2D', 'HDA2A', 'HDA3H', 'HDA3D', 'HDA3A', 'HDA4H', 'HDA4D', 'HDA4A']\n"
     ]
    }
   ],
   "source": [
    "print(BBAB_cols)\n",
    "print(BB_cols)\n",
    "print(_Label_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "THEME = '_theme_' + THEME\n",
    "CountryFolder = \"./data/football-data-co-uk/\" + COUNTRY\n",
    "CountryThemeFolderPath = os.path.join(CountryFolder, THEME)\n",
    "if not os.path.exists(CountryThemeFolderPath): os.makedirs(CountryThemeFolderPath, exist_ok=False)\n",
    "id_map_foler_path = os.path.join(CountryThemeFolderPath, '_id_map')\n",
    "if not os.path.exists(id_map_foler_path): os.makedirs(id_map_foler_path, exist_ok=False)\n",
    "dataset_foler_path = os.path.join(CountryThemeFolderPath, '_dataaset')\n",
    "if not os.path.exists(dataset_foler_path): os.makedirs(dataset_foler_path, exist_ok=False)\n",
    "checkpoint_folder_path = os.path.join(CountryThemeFolderPath, '_checkpoint')\n",
    "if not os.path.exists(checkpoint_folder_path): os.makedirs(checkpoint_folder_path, exist_ok=False)\n",
    "tokenizer_folder_path =  os.path.join(CountryThemeFolderPath, '_tokenizer')\n",
    "if not os.path.exists(tokenizer_folder_path): os.makedirs(tokenizer_folder_path, exist_ok=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "files found to rename:  0\n",
      "total rows in renamed files:  0\n",
      "files found to rename:  0\n",
      "total rows in renamed files:  0\n",
      "E1-2004-2005.xlsx E1-2005-2006.xlsx E1-2006-2007.xlsx E1-2007-2008.xlsx E1-2008-2009.xlsx E1-2009-2010.xlsx E1-2010-2011.xlsx E1-2011-2012.xlsx E1-2012-2013.xlsx E1-2013-2014.xlsx E1-2014-2015.xlsx E1-2015-2016.xlsx E1-2016-2017.xlsx E1-2017-2018.xlsx E1-2018-2019.xlsx E1-2019-2020.xlsx E1-2020-2021.xlsx E1-2021-2022.xlsx E1-2022-2023.xlsx E1-2023-2024.xlsx E1-2024-2025.xlsx E2-2004-2005.xlsx E2-2005-2006.xlsx E2-2006-2007.xlsx E2-2007-2008.xlsx E2-2008-2009.xlsx E2-2009-2010.xlsx E2-2010-2011.xlsx E2-2011-2012.xlsx E2-2012-2013.xlsx E2-2013-2014.xlsx E2-2014-2015.xlsx E2-2015-2016.xlsx E2-2016-2017.xlsx E2-2017-2018.xlsx E2-2018-2019.xlsx E2-2019-2020.xlsx E2-2020-2021.xlsx E2-2021-2022.xlsx E2-2022-2023.xlsx E2-2023-2024.xlsx E2-2024-2025.xlsx E3-2004-2005.xlsx E3-2005-2006.xlsx E3-2006-2007.xlsx E3-2007-2008.xlsx E3-2008-2009.xlsx E3-2009-2010.xlsx E3-2010-2011.xlsx E3-2011-2012.xlsx E3-2012-2013.xlsx E3-2013-2014.xlsx E3-2014-2015.xlsx E3-2015-2016.xlsx E3-2016-2017.xlsx E3-2017-2018.xlsx E3-2018-2019.xlsx E3-2019-2020.xlsx E3-2020-2021.xlsx E3-2021-2022.xlsx E3-2022-2023.xlsx E3-2023-2024.xlsx E3-2024-2025.xlsx E0-2004-2005.xlsx E0-2005-2006.xlsx E0-2006-2007.xlsx E0-2007-2008.xlsx E0-2008-2009.xlsx E0-2009-2010.xlsx E0-2010-2011.xlsx E0-2011-2012.xlsx E0-2012-2013.xlsx E0-2013-2014.xlsx E0-2014-2015.xlsx E0-2015-2016.xlsx E0-2016-2017.xlsx E0-2017-2018.xlsx E0-2018-2019.xlsx E0-2019-2020.xlsx E0-2020-2021.xlsx E0-2021-2022.xlsx E0-2022-2023.xlsx E0-2023-2024.xlsx E0-2024-2025.xlsx \n",
      "84  files found\n",
      "oGroups count-ordered  ['WH', 'BW', 'B365', 'IW', 'VC', 'LB', 'PSC', 'PS', 'SJ', 'GB', 'SB', 'WHC', 'MaxC', 'Max', 'BWC', 'BS', 'B365C', 'AvgC', 'Avg', 'VCC', 'IWC', 'BFEC', 'BFE', 'BFC', 'BF', '1XBC', '1XB']\n",
      "Rename Vector:  HDA0 ['B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365', 'B365']\n",
      "Rename Vector:  HDA1 ['BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW', 'BW']\n",
      "Rename Vector:  HDA2 ['WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH', 'WH']\n",
      "Rename Vector:  HDA3 ['IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'PSC', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'PSC', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'PSC', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'IW', 'PSC']\n",
      "Rename Vector:  HDA4 ['LB', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'PS', 'LB', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'PS', 'LB', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'PS', 'LB', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'VC', 'PS']\n",
      "1 ['HDA0', 'HDA1', 'HDA4', 'HDA2', 'HDA3']\n",
      "df_built cols:  Index(['id', 'Div', 'Date', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR',\n",
      "       'HTHG', 'HTAG', 'HTR', 'HS', 'AS', 'HST', 'AST', 'HF', 'AF', 'HC', 'AC',\n",
      "       'HY', 'AY', 'HR', 'AR', 'HDA0H', 'HDA0D', 'HDA0A', 'HDA1H', 'HDA1D',\n",
      "       'HDA1A', 'HDA4H', 'HDA4D', 'HDA4A', 'HDA2H', 'HDA2D', 'HDA2A', 'HDA3H',\n",
      "       'HDA3D', 'HDA3A'],\n",
      "      dtype='object')\n",
      "creating an empty df_new...\n",
      "df_grown:  (42095, 38)\n",
      "df_new:  (0, 38)\n"
     ]
    }
   ],
   "source": [
    "data_helpers.assign_seasonal_filenames(CountryThemeFolderPath)\n",
    "df_grown, df_new = data_helpers.get_grown_and_new_from_football_data_v2(CountryFolder, Required_Non_Odds_cols, NUMBER_BOOKIES, oddsGroupsToExclude = BOOKIE_TO_EXCLUDE, preferedOrder = PREFERED_ORDER, theme=THEME, train_mode = TRAIN_MODE, skip=True)\n",
    "if df_grown is not None: print(\"df_grown: \", df_grown.shape)\n",
    "if df_new is not None: print(\"df_new: \", df_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "def createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens):\n",
    "        tokenizer = Tokenizer(models.WordLevel(unk_token=unknown_token))\n",
    "        tokenizer.normalizer = normalizers.Sequence([normalizers.NFD(), normalizers.Lowercase(), normalizers.StripAccents()])\n",
    "        tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "        trainer = trainers.WordLevelTrainer(vocab_size=vocab_size, special_tokens=special_tokens)\n",
    "        tokenizer.train(corpus_files, trainer=trainer)\n",
    "        tokenizer.decoder = decoders.WordPiece(prefix=\" \")\n",
    "        return tokenizer\n",
    "\n",
    "def creat_team_tokenizer_uk(master_df):\n",
    "    teams = list(set(list(master_df['HomeTeam']) + list(master_df['AwayTeam'])))\n",
    "    teams_string = [str(team) for team in teams]\n",
    "    teams_string = [re.sub(r\"\\s\", \"_\", item) for item in teams_string]    # replace spaces with a '_'\n",
    "    teams_text = \" \".join(teams_string)\n",
    "\n",
    "    corpus_file = os.path.join(tokenizer_folder_path, 'team_ids_text_uk.txt')\n",
    "    f = open(corpus_file, \"w+\", encoding=\"utf-8\")\n",
    "    f.write(teams_text)\n",
    "    f.close()\n",
    "\n",
    "    corpus_files = [corpus_file]\n",
    "    unknown_token = config['unknown_token']\n",
    "    special_tokens = [unknown_token] ################### + [\"[HOME]\", \"[AWAY]\"]\n",
    "    vocab_size = len(teams_string) + len(special_tokens)\n",
    "\n",
    "    tokenizer_team = createSimpleTokenizer(corpus_files, vocab_size, unknown_token, special_tokens)\n",
    "    return tokenizer_team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n",
      "['tottenham', 'arsenal', 'liverpool', '[UNK]', 'tottenham', 'chelsea', '[UNK]', 'man_united', '[UNK]', '[UNK]', '[UNK]']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[102, 4, 59, 0, 102, 28, 0, 63, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tottenham arsenal liverpool tottenham chelsea man_united'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_team = creat_team_tokenizer_uk(df_grown)\n",
    "print(tokenizer_team.get_vocab_size())\n",
    "\n",
    "teams = ['Tottenham', 'Arsenal', 'Liverpool', 'what?', 'Tottenham', 'Chelsea', 'e_t', 'Man United', '1234', '[HOME]', '[AWAY]']\n",
    "teams = [team.strip() for team in [re.sub(r\"\\s\", \"_\", item) for item in teams]]\n",
    "teams = \" \".join(teams)\n",
    "encoding = tokenizer_team.encode(teams)\n",
    "# encoding = tokenizer.encode(\"\")\n",
    "print(encoding.tokens)\n",
    "print(encoding.type_ids)\n",
    "print(encoding.ids)\n",
    "\n",
    "tokenizer_team.decode(encoding.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_id: 1037406, report: 200, days_span: 3358, games[:10]: [1037405, 1037364, 1037330, 1037328, 1037299, 1037294, 1037293, 1037272, 1037268, 1037245]\r"
     ]
    }
   ],
   "source": [
    "#================= build_id_to_ids.ipynb\n",
    "\n",
    "targetLength = HISTORY_LEN          # A target game will be explained by maximum 'targetLength' count of past games.\n",
    "\n",
    "id_map_filename = str(targetLength) + '-' + str(SEQ_YEAR_SPAN).zfill(2)\n",
    "id_map_filePath = os.path.join(id_map_foler_path, id_map_filename + \".json\")\n",
    "\n",
    "df_total = df_grown; df_search = df_grown\n",
    "id_map = data_helpers.fixture_id_to_ids_uk_v3(id_map_foler_path, id_map_filename, targetLength, df_total, year_span=SEQ_YEAR_SPAN, testcount=-1)\n",
    "data_helpers.SaveJsonData(id_map, id_map_filePath)\n",
    "\n",
    "print(len(id_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [len(ids) for (report, ids) in id_map.values()]\n",
    "# maxLen = max(lengths)\n",
    "# plt.hist(lengths, np.linspace(0, int(maxLen*1.1), int(maxLen*1.1) + 1))\n",
    "# plt.ylim(plt.ylim())\n",
    "# maxLen = max(lengths)\n",
    "# # plt.plot([maxLen, maxLen], plt.ylim())\n",
    "# plt.title(f'Max length of ids: {maxLen}')\n",
    "\n",
    "# lengths = [len(ids) for (report, ids) in id_map.values()]\n",
    "# maxLen = max(lengths)\n",
    "# plt.hist(lengths, np.linspace(0, int(maxLen*0.9), int(maxLen*0.9) + 1))\n",
    "# plt.ylim(plt.ylim())\n",
    "# maxLen = max(lengths)\n",
    "# # plt.plot([maxLen, maxLen], plt.ylim())\n",
    "# plt.title(f'Max length of ids: {maxLen}')\n",
    "# assert maxLen == HISTORY_LEN\n",
    "\n",
    "MAX_TOKENS = HISTORY_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_raw_bbab(bbab, tokenizer_team, normalization_parms, train_mode=True):\n",
    "\n",
    "    label = []\n",
    "    if train_mode:\n",
    "        #---------------------- label, before changing bbab. They are raw full-time goals and raw odds.\n",
    "        for col in _Label_cols:\n",
    "            start = BBAB_cols.index(col)\n",
    "            label.append(bbab[start])\n",
    "\n",
    "    #----------------------- \n",
    "    start = BBAB_cols.index(Div_cols[0])\n",
    "    Div = bbab[start]\n",
    "    bbab[start] = DIVIISONS.index(Div)  # Assumes no n/a\n",
    "\n",
    "    start = BBAB_cols.index(Team_cols[0]); end = BBAB_cols.index(Team_cols[-1]) + 1\n",
    "    pair_str = [str(team) for team in bbab[start : end]]    # Team names are already normalized, removing/striping spaces.\n",
    "    pair_text = \" \".join(pair_str)\n",
    "    pair_tokens = tokenizer_team.encode(pair_text).ids\n",
    "    bbab[start : end] = pair_tokens # 0 for Unknown, by tokenizer trainig.\n",
    "\n",
    "    #--------------------- normalize\n",
    "    for col in _Cols_to_Always_Normalize:   # Odds_cols only now.\n",
    "        (mean, std, maximum) = normalization_parms[col]\n",
    "        start = BBAB_cols.index(col)\n",
    "        bbab[start] = (bbab[start] - mean) / std\n",
    "\n",
    "    #--------------------- columns for positional embedding\n",
    "    start = BBAB_cols.index(Date_cols[0])   #\n",
    "    date = bbab[start]\n",
    "    bbab[start] = (datetime.datetime.combine(date, datetime.time(0,0,0)) - config['baseDate']).days  # either positive or negative\n",
    "\n",
    "    #---------------------- bb only\n",
    "    start = BBAB_cols.index(BB_cols[0]); end = start + len(BB_cols)     # \n",
    "    bb = bbab[start : end]\n",
    "\n",
    "    return bbab, bb, label, date\n",
    "\n",
    "def getDateDetails(date):\n",
    "    baseYear = config['baseDate'].year\n",
    "    date_details = tf.Variable([date.year - baseYear, date.month, date.day, date.weekday()], dtype=tf.int32, trainable=False)\n",
    "    return date_details     # (4,)\n",
    "\n",
    "filler = tf.zeros_like([0] * len(BBAB_cols), dtype=tf.float32)\n",
    "\n",
    "def get_data_record(df_total, baseId, ids, tokenizer_team, normalization_parms, train_mode=True):\n",
    "    # try:\n",
    "        # base_bbab = list(df_grown.loc[df_grown['id'] == baseId, BBAB_cols])\n",
    "        if train_mode:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BBAB_cols].iloc[0, :])  # base_bbab follows BBAB. list\n",
    "        else:\n",
    "            base_bbab = list(df_total[df_total['id'] == baseId][BB_cols].iloc[0, :])  # base_bbab follows BB. list\n",
    "\n",
    "        base_bbab, base_bb, base_label, base_date = standardize_raw_bbab(base_bbab, tokenizer_team, normalization_parms, train_mode=train_mode)\n",
    "        # base_bbab, base_bb, base_label, base_date\n",
    "        baseId = tf.Variable(baseId, dtype=tf.int32, trainable=False)\n",
    "        base_bbab = tf.Variable(base_bbab, dtype=tf.float32, trainable=False)    # (len(BBAB_cols),)\n",
    "        base_bb = tf.Variable(base_bb, dtype=tf.float32, trainable=False)        # (len(BB_cols),)\n",
    "        base_label = tf.Variable(base_label, dtype=tf.float32, trainable=False)  # (len(_Label_cols),)\n",
    "        # print('3', base_bbab)\n",
    "        # Default sequence.\n",
    "        sequence = tf.transpose(tf.Variable([[]] * len(BBAB_cols), dtype=tf.float32, trainable=False))   # (0, len(BBAB_cols))\n",
    "        # sequence = np.array([[]] * len(BBAB_cols), dtype=config['np_float']).T\n",
    "        # print('3.5', sequence)\n",
    "        baseDateDetails = getDateDetails(base_date) # (4,)\n",
    "\n",
    "        concat = []\n",
    "        for id in ids:\n",
    "            bbab = list(df_total[df_total['id'] == id][BBAB_cols].iloc[0, :])   # bbab follows BBAB. list\n",
    "            # print('4', bbab)\n",
    "            bbab, _, _, _ = standardize_raw_bbab(bbab, tokenizer_team, normalization_parms, train_mode=train_mode)   # bbab follows BBAB. list\n",
    "            # check_normalization(bbab, normalization_parms)\n",
    "\n",
    "            bbab = tf.Variable(bbab, dtype=tf.float32, trainable=False)[tf.newaxis, :]       # (1, len(BBAB_cols))\n",
    "            # _bbab = bbab[0].numpy()\n",
    "            # check_normalization(_bbab, normalization_parms)\n",
    "\n",
    "            concat.append(bbab)     # concat doesn't create a new axis.\n",
    "\n",
    "        if len(concat) > 0:\n",
    "            sequence = tf.concat(concat, axis=0)    # (nSequence, len(BBAB_cols))\n",
    "            # if sequence.shape[0] > 0:\n",
    "            #     bbab = sequence[0].numpy()\n",
    "            #     check_normalization(bbab, normalization_parms)\n",
    "\n",
    "        seq_len_org = sequence.shape[0]\n",
    "        nMissings = MAX_TOKENS - seq_len_org\n",
    "        if nMissings > 0:\n",
    "            patch = tf.stack([filler] * nMissings, axis=0)\n",
    "            sequence = tf.concat([sequence, patch], axis=0)     # concat doesn't create a new axis. (MAX_TOKENS, len(BBAB_cols))\n",
    "        base_bb = base_bb[tf.newaxis, :]    # shape: (seq_len = 1, len(BBAB_cols))\n",
    "        baseDateDetails = baseDateDetails[tf.newaxis, :]\n",
    "        mask = tf.Variable([1] * seq_len_org + [0] * nMissings, dtype=tf.int32, trainable=False) # (MAX_TOKENS,) ## DO NOT USE tf.constant !!! unstable.\n",
    "        mask = mask[:, tf.newaxis] & mask[tf.newaxis, :]    # (MAX_TOKENS, MAX_TOKENS)\n",
    "\n",
    "        return (baseId, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "\n",
    "def generate_dataset_uk(df_total, fixture_id_to_ids, tokenizer_team, normalization_parms, train_mode=True):\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for baseId, (report, ids) in fixture_id_to_ids.items():\n",
    "            baseId = int(baseId)\n",
    "            baseId, sequence, base_bb, base_label, baseDateDetails, mask = get_data_record(df_total, baseId, ids, tokenizer_team, normalization_parms, train_mode=train_mode)\n",
    "            print(\"count: {}, baseId: {}\".format(count, baseId), end='\\r'); count += 1\n",
    "            # if count > 200: break\n",
    "            yield (baseId, sequence, base_bb, base_label, baseDateDetails, mask)\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS))),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "   \n",
    "def X_versus_Y(baseId, sequence, base_bb, base_label, baseDateDetails, mask):\n",
    "    return (baseId, sequence, base_bb, baseDateDetails, mask), (base_label)\n",
    "\n",
    "# I found, in PositionalEmbedding, batch size ranges between 3, 4, 6 and 8, while they should be 4 or 8, except margial rows. Check it.\n",
    "train_batch_size = BATCH_SIZE\n",
    "test_batch_size = BATCH_SIZE * 2\n",
    "\n",
    "def apply_train_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .shuffle(BUFFER_SIZE)       # Shuffle the training dataset.\n",
    "        .batch(train_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )\n",
    "\n",
    "def apply_test_pipeline(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .batch(test_batch_size)\n",
    "        .map(X_versus_Y, tf.data.AUTOTUNE)\n",
    "        .cache()\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_path = os.path.join(dataset_foler_path, id_map_filename + '-normalization' + \".json\")\n",
    "normalization_parms = data_helpers.get_normalization_params(df_grown, cols=_Cols_to_Always_Normalize + AB_cols)\n",
    "print(normalization_parms)\n",
    "data_helpers.SaveJsonData(normalization_parms, std_path)\n",
    "normalization_parms = data_helpers.LoadJsonData(std_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN_MODE:\n",
    "    ds_path = os.path.join(dataset_foler_path, id_map_filename + '-foundation-ds')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_grown, id_map, tokenizer_team, normalization_parms)\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size = len(ds)\n",
    "\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "starting_size = int(STARTING_PERCENT/100 * total_size)\n",
    "ending_size = int(ENDING_PERCENT/100 * total_size)\n",
    "take_size = total_size - starting_size - ending_size\n",
    "remaining_ds = ds.skip(starting_size)\n",
    "dataset = remaining_ds.take(take_size)          # starting | dataset | ending\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(TRAIN_PERCENT/100 * dataset_size)\n",
    "valid_size = int(VALID_PERCENT/100 * dataset_size)\n",
    "test_size = dataset_size - train_size - valid_size      # len(dataset) = train_size + valid_size + tast_size        NO back_size\n",
    "train_ds = dataset.take(train_size)                    # dataset[: train_size]\n",
    "remaining_ds = dataset.skip(train_size - valid_size)    # dataset[train_size - valid_size: ]\n",
    "back_ds = remaining_ds.take(valid_size)                # dataset[train_size - valid_size: train_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)            # dataset[train_size: ]\n",
    "valid_ds = remaining_ds.take(valid_size)               # dataset[train_size, train_size + valid_size]\n",
    "remaining_ds = remaining_ds.skip(valid_size)                # dataset[train_size + valid_size :]\n",
    "test_ds = remaining_ds.take(test_size)\n",
    "backtest_ds = valid_ds.concatenate(test_ds) # tf.data.Dataset.zip((valid_ds, test_ds))\n",
    "\n",
    "assert len(test_ds) == test_size\n",
    "assert dataset_size == len(train_ds) + len(valid_ds) + len(test_ds)\n",
    "\n",
    "print(\"total_size, dataset, train_ds, back_ds, valid_ds, test_ds, backtest_ds: \", \\\n",
    "      total_size, len(dataset), len(train_ds), len(back_ds), len(valid_ds), len(test_ds), len(backtest_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_baseDateDetails(ds):\n",
    "    baseId = baseDateDetails = None\n",
    "    for z in ds:\n",
    "        baseId, _, _, _, baseDateDetails, _, _ = z\n",
    "        break\n",
    "    return baseId.numpy(), baseDateDetails.numpy()\n",
    "# print(\"train_ds's first game: \", find_first_baseDateDetails(train_ds))\n",
    "# print(\"back_ds's first game: \", find_first_baseDateDetails(back_ds))\n",
    "# print(\"valid_ds's first game: \", find_first_baseDateDetails(valid_ds))\n",
    "# print(\"test_ds's first game: \", find_first_baseDateDetails(test_ds))\n",
    "\"\"\"\n",
    "train_ds's first game:  (1000001, array([[4, 8, 7, 5]]))\n",
    "back_ds's first game:  (1037717, array([[23,  2, 18,  5]]))\n",
    "valid_ds's first game:  (1039366, array([[23, 12, 22,  4]]))\n",
    "test_ds's first game:  (1041019, array([[24, 11,  2,  5]]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_batches = apply_train_pipeline(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    cnt = 5\n",
    "    for z in train_batches:\n",
    "        (baseId, sequence, base_bb, baseDateDetails, mask), (base_label) = z\n",
    "        cnt -= 1 \n",
    "        if cnt == 0: break\n",
    "    print(baseId.shape, sequence.shape, base_bb.shape, mask.shape, base_label.shape, baseDateDetails.shape)\n",
    "    sample_x = (sequence, base_bb, baseDateDetails, mask)\n",
    "    sample_y = (base_label)\n",
    "    # print(baseId.numpy(), base_bb.numpy(), baseDateDetails.numpy()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_batches = apply_test_pipeline(back_ds)\n",
    "valid_batches = apply_test_pipeline(valid_ds)\n",
    "test_batches = apply_test_pipeline(test_ds)\n",
    "backtest_batches = apply_test_pipeline(backtest_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "class PositionalEmbedding(tf.keras.Model):\n",
    "\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # if BALANCE_POS_CODE:\n",
    "        #     self.total_years = 10   # not sure if it will cover all (baseDayss - sequenceDays)\n",
    "        # else:\n",
    "        self.total_years = SEQ_YEAR_SPAN  # datetime.datetime.now().year - config['baseDate'].year + 1 + 3   # 3 extra years. ------------\n",
    "        self.total_days = 365 * self.total_years\n",
    "\n",
    "        positional_resolution = d_model\n",
    "        quotient = self.total_days / positional_resolution\n",
    "        positions = tf.constant(range(self.total_days), dtype=tf.float32)    # (total_days,). range [0, total_days)\n",
    "        fractional_pos = positions / quotient  # (total_days,). range (0, d_model)\n",
    "        half_depth = d_model/2   #\n",
    "        depths = tf.range(half_depth, dtype=tf.float32) / half_depth  # (half_depth,). range [0, 1), linear.\n",
    "        BIG = d_model * 0.8\n",
    "        depths = 1.0 / tf.pow(BIG, depths)        # (depth,). range [1, 1/BIG)\n",
    "        angle_rads = fractional_pos[:, tf.newaxis] * depths  # (total_days, half_depth,). range [dayPos, dayPos/BIG) for each day\n",
    "        pos_encoding = tf.concat([tf.math.sin(angle_rads), tf.math.cos(angle_rads)], axis=-1)   # Seperated sin and cos. (batch, seq_len, d_model)\n",
    "        self.total_positional_code = pos_encoding\n",
    "        return\n",
    "\n",
    "    def call(self, x, sequenceDays, baseDays, isEncoder):\n",
    "\n",
    "        if IGNORE_HISTORY: pass # x: (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        else:\n",
    "            if BALANCE_POS_CODE:\n",
    "                positions = tf.cast(baseDays - sequenceDays, dtype=tf.int32) if isEncoder else tf.cast(baseDays - baseDays, dtype=tf.int32)\n",
    "            else:\n",
    "                positions = tf.cast(self.total_days - sequenceDays, dtype=tf.int32) if isEncoder else tf.cast(self.total_days - baseDays, dtype=tf.int32)\n",
    "            positional_code = tf.gather(self.total_positional_code, positions)\n",
    "            if BALANCE_POS_CODE:\n",
    "                positional_code /= tf.math.sqrt(tf.cast(x.shape[-1], tf.float32)) ################# rather than multiply to x. This makes a balance. Not in \"Attention is all you need.\"\n",
    "            x = x + positional_code\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbedding(150)\n",
    "positions = tf.constant(range(pe.total_days), dtype=tf.int32)\n",
    "positions = pe.total_days - positions\n",
    "pcode = tf.gather(pe.total_positional_code, positions)\n",
    "# print(pcode)\n",
    "plt.pcolormesh(pcode.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pe = PositionalEmbedding(150)\n",
    "positions = tf.constant(range(0, 365 * 25, 365), dtype=tf.int32)\n",
    "positions = pe.total_days - positions\n",
    "pcode = tf.gather(pe.total_positional_code, positions)\n",
    "# print(pcode)\n",
    "plt.pcolormesh(pcode.numpy().T, cmap='RdBu')\n",
    "plt.ylabel('Depth')\n",
    "plt.xlabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class hParams:    \n",
    "    nDivisions = len(DIVIISONS) \n",
    "    division_embs = 4   # [0, 1, 2, 3]\n",
    "\n",
    "    nTeams = tokenizer_team.get_vocab_size()    # including Unknown\n",
    "    team_embs = TEAM_EMBS\n",
    "    nGoals  = 4  # [0, 1, 2, 3] Maximum nGoals goals for a team in each of 1st and 2nd halfs. Extra goals will be clipped_by_value.\n",
    "    goal_embs = 4\n",
    "    nShoots = 21    # [0, ..., 20]\n",
    "    shoot_embs = 4 # for combination\n",
    "    nShootTs = 11   # [0, ..., 10]\n",
    "    shootT_embs = 4 # for combination\n",
    "    nCorners = 11   # [0, ..., 10]\n",
    "    corner_embs = 4 # for combination\n",
    "    nFauls = 21     # [0, ..., 20]\n",
    "    faul_embs = 2 # for combination\n",
    "    nYellows = 5    # [0, ..., 4]\n",
    "    yellow_embs = 2 # for combination\n",
    "    \n",
    "    batch_size = BATCH_SIZE\n",
    "    days_spanning_years = 30\n",
    "    num_layers = TRANSFORMER_LAYERS\n",
    "    num_heads = TRANSFORMER_HEADS\n",
    "    m365_size = 1\n",
    "    initial_m365 = 0.9\n",
    "\n",
    "    # d_model DEPENDS...\n",
    "    # We want as small d_model as possible, because from which we can freely choose an actual d_model.\n",
    "    # Tests seem to indicate that larger d_model leads to training overfitting and validation underfitting.\n",
    "    \n",
    "    # Where goes ODDS_IN_DECODER ????????????????????\n",
    "\n",
    "    d_model = 1 * division_embs + 2 * team_embs     # 1 division, 2 teams\n",
    "    if ODDS_IN_ENCODER: d_model = d_model + 3 * NUMBER_BOOKIES  # + 3 odds * nBookies\n",
    "    d_encoder = d_decoder = 0\n",
    "    if EMBED_AB_COLS:\n",
    "        d_encoder = d_model + 1 * goal_embs + 1 * goal_embs + 1 * (shoot_embs + shootT_embs + corner_embs + faul_embs + yellow_embs)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    else:   # This mode, EMBED_AB_COLS = False, gives much smaller d_moddel, maybe avoiding overfitting.\n",
    "        d_encoder = d_model + 1 * len(AB_cols)\n",
    "        if DECODE_BASE_DATE: d_decoder = d_model + 2 * 4     # 2 * 4 : date details\n",
    "        else: d_decoder = d_model\n",
    "    d_model = max(d_encoder, d_decoder)     # (136, 118) for EMBED_AB_COLS, (126, 118) for False EMBED_AB_COLS\n",
    "    d_model += ADDITIONAL_D_MODEL               # Adjust for the model size and overfitting.\n",
    "    d_model = d_model + d_model % 2     # make it an even number.\n",
    "    print(\"d_model raw: \", d_model)\n",
    "    d_model = int( (d_model-1) / num_heads) * num_heads + num_heads  # make it a multiple of num_heads.\n",
    "    print(\"d_model refined: \", d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess(tf.keras.Model):\n",
    "    def __init__(self, hParams, isEncoder):\n",
    "        super().__init__()\n",
    "        self.isEncoder = isEncoder\n",
    "\n",
    "        # game\n",
    "        self.division_emb = tf.keras.layers.Embedding(hParams.nDivisions, hParams.division_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "        self.team_emb = tf.keras.layers.Embedding(hParams.nTeams, hParams.team_embs, dtype=tf.float32, mask_zero=False) # Learn Unknown\n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                # AB_cols\n",
    "                self.firstH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.secondH_goal_emb = tf.keras.layers.Embedding(hParams.nGoals * hParams.nGoals, hParams.goal_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shoot_emb = tf.keras.layers.Embedding(hParams.nShoots * hParams.nShoots, hParams.shoot_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.shootT_emb = tf.keras.layers.Embedding(hParams.nShootTs * hParams.nShootTs, hParams.shootT_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.corner_emb = tf.keras.layers.Embedding(hParams.nCorners * hParams.nCorners, hParams.corner_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.faul_emb = tf.keras.layers.Embedding(hParams.nFauls * hParams.nFauls, hParams.faul_embs, dtype=tf.float32, mask_zero=False)\n",
    "                self.yellow_emb = tf.keras.layers.Embedding(hParams.nYellows * hParams.nYellows, hParams.yellow_embs, dtype=tf.float32, mask_zero=False)\n",
    "            else:\n",
    "                params = []\n",
    "                for col in AB_cols:\n",
    "                    params.append(normalization_parms[col])\n",
    "                self.AB_mormalization_params = tf.Variable(params, dtype=tf.float32, trainable=False)  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "\n",
    "        if not self.isEncoder:\n",
    "            if DECODE_BASE_DATE:\n",
    "                self.day_emb = tf.keras.layers.Embedding(31, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.month_emb = tf.keras.layers.Embedding(12, 2, dtype=tf.float32, mask_zero=False)\n",
    "                self.wday_emb = tf.keras.layers.Embedding(7, 2, dtype=tf.float32, mask_zero=False)\n",
    "\n",
    "        if IGNORE_HISTORY: pass\n",
    "        else: self.dimensional_permution = tf.keras.layers.Dense(hParams.d_model)\n",
    "\n",
    "        self.idx_Days = BB_cols.index('Date')\n",
    "\n",
    "    def representDateDetails(self, dateDetails):\n",
    "        # dateDetails: (batch, 1, 4)\n",
    "        bYears, bMonths, bDays, bWDays = tf.split(dateDetails, [1, 1, 1, 1], axis=-1)   # All should be of (batch, seq_len = 1, 1)\n",
    "        bYears = tf.cast(bYears, dtype=tf.float32)  # (batch, seq_len = 1, 1)\n",
    "        bDays = self.day_emb(bDays)[:, :, -1]       # (batch, seq_len = 1, embs = 2)\n",
    "        bMonths = self.month_emb(bMonths)[:, :, -1] # (batch, seq_len = 1, embs = 2)\n",
    "        bWDays = self.wday_emb(bWDays)[:, :, -1]    # (batch, seq_len = 1, embs = 2)\n",
    "        # w = tf.Variable(np.math.pi / 25, dtype=tf.float32, trainable=False)    # 25 years are covered by pi or a half circle.\n",
    "        w = np.math.pi / 25\n",
    "        bYearsCos = tf.math.cos(bYears * w)\n",
    "        bYearsSin = tf.math.sin(bYears * w)\n",
    "        bYears = tf.concat([bYearsCos, bYearsSin], axis=-1)   # (batch, seq_len = 1, 1+1 = 2)\n",
    "        return bYears, bMonths, bDays, bWDays\n",
    "\n",
    "    def combined_embeddings_of_double_columns(self, emb_layer, columns, nValues):\n",
    "        # Assume emb_layer = Embedding(nValues * nValues, embs, mask_zero=False)\n",
    "        cols = tf.cast(columns, dtype=tf.int32)\n",
    "        cols = tf.clip_by_value(cols, 0, nValues-1)\n",
    "        combi = cols[:, :, 0] * nValues + cols[:, :, 1]   # (batch, seq_len, 1). [0, ..., nValues * nValues - 1]\n",
    "        combi = emb_layer(combi)\n",
    "        return combi    # (batch, seq_len, 1)\n",
    "\n",
    "    def call(self, x):\n",
    "        (sequence, base_bb, baseDateDetails, mask) = x # sob = sequence or base_bb\n",
    "        sequenceDays = sequence[:, :, self.idx_Days]  # (batch, seq_len)\n",
    "        baseDays = base_bb[:, :, self.idx_Days]   # (batch, 1)\n",
    "\n",
    "        # sequence follows BBAB, whereas base_bb follows \n",
    "        \n",
    "        # BB_cols = id_cols + Div_cols + Date_cols + Team_cols + Odds_cols\n",
    "        # AB_cols = Goal_cols + Result_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols\n",
    "\n",
    "        if self.isEncoder:\n",
    "            # ramainder: Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols + Red_cols  --- total 12 fields.\n",
    "            id, div, days, teams, odds, half_goals, full_goals, shoot, shootT, corner, faul, yellow\\\n",
    "            = tf.split(sequence, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), len(Half_Goal_cols), len(Full_Goal_cols), \\\n",
    "                                  len(Shoot_cols), len(ShootT_cols), len(Corner_cols), len(Faul_cols), len(Yellow_cols)], axis=-1)\n",
    "            # All shape of (batch, sequence, own_cols), all tf.flaot32\n",
    "        else:\n",
    "            id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)  \n",
    "            # remainder: [] \n",
    "            # All shape of (batch, 1, own_cols), guess., all tf.float32\n",
    "        \n",
    "        div = self.division_emb(tf.cast(div, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=1, division_embs)\n",
    "        div = tf.reshape(div, [div.shape[0], div.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=1*division_embs) --- \n",
    "        teams = self.team_emb(tf.cast(teams, dtype=tf.int32))   # (batch, MAX_TOKENS or 1, columns=2, team_embs)\n",
    "        teams = tf.reshape(teams, [teams.shape[0], teams.shape[1], -1]) # (batch, MAX_TOKENS or 1, extended_columns=2*team_embs) --- \n",
    "\n",
    "        if self.isEncoder:\n",
    "            if EMBED_AB_COLS:\n",
    "                first_half_goals = self.combined_embeddings_of_double_columns(self.firstH_goal_emb, half_goals, hParams.nGoals)\n",
    "                second_half_goals = self.combined_embeddings_of_double_columns(self.secondH_goal_emb, full_goals - half_goals, hParams.nGoals)\n",
    "                shoot = self.combined_embeddings_of_double_columns(self.shoot_emb, shoot, hParams.nShoots)\n",
    "                shootT = self.combined_embeddings_of_double_columns(self.shootT_emb, shootT, hParams.nShootTs)\n",
    "                corner = self.combined_embeddings_of_double_columns(self.corner_emb, corner, hParams.nCorners)\n",
    "                faul = self.combined_embeddings_of_double_columns(self.faul_emb, faul, hParams.nFauls)\n",
    "                yellow = self.combined_embeddings_of_double_columns(self.yellow_emb, yellow, hParams.nYellows)\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "                else: concat = [div, teams, first_half_goals, second_half_goals, shoot, shootT, corner, faul, yellow]\n",
    "\n",
    "            else:   # normalize now\n",
    "                # AB_cols = Half_Goal_cols + Full_Goal_cols + Shoot_cols + ShootT_cols + Corner_cols + Faul_cols + Yellow_cols\n",
    "                AB_values = [half_goals, full_goals, shoot, shootT, corner, faul, yellow]   # all (batch, 2 cols)\n",
    "                AB_values = tf.concat(AB_values, axis=-1) # (batch, seq_len, num_AB_cols=14)\n",
    "                # self.AB_mormalization_params  # (num AB_cols=14, 3 = <mean, std, maximum>)\n",
    "                AB_values = (AB_values - self.AB_mormalization_params[:, 0]) / self.AB_mormalization_params[:, 1]\n",
    "                if ODDS_IN_ENCODER: concat = [div, teams, odds, AB_values]\n",
    "                else: concat = [div, teams, AB_values]\n",
    "        else:\n",
    "            if DECODE_BASE_DATE:\n",
    "                bYears, bMonths, bDays, bWDays = self.representDateDetails(baseDateDetails)\n",
    "                if ODDS_IN_DECODER:  concat = [div, teams, odds, bYears, bMonths, bDays, bWDays]\n",
    "                else: concat = [div, teams, bYears, bMonths, bDays, bWDays]\n",
    "            else:\n",
    "                if ODDS_IN_DECODER: concat = [div, teams, odds]\n",
    "                else: concat = [div, teams]\n",
    "\n",
    "        concat = tf.concat(concat, axis=-1)\n",
    "        assert concat.shape[-1] <= hParams.d_model        \n",
    "\n",
    "        if IGNORE_HISTORY: pass # concat: (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "        else:\n",
    "            concat = self.dimensional_permution(concat)  # (batch, MAX_TOKENS or 1, hParams.d_model)\n",
    "\n",
    "        if self.isEncoder:  mask = mask     # (batch, MAX_TOKEN, MAX_TOKEN), although (batch, 1, MAX_TOKEN) will propagate.\n",
    "        else:   mask = mask[:, 0:concat.shape[1], :]    # concat: (batch, 1, MAX_TOKEN)\n",
    "\n",
    "        return concat, mask, sequenceDays, baseDays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isEncoder = True\n",
    "if MODEL_TYPE_CHECK:\n",
    "    pre = Preprocess(hParams, isEncoder=isEncoder)\n",
    "    x, mask, seqDays, bDays = pre(sample_x); pre.summary()\n",
    "    pos = PositionalEmbedding(hParams.d_model)\n",
    "    x = pos(x, seqDays, bDays, isEncoder=isEncoder); pos.summary()\n",
    "    @tf.function\n",
    "    def fun(input):\n",
    "        x, mask, sequenceDays, baseDays = pre(input)\n",
    "        x = pos(x, sequenceDays, baseDays, isEncoder=isEncoder)\n",
    "        return x, mask, sequenceDays, baseDays\n",
    "    x, mask, sequenceDays, baseDays = fun(sample_x)\n",
    "    print(x.shape, mask.shape, sequenceDays.shape, baseDays.shape)\n",
    "    del pre, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isEncoder = False\n",
    "if MODEL_TYPE_CHECK:\n",
    "    pre = Preprocess(hParams, isEncoder=isEncoder)\n",
    "    x, mask, seqDays, bDays = pre(sample_x); pre.summary()\n",
    "    pos = PositionalEmbedding(hParams.d_model)\n",
    "    x = pos(x, seqDays, bDays, isEncoder=isEncoder); pos.summary()\n",
    "    @tf.function\n",
    "    def fun(input):\n",
    "        x, mask, sequenceDays, baseDays = pre(input)\n",
    "        x = pos(x, sequenceDays, baseDays, isEncoder=isEncoder)\n",
    "        return x, mask, sequenceDays, baseDays\n",
    "    x, mask, sequenceDays, baseDays = fun(sample_x)\n",
    "    print(x.shape, mask.shape, sequenceDays.shape, baseDays.shape)\n",
    "    del pre, pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "      # tf.keras.layers.Dropout(dropout_rate)\n",
    "  ])\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    q: query shape == (..., seq_len_q, depth)\n",
    "    k: key shape == (..., seq_len_k, depth)\n",
    "    v: value shape == (..., seq_len_v, depth_v), seq_len_k == sel_len_v\n",
    "    mask: Float 0/1 tensor with shape broadcastable to (..., seq_len_q, seq_len_k). 1 surpresses the score to zero.\n",
    "  Returns:\n",
    "    output, attention_weights\n",
    "  \"\"\"\n",
    "\n",
    "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "  \n",
    "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  if mask is not None: scaled_attention_logits += (tf.cast(mask, dtype=tf.float32) * -1e9)\n",
    "\n",
    "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "  return output, attention_weights\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads):\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "    \n",
    "    assert d_model % self.num_heads == 0\n",
    "    \n",
    "    self.depth = d_model // self.num_heads\n",
    "    \n",
    "    self.wq = tf.keras.layers.Dense(d_model)\n",
    "    self.wk = tf.keras.layers.Dense(d_model)\n",
    "    self.wv = tf.keras.layers.Dense(d_model)\n",
    "    \n",
    "    self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "  def split_heads(self, x, batch_size):\n",
    "    \"\"\"Split the last dimension into (num_heads, depth).\n",
    "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "    \"\"\"\n",
    "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "  def call(self, v, k, q, mask):\n",
    "    batch_size = tf.shape(q)[0]\n",
    "    \n",
    "    q = self.wq(q)  # (batch_size, seq_len_q, d_model)\n",
    "    k = self.wk(k)  # (batch_size, seq_len_k, d_model)\n",
    "    v = self.wv(v)  # (batch_size, seq_len_v, d_model)\n",
    "    \n",
    "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, d_head)\n",
    "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, d_head)\n",
    "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, d_head)\n",
    "    \n",
    "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, d_head)\n",
    "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "    \n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, d_head)\n",
    "\n",
    "    concat_attention = tf.reshape(scaled_attention, \n",
    "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "        \n",
    "    return output, attention_weights\n",
    "\n",
    "\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(EncoderLayer, self).__init__()\n",
    "\n",
    "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, training, mask):\n",
    "    # value = x, key = x, query = x\n",
    "    self_att, self_att_weights = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "    self_att = self.dropout1(self_att, training=training)\n",
    "    out1 = self.layernorm1(x + self_att)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "    ffn_output = self.dropout2(ffn_output, training=training)\n",
    "    out = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "    \n",
    "    return out\n",
    "  \n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "    super(DecoderLayer, self).__init__()\n",
    "\n",
    "    self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "    self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    " \n",
    "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "    \n",
    "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "  def call(self, x, context, training, \n",
    "           look_ahead_mask, padding_mask):\n",
    "    # x: (batch, target_seq_len, d_model), context.shape: (batch_size, input_seq_len, d_model)\n",
    "\n",
    "    # value = x, key = x, query = x\n",
    "    self_att, self_att_weights = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len = 1, d_model)\n",
    "    self_att = self.dropout1(self_att, training=training)\n",
    "    self_att = self.layernorm1(self_att + x)\n",
    "\n",
    "    # value = context, key = context, query = self_att\n",
    "    cross_att, cross_att_weights = self.mha2(context, context, self_att, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "    cross_att = self.dropout2(cross_att, training=training)\n",
    "    out2 = self.layernorm2(cross_att + self_att)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "    ffn_output = self.dropout3(ffn_output, training=training)\n",
    "    out = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "    \n",
    "    return out, self_att_weights, cross_att_weights\n",
    "  \n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      self.enc_layers = [\n",
    "          EncoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, rate=dropout_rate)\n",
    "          for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x, training, encMask):\n",
    "      x = self.dropout(x, training=training)\n",
    "      for encoder_layer in self.enc_layers:\n",
    "        x = encoder_layer(x, training, encMask)\n",
    "      return x  # (batch_size, max_tokens, d_model)\n",
    " \n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super(Decoder, self).__init__()\n",
    "      self.d_model = hParams.d_model\n",
    "      self.num_layers = hParams.num_layers\n",
    "      self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "      if IGNORE_HISTORY: pass\n",
    "      else:\n",
    "        self.dec_layers = [\n",
    "            DecoderLayer(d_model=hParams.d_model, num_heads=hParams.num_heads, dff=hParams.d_model * 4, rate=dropout_rate)\n",
    "            for _ in range(hParams.num_layers)]\n",
    "\n",
    "    def call(self, x, context, training, look_ahead_mask, padding_mask):\n",
    "      if IGNORE_HISTORY: pass\n",
    "      else:\n",
    "        x = self.dropout(x, training=training)\n",
    "        for decoder_layer in self.dec_layers:\n",
    "          x, _, _  = decoder_layer(x, context, training, look_ahead_mask, padding_mask)\n",
    "      return x\n",
    "  \n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, hParams, dropout_rate=0.1):\n",
    "      super().__init__()\n",
    "      if IGNORE_HISTORY:\n",
    "        self.all_one_context = tf.ones((BATCH_SIZE, MAX_TOKENS, hParams.d_model), dtype=tf.float32) # (batch, max_tokens, d_model)\n",
    "      else:\n",
    "        self.encPreprocess = Preprocess(hParams, isEncoder=True)\n",
    "        self.decPreprocess = Preprocess(hParams, isEncoder=False)\n",
    "        self.posEmbedding = PositionalEmbedding(hParams.d_model)\n",
    "        self.encoder = Encoder(hParams, dropout_rate=dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(hParams.d_model) #-------------- to modify\n",
    "      self.decoder = Decoder(hParams, dropout_rate=dropout_rate)\n",
    "      self.one = tf.constant(1, dtype=tf.int32)\n",
    "\n",
    "    def call(self, input, training=False):\n",
    "      # inputs = (sequence, base_bb, baseDateDetails, mask)\n",
    "      # sequence: (batch, max_token, aabb), base: (batch, 1, bb), baseDateDetails: (batch, 1, 4), mask: (batch, max_token, max_token)\n",
    "      if IGNORE_HISTORY: \n",
    "        context = self.all_one_context\n",
    "      else:\n",
    "        x, encMask, sequenceDays, baseDays = self.encPreprocess(input) # (batch, max_tokens, d_model), (batch, max_tokens, max_tokens), (batch, max_tokens), (batch, 1)\n",
    "        x = self.posEmbedding(x, sequenceDays, baseDays, isEncoder=True) # (batch, max_tokens, d_model)\n",
    "        encMask = self.one - encMask    # NEGATE !!! forgoten for two YEARS !!!\n",
    "        encMask = encMask[:, tf.newaxis, :, :]  # newaxis: head\n",
    "        context = self.encoder(x, training, encMask)  # (batch, max_tokens, d_model). Only sequence and mask are used.\n",
    "\n",
    "      x, decMask, sequenceDays, baseDays = self.decPreprocess(input) # (batch, 1, d_model), (batch, 1, max_tokens), (batch, max_tokens), (batch, 1)\n",
    "      x = self.posEmbedding(x, sequenceDays, baseDays, isEncoder=False) # (batch, 1, d_model)\n",
    "      decMask = decMask[:, tf.newaxis, :, :]\n",
    "      # look_ahead_mask is None, which means [[0]], as there is only one position in x, so is nothing to mask when doing mha(value=x, key=x, query=x).\n",
    "      x = self.decoder(x, context, training, look_ahead_mask=None, padding_mask=decMask)  # (batch, 1, d_model).  Only base_bb, baseDateDetails, and mask are used.      \n",
    "\n",
    "      logits = self.final_layer(x)  # (batch, 1, d_model)\n",
    "      logits = tf.squeeze(logits, axis=-2)  # (batch, d_model)\n",
    "\n",
    "      return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL_TYPE_CHECK:\n",
    "    sample_transformer = Transformer(hParams, dropout_rate=DROPOUT)\n",
    "    y = sample_transformer(sample_x, training=True)\n",
    "    sample_transformer.summary()\n",
    "\n",
    "    @tf.function\n",
    "    def fun(x):\n",
    "        y = sample_transformer(x, training=False)\n",
    "        return y\n",
    "    y = fun(sample_x)\n",
    "    print(y.shape)\n",
    "    del sample_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Add_Norm(tf.keras.layers.Layer):\n",
    "    def __init__(self, dim, seed):\n",
    "      super().__init__()\n",
    "      self.dense =tf.keras.layers.Dense (dim, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=seed), bias_initializer=Zeros, activation='tanh')\n",
    "      self.add = tf.keras.layers.Add()\n",
    "      self.layernorm = tf.keras.layers.LayerNormalization()\n",
    "    def call(self, x):\n",
    "      dense = self.dense(x, training=False)\n",
    "      x = self.add([x, dense])  # x.shape[-1] == dim\n",
    "      x = self.layernorm(x)\n",
    "      return x\n",
    "\n",
    "Zeros = tf.keras.initializers.Zeros()\n",
    "\n",
    "# Used for earlier versions that don't allow mixing bookies.\n",
    "class Adaptor(tf.keras.Model):\n",
    "  def __init__(self, nLayers, d_main, d_output, dropout_rate=0.1):\n",
    "    super().__init__()\n",
    "    # total (nLayers + nLayers) dims = 2 * nLayers dims\n",
    "    dims = [d_main] * nLayers\n",
    "    layers = [Dense_Add_Norm(dims[id], id) for id in range(len(dims))]\n",
    "    self.seq = tf.keras.Sequential(layers)\n",
    "    self.initial = tf.keras.layers.Dense (d_main, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "    self.final = tf.keras.layers.Dense (d_output, kernel_initializer=tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.5, seed=23), activation='tanh')\n",
    "  def call(self, x, training=False):  # (batch, d_model)\n",
    "    x = self.initial(x)\n",
    "    x = self.seq(x)   # (batch, d_model)\n",
    "    x = self.final(x) # (batch, nBookies * 3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dan = Dense_Add_Norm(118 * 30, 23)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(dan)\n",
    "print(model(tf.ones((1,118 * 30), dtype=tf.float32), training=False))\n",
    "print(model.summary())\n",
    "del dan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_mean = tf.Variable([normalization_parms[col][0] for col in Odds_cols], trainable=False)\n",
    "std_variation = tf.Variable([normalization_parms[col][1] for col in Odds_cols], trainable=False)\n",
    "print(std_mean.shape, std_variation.shape)\n",
    "\n",
    "# No, _Label_cols were not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_1X2(tf.keras.Model):\n",
    "    softmax = tf.keras.layers.Softmax(axis=-1)\n",
    "\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.nQueries = nQueries\n",
    "        self.transformer = Transformer(hParams, dropout_rate=dropout_rate)\n",
    "        #   self.bookies = ['B365', 'Betfair', 'Interwetten', 'William']\n",
    "        self.bookies = ['HDA' + str(b) for b in range(NUMBER_BOOKIES)]\n",
    "\n",
    "        if SIMPLIFY_ADAPTOR:\n",
    "            self.adaptor = tf.keras.layers.Dense(len(self.bookies) * self.nQueries)\n",
    "        else:\n",
    "            self.adaptor = Adaptor(ADAPTORS_LAYERS, hParams.d_model * ADAPTORS_WIDTH_FACTOR, self.nQueries * len(self.bookies))\n",
    "     \n",
    "        if LOSS_TYPE == 'entropy':\n",
    "            self.categorical_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=False, label_smoothing=0.0, axis=-1, reduction='sum_over_batch_size')\n",
    "        return\n",
    "\n",
    "    def call(self, x, trainig=False):\n",
    "        x = self.transformer(x, training=trainig) # (batch, d_model)\n",
    "        output = self.adaptor(x)   # (batch, nBookies * nQueries)\n",
    "        output = tf.reshape(output, [output.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        output = tf.transpose(output, perm=[2, 0, 1])     # (nBookies, batch, nQueries)\n",
    "        if MODEL_ACTIVATION == 'softmax':\n",
    "            output = tf.nn.softmax(output)  # (nBookies, batch, nQueries)   #\n",
    "        elif MODEL_ACTIVATION == 'sigmoid':\n",
    "            output = tf.math.sigmoid(output * 5)  # the previous activation is tanh, ranging (-1, 1). Multiplier 5 will result in range (near 0, near 1)\n",
    "        elif MODEL_ACTIVATION == 'relu':\n",
    "            output = tf.nn.relu(output)\n",
    "        elif MODEL_ACTIVATION == 'open':\n",
    "            pass    # output = output\n",
    "        return output\n",
    "    \n",
    "    def h_true(self, ftGoals):  # Defines this QGroup. This is for 1X2 QGroup. Derived classes re-define this funciton.\n",
    "        # ftGoals:  (batch, 2)\n",
    "        ftGoals = tf.cast(ftGoals, dtype=tf.int32)  # (batch, 2)\n",
    "        h = (tf.math.greater(ftGoals[..., 0], ftGoals[..., 1]), tf.math.equal(ftGoals[..., 0], ftGoals[..., 1]), tf.math.less(ftGoals[..., 0], ftGoals[..., 1]))\n",
    "        h = tf.cast(tf.transpose(h), dtype=tf.float32)  # (batch, nQueries)\n",
    "        return h\n",
    "    \n",
    "    # def loss(self, y, output):   \n",
    "    #     # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "    #     # output: (nBookies, batch, nQueries)\n",
    "\n",
    "    #     def well(x, mu, sigma):\n",
    "    #         norm = tf.norm(x)\n",
    "    #         return tf.nn.relu(norm - sigma) * 0.5   # so this component of loss is less steep than the other component.\n",
    "    #         # return - tf.math.exp(- tf.math.pow((x-mu)/sigma, 2) / 2) / (sigma * tf.math.sqrt(np.pi*2))\n",
    "\n",
    "    #     ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "    #     odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "    #     odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "    #     happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "    #     oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "\n",
    "    #     if LOSS_TYPE != 'entropy':\n",
    "    #         (stake_p) = output  # (nBookies, batch, nQueries)\n",
    "    #         # -----------------------------------------------------------------------------------------\n",
    "    #         # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "    #         # Note: Do not normalize stake_p. It can learn whether to bet or not, as well as betting direction.\n",
    "    #         #------------------------------------------------------------------------------------------\n",
    "    #         profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)    # (nBooies, batch)\n",
    "    #         mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "    #         profit_backtest = tf.reduce_mean(mean_profit_by_game, axis=None)  # () \n",
    "    #         loss = - profit_backtest  # U.action.42\n",
    "\n",
    "    #         if MODEL_ACTIVATION == 'open':\n",
    "    #             bell_loss = well(stake_p, 0, 2)\n",
    "    #             loss += bell_loss\n",
    "    #     else:\n",
    "    #         probability_p = tf.transpose(output, perm=[1, 2, 0])\n",
    "    #         probability_p = tf.math.reduce_mean(probability_p, axis=-1) # (batch, nQueries)\n",
    "    #         loss = self.categorical_crossentropy(happen_t, probability_p)   # reduce sum\n",
    "    #         one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(probability_p).indices, tf.shape(probability_p)[-1]), axis=1)   # one_hot stake_p\n",
    "    #         # Find F1 scores between happen_t and stake_p, both (batch, nQueries)\n",
    "\n",
    "    #         profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, one_hot_stake_p), axis=-1)    # (nBooies, batch)\n",
    "    #         mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "    \n",
    "    #     return loss, mean_profit_by_game # (), (batch,)  negative average profit on a game on a bookie\n",
    "\n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "\n",
    "        def well(x, mu, sigma):\n",
    "            norm = tf.norm(x)\n",
    "            return tf.nn.relu(norm - sigma) * 0.5   # so this component of loss is less steep than the other component.\n",
    "            # return - tf.math.exp(- tf.math.pow((x-mu)/sigma, 2) / 2) / (sigma * tf.math.sqrt(np.pi*2))\n",
    "\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2), (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueries)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "\n",
    "        if LOSS_TYPE != 'entropy':\n",
    "            (stake_p) = output  # (nBookies, batch, nQueries)\n",
    "            # -----------------------------------------------------------------------------------------\n",
    "            # Note: happen_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "            # Note: Do not normalize stake_p. It can learn whether to bet or not, as well as betting direction.\n",
    "            #------------------------------------------------------------------------------------------\n",
    "            profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)    # (nBooies, batch)\n",
    "            mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "            profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # () \n",
    "            loss_sum = - profit_backtest_sum  # U.action.42\n",
    "\n",
    "            if MODEL_ACTIVATION == 'open':\n",
    "                bell_loss = well(stake_p, 0, 2)\n",
    "                loss += bell_loss\n",
    "\n",
    "            if not VECTOR_BETTING:\n",
    "                one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(stake_p).indices, tf.shape(stake_p)[-1]), axis=1)   # one_hot stake_p\n",
    "                profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, one_hot_stake_p), axis=-1)    # (nBooies, batch)\n",
    "                mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "                profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # ()\n",
    "\n",
    "        else:\n",
    "            probability_p = tf.transpose(output, perm=[1, 2, 0])\n",
    "            probability_p = tf.math.reduce_mean(probability_p, axis=-1) # (batch, nQueries)\n",
    "            loss_sum = self.categorical_crossentropy(happen_t, probability_p)   # reduce sum over batch size. What over the axis -1.\n",
    "\n",
    "            one_hot_stake_p = tf.squeeze(tf.one_hot(tf.nn.top_k(probability_p).indices, tf.shape(probability_p)[-1]), axis=1)   # one_hot stake_p\n",
    "            profit_per_bookie_game = tf.reduce_sum(tf.math.multiply(oh - 1.0, one_hot_stake_p), axis=-1)    # (nBooies, batch)\n",
    "            mean_profit_by_game = tf.reduce_mean(profit_per_bookie_game, axis=0)    # (batch,)\n",
    "            profit_backtest_sum = tf.reduce_sum(mean_profit_by_game, axis=None)  # ()\n",
    "    \n",
    "        return loss_sum, profit_backtest_sum # (), ()  Bot loss_sum and profit_backtest_sum are a sum across batch. \n",
    "    \n",
    "    \n",
    "    def backtest_event_wise(self, y, output, key_a, key_b):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "\n",
    "        # -----------------------------------------------------------------------------------------\n",
    "        # Note: oh_p and stake_p are not converted to one-hot values, unlike they should.\n",
    "        #------------------------------------------------------------------------------------------\n",
    "        def analyze(stake_p):\n",
    "            # sum_stake_p = tf.math.reduce_sum(stake_p, axis=-1)\n",
    "            norm = tf.norm(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "            profit_p = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / norm) - 1.0, stake_p), axis=-1)# (nBookies, batch)\n",
    "            profit_backtest = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1) # (nBookies, batch)\n",
    "            condition = tf.math.logical_and(key_a <= profit_p, profit_p <= key_b)   # (nBookies, batch), dtype=tf.bool\n",
    "            indices = tf.where(condition)   # (n, 2)    values: (bookieId, gameId)\n",
    "            profit_backtest = profit_backtest[condition]  # (n,)    values: (profit)\n",
    "            stake = stake_p[condition]  # (n, nQueries)\n",
    "            # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "            return indices, stake, profit_backtest\n",
    "\n",
    "        if VECTOR_BETTING:\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "        else:\n",
    "            bestQuery = tf.argmax(stake_p, axis=-1)     #   (nBookies, batch, nQueries)\n",
    "            stake_p = tf.one_hot(bestQuery, self.nQueries, dtype=tf.float32)   #   (nBookies, batch, nQueries)\n",
    "            indices, stake, profit_backtest = analyze(stake_p)\n",
    "\n",
    "        # assert indices.shape[0] == profit_backtest.shape[0]\n",
    "        return indices, stake, profit_backtest  # Not normalized. i.e. not divided with the norm of stake_p\n",
    "    \n",
    "    def get_batch_backtest(self, y, output):\n",
    "        # y: (batch, len(Team_cols)+len(Odds_cols)) \n",
    "        # output: (nBookies, batch, nQueries)\n",
    "        ftGoals, odds = tf.split(y, [2, -1], axis=-1) # (batch, 2)!, (batch, self.nQueries * len(self.bookies))\n",
    "        odds = tf.split(odds, [self.nQueries] * len(self.bookies), axis=-1)  # [(batch, nQueries)] * nBookies\n",
    "        odds = tf.stack(odds, axis=0)  # (nBookies, batch, nQueries)!\n",
    "        # odds and tfGoals were not normalized, so you don't need de-normalize them.\n",
    "        happen_t = self.h_true(ftGoals) # (batch, nQueroes)\n",
    "        oh = tf.math.multiply(odds, happen_t)   # (nBookies, batch, nQueries)\n",
    "        (stake_p) = output\n",
    "        sum = tf.math.reduce_sum(stake_p, keepdims=True, axis=-1) + 1e-12\n",
    "        indicatorP = tf.math.reduce_sum(tf.math.multiply(odds * (stake_p / sum) - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        backtestP = tf.math.reduce_sum(tf.math.multiply(oh - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "        indices = tf.where(indicatorP == indicatorP)    # (nBookies * batch, 2 [bookieId, rel_gameId])\n",
    "        indicatorP = tf.reshape(indicatorP, [-1])   # (nBookies * batch, )\n",
    "        backtestP = tf.reshape(backtestP, [-1])     # (nBookies * batch, )\n",
    "        stake_p = tf.reshape(stake_p, [-1, self.nQueries])\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        return indicatorP, backtestP, indices, stake_p\n",
    "    \n",
    "dummy_mean_profit_per_game = None\n",
    "class Model_Filter(Model_1X2):\n",
    "    def __init__(self, hParams, nQueries, dropout_rate=0.1):\n",
    "        super().__init__(hParams, nQueries, dropout_rate=dropout_rate)\n",
    "        self.bce = tf.losses.BinaryCrossentropy(from_logits=False)\n",
    "        return\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.transformer(x, training=training) # (batch, d_model)\n",
    "        stake_p = self.adaptor(x)   # (batch, nBookies * nQueries)\n",
    "        stake_p = tf.reshape(stake_p, [stake_p.shape[0], self.nQueries, -1])    # (batch, nQueries, nBookies)\n",
    "        stack = [stake_p[:, :, b] for b in range(len(self.bookies))]    # NNNNo way to swap axes efficiently.\n",
    "        stake_p = tf.stack(stack, axis=0)   # (nBookies, batch, nQueries)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=-1) # (nBookies, batch)\n",
    "        stake_p = tf.math.reduce_mean(stake_p, axis=0) # (batch,)\n",
    "        stake_p = tf.math.sigmoid(stake_p * 5)          # (batch,)\n",
    "        return stake_p\n",
    "    \n",
    "    def loss(self, y, output):   \n",
    "        # y: (batch,) \n",
    "        # output: (batch,)\n",
    "        loss = self.bce(y, output)\n",
    "        return loss, dummy_mean_profit_per_game\n",
    "\n",
    "def create_model_object(model_class):\n",
    "    try: model_c\n",
    "    except NameError: pass\n",
    "    else: del model_c\n",
    "    try: model_1x2\n",
    "    except NameError: pass\n",
    "    else: del model_1x2\n",
    "    tf.keras.backend.clear_session(); gc.collect()\n",
    "    model = model_class(hParams, nQueries=3, dropout_rate=DROPOUT)   # Do not create a reference and return directly.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = create_model_object(Model_1X2)\n",
    "\n",
    "if MODEL_TYPE_CHECK:\n",
    "    output = model_1x2(sample_x, training=True)\n",
    "    @tf.function\n",
    "    def fun(x, y):\n",
    "        output = model_1x2(x, training=False)\n",
    "        loss_sum, _ = model_1x2.loss(y, output)\n",
    "        return output, loss_sum\n",
    "    output, loss = fun(sample_x, sample_y)\n",
    "    model_1x2.summary()\n",
    "    # del model_1x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model_1x2, show_shapes=True, dpi=48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_with_dataset(dataset, profit_keys):\n",
    "    profits = [-1.0] * len(profit_keys)\n",
    "    casts = [0] * len(profit_keys)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        profit_list, cast_list = model_1x2.backtest(y, outputs, profit_keys)\n",
    "\n",
    "        for p, c, id in zip(profit_list, cast_list, range(len(profit_keys))):\n",
    "            if c > 0:\n",
    "                profits[id] = (profits[id] * casts[id] + p * c) / (casts[id] + c)\n",
    "                casts[id] = casts[id] + c\n",
    "    # print('key', profit_back_mean, nBettingsTotal)\n",
    "    return profits, casts\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def backtest_event_wise_with_dataset(ds_batches, model, key_a, key_b):\n",
    "    indices = []; stakes = []; profits = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "\n",
    "        outputs = model(x, training=False)  #\n",
    "        new_indices, new_stakes, new_profits = model.backtest_event_wise(y, outputs, key_a, key_b)  # Tensor (n, 2 <bookieId, rel_gameId>), (n, nQueries), (n,)\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ..., [bookie, rel_game_id]]. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # [[0, 123], ..., [[bookie, gameId]]], len = n\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes\n",
    "\n",
    "        new_profits = list(new_profits.numpy())                 # [0.3, ...]\n",
    "        profits = profits + new_profits     # [1.3, ..., profit], len = n\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, stake, profit) for (bookie, gameId), stake, profit in zip(indices, stakes, profits)]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "    return interval_backtests     # [(bookie, gameId, profit) for ...]\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def get_backtest_and_indicator_profits(dataset):\n",
    "    backtestP = indicatorP = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model_1x2(x, training=False)  #\n",
    "        new_indicatorP, new_backtestP = model_1x2.get_backtest_and_indicator_profits(y, outputs)    # Tensor (nBookies * batch), (nBookies * batch)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())\n",
    "    return indicatorP, backtestP\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "def get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0):\n",
    "    backtestP = np.array(backtestP, dtype=np.float32)\n",
    "    indicatorP = np.array(indicatorP, dtype=np.float32)\n",
    "    indices = np.array(indices, dtype=np.int32)\n",
    "    stakes = np.array(stakes, dtype=np.float32)\n",
    "    idx = np.argsort(indicatorP)\n",
    "    indicatorP = indicatorP[idx]    # increasing order\n",
    "    backtestP = backtestP[idx]      # hope to be increading order\n",
    "    indices = indices[idx]\n",
    "    stakes = stakes[idx]\n",
    "\n",
    "    ab = []\n",
    "    for w in range(30, int(len(backtestP))):    # 30\n",
    "        back = np.convolve(backtestP, np.ones(w), mode='valid') - threshold\n",
    "        ab += [(back[i], i, i+w-1) for i in range(len(back))]   # [ i : i + w ] : w points. inclusive boundaries.\n",
    "    ab = [(p, a, b) for (p, a, b) in ab if p > 0]   # a, b : inclusive\n",
    "    ab.sort(reverse=False)  # False !   The larger the profit, the earlier it comes in ab.\n",
    "    AB = ab.copy()\n",
    "    for i in range(len(ab)):   # less valuable intervals are screened first.\n",
    "        interval_i = ab[i];  P, A, B = interval_i    # sure A < B\n",
    "        intersects = False\n",
    "        for j in range(i+1, len(ab)):\n",
    "            interval_j = ab[j]; p, a, b = interval_j    # sure a < b\n",
    "            if not (b < A or B < a): intersects = True; break\n",
    "        if intersects:  AB.remove(interval_i)   # interval_i is unique in AB\n",
    "    AB.sort(reverse=True); AB = AB[:5]\n",
    "    print(\"AB: \", AB)\n",
    "    [interval_profit, idx_a, idx_b] = AB[0]     # idx_a, idx_b : inclusive\n",
    "    keyPairs = [[indicatorP[a], indicatorP[b]] for (B, a, b) in AB]\n",
    "    (key_a, key_b) = keyPairs[0]\n",
    "\n",
    "    return indicatorP, backtestP, indices, stakes, interval_profit, key_a, key_b, idx_a, idx_b\n",
    "\n",
    "# @tf.function  # gives a wrong result of tf.where(profit_p > key)\n",
    "def inference_step(x, odds, interval_a, interval_b):\n",
    "    stake_p = model_1x2(x, training=False)    # (nBookies, batch, nQueries)\n",
    "    nQueries = stake_p.shape[-1]\n",
    "    profit_p = tf.math.reduce_sum(tf.math.multiply(odds * stake_p - 1.0, stake_p), axis=-1)  # (nBookies, batch)\n",
    "\n",
    "    bet_bool = interval_a <= profit_p and profit_p >= interval_b    # (nBookies, batch)\n",
    "    bet_bool = tf.stack([bet_bool] * nQueries, axis=-1) # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.math.multiply(stake_p, tf.cast(bet_bool, dtype=tf.float32))   # (nBookies, batch, nQueries)\n",
    "    stake_vector = tf.reshape(stake_vector, [1, 0, 2])  # (batch, nBookies, nQueries)\n",
    "    return stake_vector\n",
    "\n",
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def inference_with_dataset(dataset, interval_a, interval_b): \n",
    "    vectors = []\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        id, div, days, teams, odds, remainder \\\n",
    "            = tf.split(base_bb, [len(id_cols), len(Div_cols), len(Date_cols), len(Team_cols), len(Odds_cols), -1], axis=-1)\n",
    "        stake_vector = inference_step(x, odds, interval_a, interval_b)  # (batch, nBookies, nQueries)\n",
    "        vectors.append(stake_vector)\n",
    "    \n",
    "    stake_vectors = tf.concat(vectors, axis=0)   # (batch, nBookies, nQueries)\n",
    "    return stake_vectors    # (batch, nBookies, nQueries)\n",
    "\n",
    "def get_dataset_backtest(model, dataset):\n",
    "    backtestP = indicatorP = indices = stakes = []\n",
    "\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(dataset):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        outputs = model(x, training=False)\n",
    "        # (nBookies * batch,), (nBookies * batch,), (nBookies * batch, 2 [bookieId, rel_gameId]), (nBookies * batch, 3 [nQueries])\n",
    "        new_indicatorP, new_backtestP, new_indices, new_stakes = model.get_batch_backtest(y, outputs)\n",
    "        indicatorP = indicatorP + list(new_indicatorP.numpy())  # + [1.2, ...], len = nBookies * batch\n",
    "        backtestP = backtestP + list(new_backtestP.numpy())     # + [1.2, ...], len = nBookies * batch\n",
    "\n",
    "        new_indices = [list(id) for id in new_indices.numpy()]  # [[0,2], ...], len = nBookies * batch. No tuple but list.\n",
    "        baseId = list(baseId.numpy())   # absolute game ids\n",
    "        new_indices = [[bookie, baseId[rel_game_id]] for [bookie, rel_game_id] in new_indices]\n",
    "        indices = indices + new_indices     # + [[0, 1000123], ..., [[bookie, gameId]]], len = nBookies * batch\n",
    "\n",
    "        new_stakes = [list(stake) for stake in new_stakes.numpy()]  # [[sHWin, sDraw, sAWin], ...] No tuple but list\n",
    "        stakes = stakes + new_stakes    # + [[sHWin, sDraw, sAWin], ...], len = nBooks * batch\n",
    "\n",
    "    indicatorP_permuted, backtestP_permuted, indices_permutated, stakes_permutated, interval_profit, interval_a, interval_b, idx_a, idx_b = \\\n",
    "        get_key_a_key_b(indicatorP, backtestP, indices, stakes, threshold=0.0)  # idx_a, idx_b : inclusive\n",
    "\n",
    "    interval_backtests = [(bookie, gameId, list(stake), profit) for (bookie, gameId), stake, profit in \\\n",
    "        zip(indices_permutated[idx_a:idx_b+1], stakes_permutated[idx_a:idx_b+1], backtestP_permuted[idx_a:idx_b+1])]  # [ [bookieId ,gameId, (sHWin, sDraw, sAway), p], ...  ]\n",
    "\n",
    "    return interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b\n",
    "\n",
    "class Adam_exponential(tf.keras.optimizers.Adam):\n",
    "    def __init__(self, initial_step, starting_rate, ex_step, ex_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-12):\n",
    "        self.step = tf.Variable(initial_step, dtype=tf.float32, trainable=False)\n",
    "        learning_rate = tf.compat.v1.train.exponential_decay(starting_rate, self.step, ex_step, ex_rate/starting_rate, staircase=False)\n",
    "        super().__init__(learning_rate, beta_1=beta_1, beta_2=beta_2, epsilon=epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class history_class():\n",
    "    def round_sig(self, x, sig=2):\n",
    "            return x\n",
    "            # return round(x, sig-int(math.floor(math.log10(abs(x))))-1)    # domain error for VERY small numbers.\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.history = {'initial_profit': -float('inf'), 'loss': [], 'val_loss_0': [], 'val_profit_0': [], 'val_loss': [], 'val_profit': [], 'learning_rate': [], 'recall': [], 'precision': [], 'time_taken': [],  'gauge': [], 'backtest_reports': []}\n",
    "    def set_initial_interval_profit(self, initail_inverval_profit):\n",
    "        self.history['initial_profit'] = initail_inverval_profit\n",
    "        self.save()\n",
    "    def removeFile(self):\n",
    "        files = glob.glob(self.filepath + \"*\")   # \"*.*\" may not work\n",
    "        result = [os.remove(file) for file in files]\n",
    "    def save(self):\n",
    "        data_helpers.SaveJsonData(self.history, self.filepath)\n",
    "    def reset(self):\n",
    "        self.removeFile()\n",
    "        # forgot to reset self.history? ---------------------- Check it.\n",
    "        self.__init__(self.filepath)\n",
    "        self.save()\n",
    "    def load(self):\n",
    "        history = data_helpers.LoadJsonData(self.filepath)\n",
    "        if history is not None: self.history = history\n",
    "\n",
    "    def append(self, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, gauge, backtest_report):\n",
    "        self.history['loss'].append(self.round_sig(train_loss, 4))\n",
    "        self.history['val_loss_0'].append(self.round_sig(val_loss_0, 4))\n",
    "        self.history['val_profit_0'].append(self.round_sig(val_profit_0, 4))\n",
    "        self.history['val_loss'].append(self.round_sig(val_loss, 4))\n",
    "        self.history['val_profit'].append(self.round_sig(val_profit, 4))\n",
    "        self.history['learning_rate'].append(learning_rate)\n",
    "        self.history['recall'].append(self.round_sig(recall, 4))\n",
    "        self.history['precision'].append(self.round_sig(precision, 4))\n",
    "        self.history['time_taken'].append(time_taken)\n",
    "        self.history['gauge'].append(gauge)\n",
    "        self.history['backtest_reports'].append(backtest_report)\n",
    "        self.save()\n",
    "\n",
    "    def get_zipped_history(self):\n",
    "        z = zip(self.history['loss'], self.history['val_loss_0'], self.history['val_profit_0'], self.history['val_loss'], self.history['val_profit'], self.history['learning_rate'], self.history['recall'], self.history['precision'], self.history['time_taken'])\n",
    "        return list(z)\n",
    "    # def append_backtests(self, epoch, key_a, key_b, interval_profit, backtest):\n",
    "    #     self.history['backtests'].append((epoch, key_a, key_b, interval_profit, backtest))\n",
    "    #     self.save()\n",
    "    def get_backtest_report(self, epoch):\n",
    "        return self.history['backtest_reports'][epoch]     # sure exists. epoch is selected.\n",
    "    def len(self):\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss_0'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_profit_0'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_loss'])\n",
    "        assert len(self.history['loss']) == len(self.history['val_profit'])\n",
    "        assert len(self.history['loss']) == len(self.history['recall'])\n",
    "        assert len(self.history['loss']) == len(self.history['precision'])\n",
    "        assert len(self.history['loss']) == len(self.history['learning_rate'])\n",
    "        assert len(self.history['loss']) == len(self.history['time_taken'])\n",
    "        assert len(self.history['loss']) == len(self.history['gauge'])\n",
    "        assert len(self.history['loss']) == len(self.history['backtest_reports'])\n",
    "        return len(self.history['loss'])\n",
    "    def get_min_val_loss(self):\n",
    "        return float('inf') if self.len() <= 0 else min(self.history['val_loss'])\n",
    "    def get_max_gauge(self, epoch):\n",
    "        gauges = self.history['gauge']\n",
    "        return -float('inf') if (len(gauges) <= 0 or epoch <= 0) else max(gauges[:epoch])\n",
    "    def replace_gauge(self, epoch, gauge):\n",
    "        self.history['gauge'][epoch] = gauge;   self.save()\n",
    "    def show(self, ax, show_val_0=True):\n",
    "        ax.set_title(TEST_ID + \": loss history\")\n",
    "        ax.plot(self.history['loss'], label='train_loss')\n",
    "        if show_val_0: ax.plot(self.history['val_loss_0'], label='GET_VAL_LOSS_0')\n",
    "        ax.plot(self.history['val_loss'], label='val_loss')\n",
    "        ax.grid(True)\n",
    "        ax.set_ylabel('loss')\n",
    "        ax.set_xlabel('epoch', loc='right')\n",
    "        ax.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class recall():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.recall = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):    # (batch,)\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    labeled_positives = tf.math.reduce_sum(label, axis=None)\n",
    "    recall = hit_positives / (labeled_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.recall = self.recall * (self.n-1)/self.n + recall / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.recall\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.recall = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "  \n",
    "recall_object = recall(min_seq_len=5)\n",
    "\n",
    "class precision():\n",
    "  def __init__(self, **kwargs):\n",
    "    self.n = None\n",
    "    self.precision = None\n",
    "    self.reset()\n",
    "\n",
    "  def update(self, label, pred):\n",
    "    label = tf.cast(label, dtype=tf.float32)\n",
    "    pred = tf.cast(pred, dtype=tf.float32)\n",
    "    hit_positives = tf.math.reduce_sum(label * pred, axis=None)\n",
    "    predicted_positives = tf.math.reduce_sum(pred, axis=None)\n",
    "    precision = hit_positives / (predicted_positives + 1e-9) #tf.keras.backend.epsilon())\n",
    "    self.n += 1\n",
    "    self.precision = self.precision * (self.n-1)/self.n + precision / self.n\n",
    "\n",
    "  def result(self):\n",
    "    return self.precision\n",
    "  \n",
    "  def reset(self):\n",
    "    self.n = 0\n",
    "    self.precision = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "\n",
    "precision_object = precision(min_seq_len=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function    # Removing this decoration leads to GPU OOM!!!\n",
    "def train_step(model, optimizer, x, y):\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(x, training=True)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "        loss_sum, _ = model.loss(y, outputs)    # (), (batch,)\n",
    "\n",
    "    grads = tape.gradient(loss_sum, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    del tape    # new\n",
    "    return loss_sum  # (), (batch,)\n",
    "\n",
    "@tf.function\n",
    "def find_loss_for_step(model, x, y):\n",
    "    outputs = model(x, training=False)  # [ (batch, 1), (batch, nQueries) for _ in bookies]\n",
    "    loss_sum, backtest_profit_sum = model.loss(y, outputs)    # (), (batch,)    \n",
    "    return loss_sum, backtest_profit_sum\n",
    "\n",
    "def find_loss_for_dataset(model, ds_batches):\n",
    "    prev_nSum = new_nSum = 0\n",
    "    the_loss = tf.Variable(0.0, dtype=tf.float32, trainable=False) \n",
    "    the_backtest_profit = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "        loss_sum, backtest_profit_sum = find_loss_for_step(model, x, y)\n",
    "        batch_size = baseId.shape[0]\n",
    "        new_nSum += batch_size  # += baseId.shape[0], given sum of loss and profit. Ignore that the last batch might be smaller than others.\n",
    "        the_loss = the_loss * prev_nSum / new_nSum + loss_sum / new_nSum\n",
    "        the_backtest_profit = the_backtest_profit * prev_nSum / new_nSum + backtest_profit_sum / new_nSum\n",
    "        prev_nSum = new_nSum\n",
    "    return the_loss, the_backtest_profit    # agerage loss and backtest_profit per game\n",
    "\n",
    "# @tf.function\n",
    "def find_recall_precision(model, ds_batches):\n",
    "    recall_object.reset(); precision_object.reset()\n",
    "    for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (y)) in enumerate(ds_batches):\n",
    "        x = (sequence, base_bb, baseDateDetails, mask)\n",
    "        pred = model(x, training=False)\n",
    "        pred = tf.cast(pred > 0.5, dtype=tf.int32)\n",
    "        # Wrong. transorm y to label.   #-------------------------------------------------------- Wrong\n",
    "        recall_object.update(y, pred); precision_object.update(y, pred)\n",
    "    return recall_object.result(), precision_object.result()\n",
    "\n",
    "def replace_baseLabel_with_profitable(model, ds_batches, threshold=0.0001):     # ds_batches: either train_batches or valid_batches\n",
    "    def generator():\n",
    "        count = 0\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = base_label\n",
    "            # print('3', sequence.shape, base_bb.shape, baseDateDetails.shape, mask.shape, y.shape)\n",
    "            _, mean_profit_per_game = find_loss_for_step(model, x, y)  # (), (batch,)\n",
    "            profitable = tf.cast(mean_profit_per_game >= threshold, dtype=tf.int32)\n",
    "            for i in range(baseId.shape[0]):\n",
    "                # create a new dataset format, to which to apply apply_train/test_pipeline_c(.). Note it's a different pipeline.\n",
    "                yield (baseId[i], sequence[i], base_bb[i], baseDateDetails[i], mask[i], profitable[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.int32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS)), tf.TensorShape(())),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def select_profitable_items(model, ds_batches, threshold=0.5):  # ds_batches: either train_batches or valid_batches\n",
    "    def generator():\n",
    "        for step, ((baseId, sequence, base_bb, baseDateDetails, mask), (base_label)) in enumerate(ds_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask)\n",
    "            output = model(x, training=False)   # (batch, ...)\n",
    "            for i in range(output.shape[0]):\n",
    "                if output[i] >= threshold:\n",
    "                    # back to the origitnal dataset format, to which to apply apply_train/test_pipeline(.)\n",
    "                    yield (baseId[i], sequence[i], base_bb[i], base_label[i], baseDateDetails[i], mask[i])\n",
    "\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_types=(tf.int32, tf.float32, tf.float32, tf.float32, tf.int32, tf.int32),\n",
    "        output_shapes=(tf.TensorShape(()), tf.TensorShape((MAX_TOKENS, len(BBAB_cols))), tf.TensorShape((1, len(BB_cols),)), tf.TensorShape((len(_Label_cols),)), tf.TensorShape((1, 4)), tf.TensorShape((MAX_TOKENS, MAX_TOKENS))),\n",
    "        args=()\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def analyze(model, ds_batches, checkpointPath):\n",
    "    model.load_weights(checkpointPath)\n",
    "    interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b = get_dataset_backtest(model, ds_batches)\n",
    "    fig, axes = plt.subplots(nrows=4, ncols=1, figsize=(15,12))\n",
    "    axes[0].plot(indicatorP_permuted); axes[0].grid(True)\n",
    "    axes[1].plot(backtestP_permuted); axes[1].grid(True)\n",
    "    w = 50; idx_1 = idx_a - int(w/2); idx_2 = idx_b - int(w/2)\n",
    "    back = np.convolve(np.array(backtestP_permuted), np.ones(w), mode='valid') / w\n",
    "    axes[2].plot(back, lw=0.5); axes[2].plot([idx_1, idx_1], plt.ylim(), 'r', lw=0.5); axes[2].plot([idx_2, idx_2], plt.ylim(), 'r', lw=0.5); axes[2].grid(True)\n",
    "    axes[3].plot(indicatorP_permuted, backtestP_permuted); axes[3].grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def get_interval_trajectory(model, dataset, checkpointPath, chunk_size_pct, step_counts):\n",
    "    length = len(dataset)\n",
    "    assert step_counts > 1\n",
    "    assert chunk_size_pct * step_counts >= 100\n",
    "    chunk_size = int(length * chunk_size_pct / 100)\n",
    "    step_size = int((length - chunk_size) / (step_counts - 1))\n",
    "    intervals = []\n",
    "    model.load_weights(checkpointPath)\n",
    "    for step in range(step_counts):\n",
    "        chunk_a = step_size * step\n",
    "        ds = dataset.skip(chunk_a)\n",
    "        ds = ds.take(chunk_size)\n",
    "        ds_batches = apply_test_pipeline(ds)\n",
    "        interval_backtests, indicatorP_permuted, backtestP_permuted, interval_a, interval_b, interval_profit, idx_a, idx_b = get_dataset_backtest(model, ds_batches)\n",
    "        intervals.append((interval_a, interval_b, interval_profit))\n",
    "    return intervals\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sub_epoch(epoch, batch_id_seen, sub_loss, epoch_loss, samples_seen, learning_rate):\n",
    "    print(\"epoch: {}, batchId_seen: {}, sub_loss: {:.6}, epoch_loss: {:.6f}, samples_seen: {}, learning_rate: {:.5e}                  \".\n",
    "            format(epoch, batch_id_seen, float(sub_loss), float(epoch_loss), samples_seen, learning_rate), end='\\r')\n",
    "\n",
    "def accumulate(backtests, compound=False):\n",
    "    initial = 1.0; Div = 3\n",
    "    sum = initial; minS = sum; maxS = sum\n",
    "    for (bookie, gameId, stake, profit) in backtests:\n",
    "        sum_stake = 0.0\n",
    "        for s in stake: sum_stake += s\n",
    "        sum += (profit / sum_stake * ( sum if compound else initial*sum_stake/Div))\n",
    "        if sum < minS: minS = sum\n",
    "        if sum > maxS: maxS = sum\n",
    "        if sum < 0.2: break\n",
    "        # if sum > initial * 2: initial = sum   # this is a step-wise compound. as risky as simple compound.\n",
    "    return sum, minS, maxS\n",
    "\n",
    "def run_backtest(epoch, model, history, playback=False):     # Read-only\n",
    "    #-------------------- make a backtest virtually/actually\n",
    "    A_ds = valid_batches; len_A_ds = len(valid_ds)\n",
    "    B_ds = test_batches; len_B_ds = len(test_ds)\n",
    "\n",
    "    if playback:\n",
    "        interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests = \\\n",
    "            history.get_backtest_report(epoch)\n",
    "    else:   # idx_a, idx_b : inclusive\n",
    "        print(\"Wait...\", end='')\n",
    "        A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b = get_dataset_backtest(model, A_ds)\n",
    "        B_backtests = backtest_event_wise_with_dataset(B_ds, model, interval_a, interval_b)\n",
    "\n",
    "    # We have A_backtests, A_indicatorP_permuted, A_backtestP_permuted, interval_a, interval_b, A_interval_profit, idx_a, idx_b, B_backtests\n",
    "\n",
    "    print(\"iterval_a: {:.8f}, interval_b: {:.8f}, idx_a: {}, idx_b: {}, A_interval_profit: {:.4f}\".format(interval_a, interval_b, idx_a, idx_b, A_interval_profit))\n",
    "    \n",
    "    #---- validation/verification\n",
    "    A_len_backtest = len(A_backtests)\n",
    "    B_len_backtest = len(B_backtests)\n",
    "    A_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in A_backtests]))\n",
    "    B_acc_profit = np.sum(np.array([profit for (_, _, _, profit) in B_backtests]))\n",
    "    A_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in A_backtests]))\n",
    "    B_acc_stake = np.sum(np.array([[stake[0], stake[1], stake[2]] for (_, _, stake, _) in B_backtests]))\n",
    "    A_final_capital, A_bottom_capital, A_top_capital = accumulate(A_backtests, compound=False)\n",
    "    B_final_capital, B_bottom_capital, B_top_capital = accumulate(B_backtests, compound=False)\n",
    "    A_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in A_backtests]\n",
    "    B_backtestsToShow = [[bookie, gameId-config['baseGameId'], [int(s*10000)/10000 for s in stake], int(profit * 10000)/10000] for (bookie, gameId, stake, profit) in B_backtests]\n",
    "\n",
    "    format = \"len_backtest/len_dataset: {}/{}, accProfit/accStake: {:.4f}/{:.4f}, capital (final/bottom/top): {:.4f}/{:.4f}/{:.4f}\"\n",
    "    A_format = \"A_test:: \" + format; B_format = \"B_test:: \" + format\n",
    "    print(A_format.format(A_len_backtest, len_A_ds, A_acc_profit, A_acc_stake, A_final_capital, A_bottom_capital, A_top_capital))\n",
    "    print(B_format.format(B_len_backtest, len_B_ds, B_acc_profit, B_acc_stake, B_final_capital, B_bottom_capital, B_top_capital))\n",
    "    print(\"A_backtest: \", A_backtestsToShow[:20])\n",
    "    print(\"B_backtest: \", B_backtestsToShow[:20])\n",
    "\n",
    "    backtest_report = (interval_a, interval_b, A_interval_profit, idx_a, idx_b, A_backtests, B_backtests)\n",
    "    gauge = B_acc_profit\n",
    "\n",
    "    return backtest_report, gauge\n",
    "\n",
    "# conclude_train_epoch(-1, 0, 0, 0, 0, playback=False)\n",
    "def conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback=False, gaugeTerm='val_loss'):\n",
    "    global focus_interval_id, focus_scores, focus_back, focus_valid, focus_test\n",
    "    print(\"epoch: {}, train_loss: {:.5f}, val_loss_0: {:.5f}, val_profit_0: {:.5f}, val_loss: {:.5f}, val_profit: {:.5f}, learning_rate: {:.5e}, recall/precision: {:.4f}/{:.4f}, time_taken: {:.1f} m\".format(epoch, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken))\n",
    "\n",
    "    backtest_report = ()\n",
    "    if gaugeTerm == 'val_loss': gauge = - val_loss\n",
    "    elif gaugeTerm == 'recall': gauge = recall\n",
    "    elif gaugeTerm == 'precision': gauge = precision\n",
    "\n",
    "    # backtest_report, gauge = run_backtest(epoch, model, history, playback)   #-------------------------------------------------\n",
    "\n",
    "    upgraded = False\n",
    "    #-------------------- get interval scores from arguments, create the best checkpoint if we have a highest ever score.\n",
    "    if gauge > history.get_max_gauge(epoch):\n",
    "        upgraded = True\n",
    "        # focus_interval_id, focus_scores, focus_back, focus_valid, focus_test = bets_interval_id, interval_scores, best_interval_back, best_interval_valid, best_interval_test\n",
    "        if not playback: model.save_weights(checkpointPathBest)\n",
    "        print(\"----------------------------------------------------------------------------------------------------- best checkpoint updated\")\n",
    "    if playback: history.replace_gauge(epoch, gauge)\n",
    "\n",
    "    #--------------------- Save, finally\n",
    "    if not playback:\n",
    "        model.save_weights(checkpointPath)\n",
    "        # self, train_loss, val_loss, learning_rate, recall, precision, time_taken, gauge, backtest_report\n",
    "        history.append(train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, gauge, backtest_report)\n",
    "\n",
    "    return upgraded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath = os.path.join(checkpoint_folder_path, TEST_ID + '_weights')\n",
    "checkpointPathBest = os.path.join(checkpoint_folder_path, TEST_ID + '_weights_best')\n",
    "historyPath = os.path.join(checkpoint_folder_path, TEST_ID + '_history.json')\n",
    "history = history_class(historyPath); history.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 5))\n",
    "history.show(ax, show_val_0=GET_VAL_LOSS_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1x2 = create_model_object(Model_1X2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken in history.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='val_loss')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function  #-------------------- Weird: no work.\n",
    "def train(epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False, gaugeTerm='val_loss', get_val_loss_0=False):\n",
    "    epochs = epochs; seen_upgrades = 0\n",
    "    for epoch in range(history.len(), history.len() + epochs):\n",
    "        start_time = time.time()\n",
    "        optimizer.step.assign(history.len()); learning_rate = optimizer.lr.numpy()\n",
    "        prev_nSum = new_nSum = 0; prev_mSum = new_mSum = 0; epoch_loss = 0.; sub_loss = 0.\n",
    "        samples_seen = 0 # sub_loss = tf.Variable(0.0, dtype=tf.float32, trainable=False)\n",
    "        set_seed(epoch)        \n",
    "        train_batches = apply_train_pipeline(train_ds)      # the pipeline includes suffling.\n",
    "        for batch_id, ((baseId, sequence, base_bb, baseDateDetails, mask), (value)) in enumerate(train_batches):\n",
    "            x = (sequence, base_bb, baseDateDetails, mask); y = value\n",
    "            samples_seen += sequence.shape[0]\n",
    "            loss_sum = train_step(model, optimizer, x, y)  # ()\n",
    "\n",
    "            batch_size = baseId.shape[0]\n",
    "            new_nSum += batch_size; new_mSum += batch_size\n",
    "            sub_loss = sub_loss * prev_nSum / new_nSum + loss_sum / new_nSum\n",
    "            epoch_loss = epoch_loss * prev_mSum / new_mSum + loss_sum / new_mSum\n",
    "            prev_nSum = new_nSum; prev_mSum = new_mSum\n",
    "\n",
    "            if batch_id % 50 == 0: \n",
    "                # optimizer.step.assign(history.len()); learning_rate = optimizer.lr.numpy()  #----------------------------------------------------\n",
    "                show_sub_epoch(epoch, batch_id, sub_loss, epoch_loss, samples_seen, learning_rate); prev_nSum = new_nSum = 0; sub_loss = 0.0\n",
    "\n",
    "        show_sub_epoch(epoch, batch_id, sub_loss, epoch_loss, samples_seen, learning_rate)  # closing show\n",
    "        val_loss_0 = val_profit_0 = 0.0\n",
    "        if get_val_loss_0: val_loss_0, val_profit_0 = find_loss_for_dataset(model, train_batches)\n",
    "        val_loss, val_profit = find_loss_for_dataset(model, valid_batches)\n",
    "\n",
    "        recall = precision = -1.0\n",
    "        if get_f1: recall, precision = find_recall_precision(model, valid_batches); recall = recall.numpy(); precision = precision.numpy()\n",
    "        \n",
    "        # epoch, model, history, checkpointPath, checkpointPathBest, epoch_loss, val_loss, learning_rate, recall, precision, time_taken, playback=False\n",
    "        upgraded = conclude_train_epoch(epoch, model, history, checkpointPath, checkpointPathBest, float(epoch_loss), float(val_loss_0), float(val_profit_0), float(val_loss), float(val_profit), learning_rate, recall, precision, (time.time()-start_time)/60, playback=False, gaugeTerm=gaugeTerm)\n",
    "        if upgraded: seen_upgrades += 1\n",
    "        if seen_upgrades >= nUpgrades: break\n",
    "\n",
    "    return seen_upgrades\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'PRETRAIN': # model_1x2 is trained to find profitable staking vectors for games, although average profitability is far below zero.\n",
    "    try:    model_1x2.load_weights(checkpointPath).expect_partial(); print(\"model_1x2 loaded its previous checkpoint.\")\n",
    "    except: print(\"model_1x2 loaded its initial weights.\")    # The raw model_1x2 itself is the starting point.\n",
    "    optimizer = Adam_exponential(history.len(), STARTING_LEARNING_RATE, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.99, epsilon=1e-12)\n",
    "    # Goal: get checkpointPathBest checkpoint, which is used as the starting checkpoint of both TRAIN_C and FINETUNE.\n",
    "    nEpochs = 300; wanted_upgrades = 60\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_1x2, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1 = False, gaugeTerm='val_loss', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "    \n",
    "    OPERATION = 'TRAIN_C'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_1x2 = create_model_object(Model_1X2)\n",
    "analyze(model_1x2, test_batches, checkpointPathBest)    # baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "back_valid = back_ds.concatenate(valid_ds)\n",
    "intervals = get_interval_trajectory(model_1x2, back_valid, checkpointPathBest, chunk_size_pct=25, step_counts=4)\n",
    "intervals_a = [a for (a, _, _) in intervals]\n",
    "intervals_b = [b for (_, b, _) in intervals]\n",
    "intervals_profit = [p for (_, _, p) in intervals]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def find_extrapolation(sequence, tail_data_len):\n",
    "    assert tail_data_len <= len(sequence)\n",
    "    x = np.array([[i] for i in range(tail_data_len)], dtype=np.float32)\n",
    "    y = np.array(sequence[- tail_data_len :], dtype=np.float32)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x, y)\n",
    "    extrapolation = model.predict(np.array([[tail_data_len]], dtype=np.float32))[0]\n",
    "    return extrapolation\n",
    "\n",
    "intervals_a.append(find_extrapolation(intervals_a, 2))\n",
    "intervals_b.append(find_extrapolation(intervals_b, 2))\n",
    "\n",
    "plt.plot(intervals_a, label='a')\n",
    "plt.plot(intervals_b, label='b')\n",
    "plt.plot(intervals_profit,label='p')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = intervals_a[-1]; b = intervals_b[-1]\n",
    "backtests = backtest_event_wise_with_dataset(test_batches, model_1x2, a, b)  # [(bookie, gameId, [stakeHomeWin, stakeDraw, stakeAwayWin], profit), ...]\n",
    "sum, minS, maxS = accumulate(backtests, compound=False)\n",
    "print('sum, minS, maxS: ', sum, minS, maxS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(backtests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath_c = os.path.join(checkpoint_folder_path, TEST_ID + '_weights_c')\n",
    "checkpointPathBest_c = os.path.join(checkpoint_folder_path, TEST_ID + '_weights_best_c')\n",
    "historyPath_c = os.path.join(checkpoint_folder_path, TEST_ID + '_history_c.json')\n",
    "history_c = history_class(historyPath_c); history_c.load()\n",
    "\n",
    "model_1x2 = create_model_object(Model_1X2)\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken in history_c.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='precision')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(16, 2))\n",
    "history_c.show(ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'TRAIN_C':  # model_c is trained to find games for which model_1x2 can can find positively profitable staking vector.\n",
    "\n",
    "    def X_versus_Y_c(baseId, sequence, base_bb, baseDateDetails, mask, profitable):\n",
    "        return (baseId, sequence, base_bb, baseDateDetails, mask), (profitable)\n",
    "    def apply_train_pipeline_c(ds):\n",
    "        return (ds.shuffle(BUFFER_SIZE).batch(train_batch_size).map(X_versus_Y_c, tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "    def apply_test_pipeline_c(ds):\n",
    "        return (ds.shuffle(BUFFER_SIZE).batch(test_batch_size).map(X_versus_Y_c, tf.data.AUTOTUNE).cache().prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "    def create_replace_baseLabel_with_profitable(path, model, ds_batches, threshold=0.0001):     \n",
    "        if os.path.exists(path):  ds = tf.data.Dataset.load(path)\n",
    "        else: ds = replace_baseLabel_with_profitable(model, ds_batches, threshold=threshold); tf.data.Dataset.save(ds, path);  ds = tf.data.Dataset.load(path)\n",
    "        return ds\n",
    "\n",
    "    model_1x2 = create_model_object(Model_1X2)\n",
    "    model_1x2.load_weights(checkpointPathBest)    # finetuning model_1x2 starts from the best pretrained model_1x2.\n",
    "    REPLACE_THRESHOLD = 0.00005\n",
    "    path_train_ds_with_profitable = os.path.join(dataset_foler_path, id_map_filename + '-train_ds_with_profitable')\n",
    "    train_ds_with_profitable = create_replace_baseLabel_with_profitable(path_train_ds_with_profitable, model_1x2, train_batches, threshold=REPLACE_THRESHOLD) # train_batches\n",
    "    print(\"train len: \", len(train_ds_with_profitable))\n",
    "    path_valid_ds_with_profitable = os.path.join(dataset_foler_path, id_map_filename + '-valid_ds_with_profitable')\n",
    "    valid_ds_with_profitable = create_replace_baseLabel_with_profitable(path_valid_ds_with_profitable, model_1x2, valid_batches, threshold=REPLACE_THRESHOLD) # valid_batches\n",
    "    print(\"valid len: \", len(valid_ds_with_profitable))\n",
    "    valid_batches_with_profitable = apply_test_pipeline_c(valid_ds_with_profitable)\n",
    "\n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    try: model_c.load_weights(checkpointPath_c); print(\"model_c loaded its previous checkpoint.\")\n",
    "    except: model_c.load_weights(checkpointPathBest); print(\"model_c loaded model_1x2's best pretrain checkpoint\")\n",
    "    # Goal: get checkpointPathBest_c checkpoint, which is used to filter datasets in favor of model_1x2\n",
    "    optimizer_c = Adam_exponential(history.len(), STARTING_LEARNING_RATE/5, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nEpochs = 20; wanted_upgrades = 5\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_c, optimizer_c, history_c, checkpointPath_c, checkpointPathBest_c, train_ds_with_profitable, valid_batches_with_profitable, apply_train_pipeline_c, get_f1=True, gaugeTerm='precision', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "\n",
    "    OPERATION = 'FINETUNE'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointPath_f = os.path.join(checkpoint_folder_path, TEST_ID + '_weights_f')\n",
    "checkpointPathBest_f = os.path.join(checkpoint_folder_path, TEST_ID + '_weights_best_f')\n",
    "historyPath_f = os.path.join(checkpoint_folder_path, TEST_ID + '_history_f.json')\n",
    "history_f = history_class(historyPath_f); history_f.load()\n",
    "\n",
    "model_1x2 = create_model_object(Model_1X2)\n",
    "try:    model_1x2.load_weights(checkpointPath)\n",
    "except: pass\n",
    "\n",
    "epoch = 0   # no gauge, as it's not an argument but measured in conclude_train_epoch.\n",
    "for train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken in history_f.get_zipped_history():\n",
    "    _ = conclude_train_epoch(epoch, model_1x2, history, checkpointPath, checkpointPathBest, train_loss, val_loss_0, val_profit_0, val_loss, val_profit, learning_rate, recall, precision, time_taken, playback = True, gaugeTerm='val_loss')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset_for_profitable_items(path, model, ds_batches, threshold=0.5):\n",
    "    if path is None: ds = select_profitable_items(model, ds_batches, threshold=threshold)\n",
    "    else:\n",
    "        if os.path.exists(path):  ds = tf.data.Dataset.load(path)\n",
    "        else: ds = select_profitable_items(model_c, ds_batches, threshold=threshold); tf.data.Dataset.save(ds, path);  ds = tf.data.Dataset.load(path)\n",
    "    return ds\n",
    "\n",
    "if OPERATION == 'FINETUNE': # model_1x2 is trained to find profitable staking vectors for games that are filtered in by model_c.\n",
    "   \n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "    FILTER_THRESHOLD = 0.5\n",
    "    path_train_ds_filtered = os.path.join(dataset_foler_path, id_map_filename + '-train_ds_filtered')\n",
    "    train_ds_filtered = filter_dataset_for_profitable_items(path_train_ds_filtered, model_c, train_batches, threshold=FILTER_THRESHOLD)    # train_batches\n",
    "    print(\"train len: \", len(train_ds_filtered))\n",
    "    path_valid_ds_filtered = os.path.join(dataset_foler_path, id_map_filename + '-valid_ds_filtered')\n",
    "    valid_ds_filtered = filter_dataset_for_profitable_items(path_valid_ds_filtered, model_c, valid_batches, threshold=FILTER_THRESHOLD)    # valid_batches\n",
    "    print(\"valid len: \", len(valid_ds_filtered))\n",
    "    valid_batches_filtered = apply_test_pipeline(valid_ds_filtered)\n",
    "\n",
    "    model_1x2 = create_model_object(Model_1X2)\n",
    "    try: model_1x2.load_weights(checkpointPath_f); print(\"model_1x2 loaded its previous checkpoint.\")\n",
    "    except: model_1x2.load_weights(checkpointPathBest); print(\"model_1x2 loaded its best pretrain checkpoints.\")\n",
    "    # Goal: get checkpointPathBest_f checkpoint, which is used as our final model. The final model needs the dataset filter model_c as seen in 'TEST' operation.\n",
    "    optimizer_f = Adam_exponential(history.len(), STARTING_LEARNING_RATE/10, EXAMPLE_LEARNING_STEP, EXAMPLE_LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-12)\n",
    "    # epochs, nUpgrades, model, optimizer, history, checkpointPath, checkpointPathBest, train_ds, valid_batches, apply_train_pipeline, get_f1=False\n",
    "    nEpochs = 10; wanted_upgrades = 3\n",
    "    nUpgrades = train(nEpochs, wanted_upgrades, model_1x2, optimizer_f, history_f, checkpointPath_f, checkpointPathBest_f, train_ds_filtered, valid_batches_filtered, apply_train_pipeline, get_f1 = False, gaugeTerm='val_loss', get_val_loss_0=GET_VAL_LOSS_0)\n",
    "\n",
    "    OPERATION = 'TEST'   # Certificate for the next operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OPERATION == 'TEST': # model_1x2, after pretrained and finetuned, is tested on a dataset that is filered in by model_c.\n",
    "    model_c = create_model_object(Model_Filter)\n",
    "    model_c.load_weights(checkpointPathBest_c)  # It's time to use the best classifier.\n",
    "\n",
    "    FILTER_THRESHOLD = 0.5\n",
    "    ds_filtered = filter_dataset_for_profitable_items(None, model_c, test_batches, threshold=0.5)\n",
    "    ds_filtered_batches = apply_test_pipeline(ds_filtered)\n",
    "    analyze(model_1x2, ds_filtered_batches, checkpointPathBest_f)  # compare with the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_a = 0.7    # Choose onefiltered_test_ds\n",
    "interval_b = 0.8    # Choose one\n",
    "\n",
    "if not TRAIN_MODE:      # just sketch\n",
    "    # df_grown gets to be the existing df_grown, because the train_mode if False.\n",
    "    df_total, df_new = df_grown, df_new = data_helpers.get_grown_and_new_from_football_data(CountryThemeFolderPath, Required_Non_Odds_cols, NUMBER_BOOKIES, train_mode = TRAIN_MODE, skip=False)\n",
    "    df_white = df_new\n",
    "    df_black = data_helpers.read_excel(path)\n",
    "\n",
    "    df_total = df_total; df_search = df_new\n",
    "    additional_id_to_ids = data_helpers.fixture_id_to_ids_uk_maxflow(CountryThemeFolderPath, targetLength, minCurrent, sinceDaysAgo, qualityPct, conductance365, df_total, df_search, chooseDivs=chooseDivs)\n",
    "    \n",
    "    ds_path = os.path.join(dataset_foler_path, id_map_filename + '-dataset-inference')\n",
    "\n",
    "    if os.path.exists(ds_path):\n",
    "        ds = tf.data.Dataset.load(ds_path)\n",
    "    else:\n",
    "        ds = generate_dataset_uk(df_total, additional_id_to_ids, tokenizer_team, normalization_parms, train_mode=False)  #-----------------                                                                          #-----------------\n",
    "        tf.data.Dataset.save(ds, ds_path)       # This is the true line by which the dataset generator gets to work.\n",
    "        ds = tf.data.Dataset.load(ds_path)      # Weird, this line is required. len(ds) would have no value!!!\n",
    "\n",
    "    ds_inference = apply_test_pipeline(ds)\n",
    "    stake_vectors = inference_with_dataset(ds_inference, interval_a, interval_b) # (batch, nBookies, nQueries)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
